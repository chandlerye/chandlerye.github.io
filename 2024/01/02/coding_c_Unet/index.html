<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>Coding-条件Unet-基于注意力机制 | Chandler&#39;s blog</title>
  <meta name="description" content="摘要 unet.py test.py    摘要 可将时间条件和类别条件引入模型，共两个文件：unet.py,test.py,模型为 b&#x3D;Unet(t,c,a)b&#x3D;Unet(t,c,a)  b&#x3D;Unet(t,c,a) 其中bbb是输出，ttt是时间条件，ccc是类别条件，aaa是输入   12345678910111213141516171819202122232425262728293031">
<meta property="og:type" content="article">
<meta property="og:title" content="Coding-条件Unet-基于注意力机制">
<meta property="og:url" content="http://example.com/2024/01/02/coding_c_Unet/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="摘要 unet.py test.py    摘要 可将时间条件和类别条件引入模型，共两个文件：unet.py,test.py,模型为 b&#x3D;Unet(t,c,a)b&#x3D;Unet(t,c,a)  b&#x3D;Unet(t,c,a) 其中bbb是输出，ttt是时间条件，ccc是类别条件，aaa是输入   12345678910111213141516171819202122232425262728293031">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-01-02T07:35:07.623Z">
<meta property="article:modified_time" content="2024-05-22T02:18:19.529Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Code-基础模型">
<meta name="twitter:card" content="summary">
  <!-- Canonical links -->
  <link rel="canonical" href="http://example.com/2024/01/02/coding_c_Unet/index.html">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" rel="stylesheet">
  
  
  
  
<meta name="generator" content="Hexo 6.3.0"></head>


<body class="main-center" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/chandlerye" target="_blank">
          <img class="img-circle img-rotate" src="/images/favicon.ico" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Chandler</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">一头牛马</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> YanTai, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/chandlerye" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>记录学习历程!</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B/">编程</a><span class="category-list-count">30</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/Code-%E5%88%86%E7%B1%BB/" style="font-size: 13.5px;">Code-分类</a> <a href="/tags/Code-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 13.83px;">Code-图像处理</a> <a href="/tags/Code-%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B/" style="font-size: 13.67px;">Code-基础模型</a> <a href="/tags/Code-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%B1%87%E6%80%BB/" style="font-size: 13.83px;">Code-基础知识汇总</a> <a href="/tags/Code-%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/" style="font-size: 13px;">Code-文件操作</a> <a href="/tags/Code-%E7%94%9F%E6%88%90/" style="font-size: 13.33px;">Code-生成</a> <a href="/tags/Paper-%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F/" style="font-size: 14px;">Paper-医学影像</a> <a href="/tags/Paper-%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/" style="font-size: 13.5px;">Paper-综述文章</a> <a href="/tags/Qt/" style="font-size: 13.17px;">Qt</a> <a href="/tags/%E5%8C%BB%E5%AD%A6/" style="font-size: 13.17px;">医学</a> <a href="/tags/%E6%B3%95%E5%BE%8B/" style="font-size: 13px;">法律</a> <a href="/tags/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" style="font-size: 13.33px;">编程基础知识</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">七月 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">六月 2024</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">五月 2024</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">二月 2024</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">一月 2024</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">十二月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a><span class="archive-list-count">7</span></li></ul>
    </div>
  </div>


    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-coding_c_Unet" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      Coding-条件Unet-基于注意力机制
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2024/01/02/coding_c_Unet/" class="article-date">
	  <time datetime="2024-01-02T07:35:07.623Z" itemprop="datePublished">2024-01-02</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/%E7%BC%96%E7%A8%8B/">编程</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/Code-%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B/" rel="tag">Code-基础模型</a>
  </span>


        

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2024/01/02/coding_c_Unet/#comments" class="article-comment-link">评论</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <!-- toc -->
<ul>
<li><a href="#%E6%91%98%E8%A6%81">摘要</a></li>
<li><a href="#unetpy">unet.py</a></li>
<li><a href="#testpy">test.py</a></li>
</ul>
<!-- tocstop -->
<h3><span id="摘要"> 摘要</span></h3>
<p>可将时间条件和类别条件引入模型，共两个文件：<a target="_blank" rel="noopener" href="http://unet.py">unet.py</a>,<a target="_blank" rel="noopener" href="http://test.py">test.py</a>,模型为</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi><mo>=</mo><mi>U</mi><mi>n</mi><mi>e</mi><mi>t</mi><mo>(</mo><mi>t</mi><mo separator="true">,</mo><mi>c</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">b=Unet(t,c,a) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">b</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.10903em;">U</span><span class="mord mathit">n</span><span class="mord mathit">e</span><span class="mord mathit">t</span><span class="mopen">(</span><span class="mord mathit">t</span><span class="mpunct">,</span><span class="mord mathit">c</span><span class="mpunct">,</span><span class="mord mathit">a</span><span class="mclose">)</span></span></span></span></span></p>
<p>其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">b</span></span></span></span>是输出，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.61508em;"></span><span class="strut bottom" style="height:0.61508em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span></span></span></span>是时间条件，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">c</span></span></span></span>是类别条件，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span></span></span></span>是输入</p>
<h3><span id="unetpy"> </span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.nn.modules.normalization <span class="keyword">import</span> GroupNorm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_norm</span>(<span class="params">norm, num_channels, num_groups</span>):</span><br><span class="line">    <span class="keyword">if</span> norm == <span class="string">&quot;in&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.InstanceNorm2d(num_channels, affine=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">elif</span> norm == <span class="string">&quot;bn&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.BatchNorm2d(num_channels)</span><br><span class="line">    <span class="keyword">elif</span> norm == <span class="string">&quot;gn&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.GroupNorm(num_groups, num_channels)</span><br><span class="line">    <span class="keyword">elif</span> norm <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.Identity()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;unknown normalization type&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEmbedding</span>(nn.Module):</span><br><span class="line">    __doc__ = <span class="string">r&quot;&quot;&quot;Computes a positional embedding of timesteps.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        x: tensor of shape (N)</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        tensor of shape (N, dim)</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): embedding dimension</span></span><br><span class="line"><span class="string">        scale (float): linear scale to be applied to timesteps. Default: 1.0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, scale=<span class="number">1.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> dim % <span class="number">2</span> == <span class="number">0</span></span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.scale = scale</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        device = x.device</span><br><span class="line">        half_dim = self.dim // <span class="number">2</span></span><br><span class="line">        emb = math.log(<span class="number">10000</span>) / half_dim</span><br><span class="line">        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)</span><br><span class="line">        emb = torch.outer(x * self.scale, emb)</span><br><span class="line">        emb = torch.cat((emb.sin(), emb.cos()), dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> emb</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Downsample</span>(nn.Module):</span><br><span class="line">    __doc__ = <span class="string">r&quot;&quot;&quot;Downsamples a given tensor by a factor of 2. Uses strided convolution. Assumes even height and width.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        x: tensor of shape (N, in_channels, H, W)</span></span><br><span class="line"><span class="string">        time_emb: ignored</span></span><br><span class="line"><span class="string">        y: ignored</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        tensor of shape (N, in_channels, H // 2, W // 2)</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        in_channels (int): number of input channels</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.downsample = nn.Conv2d(in_channels, in_channels, <span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, time_emb, y</span>):</span><br><span class="line">        <span class="keyword">if</span> x.shape[<span class="number">2</span>] % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;downsampling tensor height should be even&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> x.shape[<span class="number">3</span>] % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;downsampling tensor width should be even&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.downsample(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Upsample</span>(nn.Module):</span><br><span class="line">    __doc__ = <span class="string">r&quot;&quot;&quot;Upsamples a given tensor by a factor of 2. Uses resize convolution to avoid checkerboard artifacts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        x: tensor of shape (N, in_channels, H, W)</span></span><br><span class="line"><span class="string">        time_emb: ignored</span></span><br><span class="line"><span class="string">        y: ignored</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        tensor of shape (N, in_channels, H * 2, W * 2)</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        in_channels (int): number of input channels</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.upsample = nn.Sequential(</span><br><span class="line">            nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">&quot;nearest&quot;</span>),</span><br><span class="line">            nn.Conv2d(in_channels, in_channels, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, time_emb, y</span>):</span><br><span class="line">        <span class="keyword">return</span> self.upsample(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionBlock</span>(nn.Module):</span><br><span class="line">    __doc__ = <span class="string">r&quot;&quot;&quot;Applies QKV self-attention with a residual connection.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        x: tensor of shape (N, in_channels, H, W)</span></span><br><span class="line"><span class="string">        norm (string or None): which normalization to use (instance, group, batch, or none). Default: &quot;gn&quot;</span></span><br><span class="line"><span class="string">        num_groups (int): number of groups used in group normalization. Default: 32</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        tensor of shape (N, in_channels, H, W)</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        in_channels (int): number of input channels</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, norm=<span class="string">&quot;gn&quot;</span>, num_groups=<span class="number">32</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.norm = get_norm(norm, in_channels, num_groups)</span><br><span class="line">        self.to_qkv = nn.Conv2d(in_channels, in_channels * <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        self.to_out = nn.Conv2d(in_channels, in_channels, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b, c, h, w = x.shape</span><br><span class="line">        q, k, v = torch.split(self.to_qkv(self.norm(x)), self.in_channels, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        q = q.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).view(b, h * w, c)</span><br><span class="line">        k = k.view(b, c, h * w)</span><br><span class="line">        v = v.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).view(b, h * w, c)</span><br><span class="line"></span><br><span class="line">        dot_products = torch.bmm(q, k) * (c ** (-<span class="number">0.5</span>))</span><br><span class="line">        <span class="keyword">assert</span> dot_products.shape == (b, h * w, h * w)</span><br><span class="line"></span><br><span class="line">        attention = torch.softmax(dot_products, dim=-<span class="number">1</span>)</span><br><span class="line">        out = torch.bmm(attention, v)</span><br><span class="line">        <span class="keyword">assert</span> out.shape == (b, h * w, c)</span><br><span class="line">        out = out.view(b, h, w, c).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.to_out(out) + x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBlock</span>(nn.Module):</span><br><span class="line">    __doc__ = <span class="string">r&quot;&quot;&quot;Applies two conv blocks with resudual connection. Adds time and class conditioning by adding bias after first convolution.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        x: tensor of shape (N, in_channels, H, W)</span></span><br><span class="line"><span class="string">        time_emb: time embedding tensor of shape (N, time_emb_dim) or None if the block doesn&#x27;t use time conditioning</span></span><br><span class="line"><span class="string">        y: classes tensor of shape (N) or None if the block doesn&#x27;t use class conditioning</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        tensor of shape (N, out_channels, H, W)</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        in_channels (int): number of input channels</span></span><br><span class="line"><span class="string">        out_channels (int): number of output channels</span></span><br><span class="line"><span class="string">        time_emb_dim (int or None): time embedding dimension or None if the block doesn&#x27;t use time conditioning. Default: None</span></span><br><span class="line"><span class="string">        num_classes (int or None): number of classes or None if the block doesn&#x27;t use class conditioning. Default: None</span></span><br><span class="line"><span class="string">        activation (function): activation function. Default: torch.nn.functional.relu</span></span><br><span class="line"><span class="string">        norm (string or None): which normalization to use (instance, group, batch, or none). Default: &quot;gn&quot;</span></span><br><span class="line"><span class="string">        num_groups (int): number of groups used in group normalization. Default: 32</span></span><br><span class="line"><span class="string">        use_attention (bool): if True applies AttentionBlock to the output. Default: False</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        in_channels,</span></span><br><span class="line"><span class="params">        out_channels,</span></span><br><span class="line"><span class="params">        dropout,</span></span><br><span class="line"><span class="params">        time_emb_dim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        num_classes=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        activation=F.relu,</span></span><br><span class="line"><span class="params">        norm=<span class="string">&quot;gn&quot;</span>,</span></span><br><span class="line"><span class="params">        num_groups=<span class="number">32</span>,</span></span><br><span class="line"><span class="params">        use_attention=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.activation = activation</span><br><span class="line"></span><br><span class="line">        self.norm_1 = get_norm(norm, in_channels, num_groups)</span><br><span class="line">        self.conv_1 = nn.Conv2d(in_channels, out_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.norm_2 = get_norm(norm, out_channels, num_groups)</span><br><span class="line">        self.conv_2 = nn.Sequential(</span><br><span class="line">            nn.Dropout(p=dropout),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.time_bias = nn.Linear(time_emb_dim, out_channels) <span class="keyword">if</span> time_emb_dim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        self.class_bias = nn.Embedding(num_classes, out_channels) <span class="keyword">if</span> num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.residual_connection = nn.Conv2d(in_channels, out_channels, <span class="number">1</span>) <span class="keyword">if</span> in_channels != out_channels <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.attention = nn.Identity() <span class="keyword">if</span> <span class="keyword">not</span> use_attention <span class="keyword">else</span> AttentionBlock(out_channels, norm, num_groups)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, time_emb=<span class="literal">None</span>, y=<span class="literal">None</span></span>):</span><br><span class="line">        out = self.activation(self.norm_1(x))</span><br><span class="line">        out = self.conv_1(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.time_bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> time_emb <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&quot;time conditioning was specified but time_emb is not passed&quot;</span>)</span><br><span class="line">            out += self.time_bias(self.activation(time_emb))[:, :, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.class_bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&quot;class conditioning was specified but y is not passed&quot;</span>)</span><br><span class="line"></span><br><span class="line">            out += self.class_bias(y)[:, :, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">        out = self.activation(self.norm_2(out))</span><br><span class="line">        out = self.conv_2(out) + self.residual_connection(x)</span><br><span class="line">        out = self.attention(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UNet</span>(nn.Module):</span><br><span class="line">    __doc__ = <span class="string">&quot;&quot;&quot;UNet model used to estimate noise.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        x: tensor of shape (N, in_channels, H, W)</span></span><br><span class="line"><span class="string">        time_emb: time embedding tensor of shape (N, time_emb_dim) or None if the block doesn&#x27;t use time conditioning</span></span><br><span class="line"><span class="string">        y: classes tensor of shape (N) or None if the block doesn&#x27;t use class conditioning</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        tensor of shape (N, out_channels, H, W)</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        img_channels (int): number of image channels</span></span><br><span class="line"><span class="string">        base_channels (int): number of base channels (after first convolution)</span></span><br><span class="line"><span class="string">        channel_mults (tuple): tuple of channel multiplers. Default: (1, 2, 4, 8)</span></span><br><span class="line"><span class="string">        time_emb_dim (int or None): time embedding dimension or None if the block doesn&#x27;t use time conditioning. Default: None</span></span><br><span class="line"><span class="string">        time_emb_scale (float): linear scale to be applied to timesteps. Default: 1.0</span></span><br><span class="line"><span class="string">        num_classes (int or None): number of classes or None if the block doesn&#x27;t use class conditioning. Default: None</span></span><br><span class="line"><span class="string">        activation (function): activation function. Default: torch.nn.functional.relu</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate at the end of each residual block</span></span><br><span class="line"><span class="string">        attention_resolutions (tuple): list of relative resolutions at which to apply attention. Default: ()</span></span><br><span class="line"><span class="string">        norm (string or None): which normalization to use (instance, group, batch, or none). Default: &quot;gn&quot;</span></span><br><span class="line"><span class="string">        num_groups (int): number of groups used in group normalization. Default: 32</span></span><br><span class="line"><span class="string">        initial_pad (int): initial padding applied to image. Should be used if height or width is not a power of 2. Default: 0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        img_channels,</span></span><br><span class="line"><span class="params">        base_channels,</span></span><br><span class="line"><span class="params">        channel_mults=(<span class="params"><span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span></span>),</span></span><br><span class="line"><span class="params">        num_res_blocks=<span class="number">2</span>,</span></span><br><span class="line"><span class="params">        time_emb_dim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        time_emb_scale=<span class="number">1.0</span>,</span></span><br><span class="line"><span class="params">        num_classes=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        activation=F.relu,</span></span><br><span class="line"><span class="params">        dropout=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">        attention_resolutions=(<span class="params"></span>),</span></span><br><span class="line"><span class="params">        norm=<span class="string">&quot;gn&quot;</span>,</span></span><br><span class="line"><span class="params">        num_groups=<span class="number">32</span>,</span></span><br><span class="line"><span class="params">        initial_pad=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.activation = activation</span><br><span class="line">        self.initial_pad = initial_pad</span><br><span class="line"></span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.time_mlp = nn.Sequential(</span><br><span class="line">            PositionalEmbedding(base_channels, time_emb_scale),</span><br><span class="line">            nn.Linear(base_channels, time_emb_dim),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            nn.Linear(time_emb_dim, time_emb_dim),</span><br><span class="line">        ) <span class="keyword">if</span> time_emb_dim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">        self.init_conv = nn.Conv2d(img_channels, base_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.downs = nn.ModuleList()</span><br><span class="line">        self.ups = nn.ModuleList()</span><br><span class="line"></span><br><span class="line">        channels = [base_channels]</span><br><span class="line">        now_channels = base_channels</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, mult <span class="keyword">in</span> <span class="built_in">enumerate</span>(channel_mults):</span><br><span class="line">            out_channels = base_channels * mult</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks):</span><br><span class="line">                self.downs.append(ResidualBlock(</span><br><span class="line">                    now_channels,</span><br><span class="line">                    out_channels,</span><br><span class="line">                    dropout,</span><br><span class="line">                    time_emb_dim=time_emb_dim,</span><br><span class="line">                    num_classes=num_classes,</span><br><span class="line">                    activation=activation,</span><br><span class="line">                    norm=norm,</span><br><span class="line">                    num_groups=num_groups,</span><br><span class="line">                    use_attention=i <span class="keyword">in</span> attention_resolutions,</span><br><span class="line">                ))</span><br><span class="line">                now_channels = out_channels</span><br><span class="line">                channels.append(now_channels)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> i != <span class="built_in">len</span>(channel_mults) - <span class="number">1</span>:</span><br><span class="line">                self.downs.append(Downsample(now_channels))</span><br><span class="line">                channels.append(now_channels)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        self.mid = nn.ModuleList([</span><br><span class="line">            ResidualBlock(</span><br><span class="line">                now_channels,</span><br><span class="line">                now_channels,</span><br><span class="line">                dropout,</span><br><span class="line">                time_emb_dim=time_emb_dim,</span><br><span class="line">                num_classes=num_classes,</span><br><span class="line">                activation=activation,</span><br><span class="line">                norm=norm,</span><br><span class="line">                num_groups=num_groups,</span><br><span class="line">                use_attention=<span class="literal">True</span>,</span><br><span class="line">            ),</span><br><span class="line">            ResidualBlock(</span><br><span class="line">                now_channels,</span><br><span class="line">                now_channels,</span><br><span class="line">                dropout,</span><br><span class="line">                time_emb_dim=time_emb_dim,</span><br><span class="line">                num_classes=num_classes,</span><br><span class="line">                activation=activation,</span><br><span class="line">                norm=norm,</span><br><span class="line">                num_groups=num_groups,</span><br><span class="line">                use_attention=<span class="literal">False</span>,</span><br><span class="line">            ),</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, mult <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">list</span>(<span class="built_in">enumerate</span>(channel_mults))):</span><br><span class="line">            out_channels = base_channels * mult</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks + <span class="number">1</span>):</span><br><span class="line">                self.ups.append(ResidualBlock(</span><br><span class="line">                    channels.pop() + now_channels,</span><br><span class="line">                    out_channels,</span><br><span class="line">                    dropout,</span><br><span class="line">                    time_emb_dim=time_emb_dim,</span><br><span class="line">                    num_classes=num_classes,</span><br><span class="line">                    activation=activation,</span><br><span class="line">                    norm=norm,</span><br><span class="line">                    num_groups=num_groups,</span><br><span class="line">                    use_attention=i <span class="keyword">in</span> attention_resolutions,</span><br><span class="line">                ))</span><br><span class="line">                now_channels = out_channels</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span>:</span><br><span class="line">                self.ups.append(Upsample(now_channels))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(channels) == <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        self.out_norm = get_norm(norm, base_channels, num_groups)</span><br><span class="line">        self.out_conv = nn.Conv2d(base_channels, img_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, time=<span class="literal">None</span>, y=<span class="literal">None</span></span>):</span><br><span class="line">        ip = self.initial_pad</span><br><span class="line">        <span class="keyword">if</span> ip != <span class="number">0</span>:</span><br><span class="line">            x = F.pad(x, (ip,) * <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.time_mlp <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> time <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&quot;time conditioning was specified but tim is not passed&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            time_emb = self.time_mlp(time)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            time_emb = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;class conditioning was specified but y is not passed&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        x = self.init_conv(x)</span><br><span class="line"></span><br><span class="line">        skips = [x]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.downs:</span><br><span class="line">            x = layer(x, time_emb, y)</span><br><span class="line">            skips.append(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.mid:</span><br><span class="line">            x = layer(x, time_emb, y)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.ups:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer, ResidualBlock):</span><br><span class="line">                x = torch.cat([x, skips.pop()], dim=<span class="number">1</span>)</span><br><span class="line">            x = layer(x, time_emb, y)</span><br><span class="line"></span><br><span class="line">        x = self.activation(self.out_norm(x))</span><br><span class="line">        x = self.out_conv(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.initial_pad != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> x[:, :, ip:-ip, ip:-ip]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3><span id="testpy"> </span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> unet <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    activations = &#123;</span><br><span class="line">            <span class="string">&quot;relu&quot;</span>: F.relu,</span><br><span class="line">            <span class="string">&quot;mish&quot;</span>: F.mish,</span><br><span class="line">            <span class="string">&quot;silu&quot;</span>: F.silu,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    model = UNet(</span><br><span class="line">            img_channels=<span class="number">3</span>,</span><br><span class="line"></span><br><span class="line">            base_channels=<span class="number">64</span>,</span><br><span class="line">            channel_mults=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            time_emb_dim=<span class="number">512</span>,</span><br><span class="line">            norm=<span class="string">&#x27;gn&#x27;</span>,</span><br><span class="line">            dropout=<span class="number">0.1</span>,</span><br><span class="line">            activation=activations[<span class="string">&#x27;silu&#x27;</span>],</span><br><span class="line">            attention_resolutions=(<span class="number">1</span>,),</span><br><span class="line"></span><br><span class="line">            num_classes=<span class="number">10</span>,</span><br><span class="line">            initial_pad=<span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    a = torch.FloatTensor(<span class="number">1</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>)</span><br><span class="line">    t = torch.randn(<span class="number">1</span>).long()</span><br><span class="line">    y = torch.randn(<span class="number">1</span>).long()</span><br><span class="line">    b = model(a,t,y)</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>
    <div class="article-footer">
      <!-- <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="http://example.com/2024/01/02/coding_c_Unet/" title="Coding-条件Unet-基于注意力机制" target="_blank" rel="external">http://example.com/2024/01/02/coding_c_Unet/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote> -->


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://github.com/chandlerye" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/favicon.ico" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://github.com/chandlerye" target="_blank"><span class="text-dark">Chandler</span><small class="ml-1x">一头牛马</small></a></h3>
        <div>江上两条红船，寒风斜雨摇摆</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/2024/01/08/coding_network_CNN/" title="Coding-猫狗数据集分类-基于CNN"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/2023/12/26/coding_medical_network_example/" title="Coding-3D医学图像分类-基于3D-Resnet"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  
  <div class="bar-right">
    
  </div>
  </div>
</nav>
  


</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/chandlerye" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>



  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   




   






</body>
</html>