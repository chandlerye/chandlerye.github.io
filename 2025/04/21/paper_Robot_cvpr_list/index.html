<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>PaperReading-Robot_list-CVPR | Chandler&#39;s blog</title>
  <meta name="description" content="英文题目：DexGraspAnything: Towards Universal Robotic Dexterous Grasping with Physics Awareness  中文题目：DexGraspAnything：迈向具有物理感知的通用机器人灵巧抓取 研究背景： 灵巧抓取对机器人至关重要，是机器人实现复杂操作任务的基础能力。五手指灵巧手相比简单抓手，在灵活性、操作精度和通用性上优势">
<meta property="og:type" content="article">
<meta property="og:title" content="PaperReading-Robot_list-CVPR">
<meta property="og:url" content="http://example.com/2025/04/21/paper_Robot_cvpr_list/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="英文题目：DexGraspAnything: Towards Universal Robotic Dexterous Grasping with Physics Awareness  中文题目：DexGraspAnything：迈向具有物理感知的通用机器人灵巧抓取 研究背景： 灵巧抓取对机器人至关重要，是机器人实现复杂操作任务的基础能力。五手指灵巧手相比简单抓手，在灵活性、操作精度和通用性上优势">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-04-21T08:20:48.021Z">
<meta property="article:modified_time" content="2025-04-21T08:23:17.412Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Paper-Robot_list">
<meta property="article:tag" content="Paper-CVPR">
<meta name="twitter:card" content="summary">
  <!-- Canonical links -->
  <link rel="canonical" href="http://example.com/2025/04/21/paper_Robot_cvpr_list/index.html">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 6.3.0"></head>


<body class="main-center" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/chandlerye" target="_blank">
          <img class="img-circle img-rotate" src="/images/favicon.ico" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Chandler</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">一头牛马</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> YanTai, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/chandlerye" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>记录学习历程!</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B/">编程</a><span class="category-list-count">52</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/Code-isaacsim/" style="font-size: 13px;">Code-isaacsim</a> <a href="/tags/Code-%E5%88%86%E7%B1%BB/" style="font-size: 13.5px;">Code-分类</a> <a href="/tags/Code-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 13.88px;">Code-图像处理</a> <a href="/tags/Code-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 13.13px;">Code-强化学习</a> <a href="/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/" style="font-size: 14px;">Code-文件处理</a> <a href="/tags/Code-%E6%9C%BA%E5%99%A8%E4%BA%BA/" style="font-size: 13.25px;">Code-机器人</a> <a href="/tags/Code-%E6%A8%A1%E5%9E%8B%E5%BA%93/" style="font-size: 13.63px;">Code-模型库</a> <a href="/tags/Code-%E7%94%9F%E6%88%90/" style="font-size: 13.25px;">Code-生成</a> <a href="/tags/Paper-CVPR/" style="font-size: 13px;">Paper-CVPR</a> <a href="/tags/Paper-ICRA/" style="font-size: 13px;">Paper-ICRA</a> <a href="/tags/Paper-Robot-list/" style="font-size: 13.25px;">Paper-Robot_list</a> <a href="/tags/Paper-TRO/" style="font-size: 13px;">Paper-TRO</a> <a href="/tags/Paper-%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F/" style="font-size: 13.75px;">Paper-医学影像</a> <a href="/tags/Paper-%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/" style="font-size: 13.38px;">Paper-综述文章</a> <a href="/tags/Qt/" style="font-size: 13.13px;">Qt</a> <a href="/tags/%E5%8C%BB%E5%AD%A6/" style="font-size: 13.13px;">医学</a> <a href="/tags/%E6%B3%95%E5%BE%8B/" style="font-size: 13px;">法律</a> <a href="/tags/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" style="font-size: 13.25px;">编程基础知识</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/04/">四月 2025</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">十二月 2024</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/11/">十一月 2024</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">七月 2024</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">六月 2024</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">五月 2024</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">二月 2024</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">一月 2024</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">十二月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a><span class="archive-list-count">7</span></li></ul>
    </div>
  </div>


    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-paper_Robot_cvpr_list" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      PaperReading-Robot_list-CVPR
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2025/04/21/paper_Robot_cvpr_list/" class="article-date">
	  <time datetime="2025-04-21T08:20:48.021Z" itemprop="datePublished">2025-04-21</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/Paper/">Paper</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/Paper-CVPR/" rel="tag">Paper-CVPR</a>, <a class="article-tag-link-link" href="/tags/Paper-Robot-list/" rel="tag">Paper-Robot_list</a>
  </span>


        

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2025/04/21/paper_Robot_cvpr_list/#comments" class="article-comment-link">评论</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <h3><span id="英文题目dexgraspanything-towards-universal-robotic-dexterous-grasping-with-physics-awareness"> 英文题目：DexGraspAnything: Towards Universal Robotic Dexterous Grasping with Physics Awareness</span></h3>
<h3><span id="中文题目dexgraspanything迈向具有物理感知的通用机器人灵巧抓取"> 中文题目：DexGraspAnything：迈向具有物理感知的通用机器人灵巧抓取</span></h3>
<p><strong>研究背景</strong>：<br>
灵巧抓取对机器人至关重要，是机器人实现复杂操作任务的基础能力。五手指灵巧手相比简单抓手，在灵活性、操作精度和通用性上优势显著，能更好地适应人类环境，与各种物体和工具交互，是具身智能交互的核心。早期灵巧抓取研究主要采用分析方法，依赖手动推导和物理约束优化抓取姿势，但因搜索空间大、优化复杂，成功率较低。随着深度学习发展，数据驱动方法成为主流，其中基于回归的方法生成的抓取姿势多样性有限，生成式方法尤其是扩散模型虽能生成多样姿势，但缺乏物理约束，导致生成的抓取姿势往往不是最优，存在手与物体穿透或接触不足的问题。</p>
<p><strong>所存在的问题</strong>：</p>
<ul>
<li><strong>传统分析方法</strong>：搜索空间大且优化复杂，导致灵巧抓取的成功率低。</li>
<li><strong>数据驱动的回归方法</strong>：直接从输入数据预测抓取参数，易出现模式崩溃和平均化问题，生成的抓取姿势多样性受限。</li>
<li><strong>基于扩散模型的生成方法</strong>：在训练和采样过程中缺乏物理约束，生成的抓取姿势常出现手与物体穿透或接触不足的情况，成功率不理想。</li>
<li><strong>现有数据集</strong>：存在数据分布狭窄、物体类别有限、可扩展性差等问题，难以满足基于扩散模型的生成方法对大规模、高质量训练数据的需求。</li>
</ul>
<p><strong>解决方法</strong>：<br>
提出DexGrasp Anything方法，将精心设计的物理约束集成到扩散模型的训练和采样阶段。具体包括：</p>
<ul>
<li><strong>物理约束设计</strong>：引入表面拉力，确保抓取可行性，使手指内表面靠近物体表面；引入外部穿透排斥力，减少手与物体的不必要相交，保持交互的空间准确性；引入自穿透排斥力，维持手的结构几何形状，避免手指间碰撞。</li>
<li><strong>物理感知训练</strong>：定义物理感知训练目标，将标准均方误差目标与多个物理约束目标线性组合，使扩散模型在训练过程中学习物理先验。通过估计的干净样本将梯度传播到含噪数据，引导模型生成更符合物理规则的抓取姿势。</li>
<li><strong>物理引导采样</strong>：利用训练好的扩散生成器，在采样过程中进一步增强物理约束。采用分类器引导技术，将物理约束纳入采样过程，通过调整后验均值来引导扩散模型生成更合理的抓取姿势。同时，应用球形高斯约束来减轻估计偏差，逐步调整抓取配置，使其更符合物理可行性。</li>
<li><strong>LLM增强的表示提取</strong>：利用强大的大语言模型（LLM）增强传统的物体表示。通过提示LLM获取物体的语义先验信息，与几何物体特征结合，经点Transformer编码和跨注意力机制集成到扩散模型中，提升模型生成精确抓取姿势的能力。</li>
</ul>
<p><strong>所用到的数据集</strong>：<br>
构建了DexGrasp Anything（DGA）数据集，这是目前最大且最多样化的灵巧抓取数据集：</p>
<ul>
<li><strong>数据构建</strong>：收集多个来源的现有数据，包括模拟数据、真实捕获数据和人类手部抓取数据，如GRAB、DexGraspNet等。利用机器人遥操作系统将人类手部数据集转换为灵巧手参数，并进行严格过滤，确保数据质量。使用训练好的模型以“模型在环”的策略生成更多数据，最终形成大规模多样的数据集。</li>
<li><strong>数据统计</strong>：该数据集包含两个主要部分，DGA - curated约有88万个抓取姿势，涉及5664个不同物体；DGA - generated约有252万个抓取姿势，涵盖10034个不同物体，总共包含超过340万个抓取姿势，涉及15698个物体。</li>
<li><strong>数据特点</strong>：数据规模大，远超之前的数据集；物体种类多样，涵盖广泛的类别和来源，其物体特征在特征空间中分布更广；抓取姿势多样，有助于提升现有方法生成结果的多样性，同时保持或提高抓取成功率。</li>
</ul>
<h3><span id="英文题目spatial-temporal-graph-diffusion-policy-with-kinematic-modeling-for-bimanual-robotic-manipulation"> 英文题目：Spatial-Temporal Graph Diffusion Policy with Kinematic Modeling for Bimanual Robotic Manipulation</span></h3>
<h3><span id="中文题目用于双机器人操作的结合运动学建模的时空图扩散策略"> 中文题目：用于双机器人操作的结合运动学建模的时空图扩散策略</span></h3>
<p><strong>研究背景</strong>：<br>
双机器人操作是机器人执行复杂任务的基本能力，对机器人系统至关重要。模仿学习在单臂操作中取得成功，但在双机器人操作场景中面临挑战。双机器人操作需要协调双臂运动并符合物理约束，这使得现有方法在实际应用中可靠性和可行性受限。主流方法采用预测下一个最佳末端执行器姿势（NBP），再通过逆运动学计算关节旋转角度的两阶段流水线，但这种方法存在诸多问题。</p>
<p><strong>所存在的问题</strong>：<br>
现有方法在双机器人操作任务中存在两个主要问题。一是很少考虑机器人的物理结构，可能导致双臂自碰撞或相互干扰；二是忽略运动学约束，预测的姿势可能不符合机器人关节的实际限制，使得生成的运动不可靠，在实际执行中容易失败。</p>
<p><strong>解决方法</strong>：<br>
提出Kinematics enhanced Spatial-TemporAl gRaph Diffuser（KStar Diffuser）框架，将机器人结构和运动学融入双机器人运动生成过程。构建动态时空图，根据机器人的URDF规范，节点表示关节属性，边捕获空间关系和时间依赖，通过图卷积网络（GCN）编码为扩散过程提供物理约束；引入可微运动学模块，通过可微正向运动学将预测的关节位置映射到参考末端执行器姿势，作为扩散过程的条件，确保生成的运动满足结构和运动学约束。</p>
<p><strong>所用到的数据集</strong>：<br>
使用RLBench2基准数据集进行实验，该数据集是为双机器人操作定制的扩展版本，包含许多与现实场景相似的任务，用于评估KStar Diffuser在双机器人操作任务中的能力。同时，构建了两个基于模拟基准的真实世界任务数据集，即lift plate和handover item easy，用于进一步评估模型在真实环境中的有效性。</p>
<h3><span id="英文题目fmb-a-functional-manipulation-benchmark-for-generalizable-robotic-learning"> 英文题目：FMB: a Functional Manipulation Benchmark for Generalizable Robotic Learning</span></h3>
<h3><span id="中文题目fmb用于可泛化机器人学习的功能性操作基准"> 中文题目：FMB：用于可泛化机器人学习的功能性操作基准</span></h3>
<p><strong>研究背景</strong>：<br>
机器人操作是机器人研究的基础问题，但让机器人实现类似人类的灵巧操作仍极具挑战，主要困难在于处理复杂的接触动力学以及环境和物体的可变性。虽然机器人学习技术有潜力解决这些问题，但目前的研究存在局限性，要么侧重于简单技能的广泛泛化，未充分考虑操作中的物理挑战；要么专注于执行狭窄任务的复杂技能，缺乏广泛的泛化能力。</p>
<p><strong>所存在的问题</strong>：<br>
现有研究难以同时在广泛泛化和处理灵巧操作的物理复杂性方面取得进展。在机器人操作任务中，缺乏一个综合且易于使用的框架，该框架应包含具有实际相关性的挑战性任务、适量的高质量数据、易于重现的实验设置、相关方法的基线结果以及对实验结果的深入分析。</p>
<p><strong>解决方法</strong>：<br>
提出功能性操作基准（FMB），通过精心设计任务、提供全面的数据集以及可重现的硬件和软件系统，来研究机器人操作学习中的关键挑战。FMB中的任务分为单物体多阶段操作任务和多物体多阶段操作任务，涵盖抓取、重新定位和装配等基本操作技能。同时，提供了一套模仿学习框架，包括训练好的策略，方便研究人员对不同阶段的操作技能进行研究。</p>
<p><strong>所用到的数据集</strong>：<br>
FMB构建了包含22,550个演示轨迹的数据集，涵盖单物体和多物体操作任务。其中，单物体操作任务数据集包含从抓取到插入的完整演示2700个，以及单独插入阶段的演示4050个；多物体操作任务数据集包含3个多物体装配任务的150个端到端演示。数据集中的每个演示轨迹都包含多个传感器模态的数据，如多个摄像头的RGB和深度图像、机器人运动学信息以及末端执行器的力/扭矩测量数据。</p>
<h3><span id="英文题目bundlesdf-neural-6-dof-tracking-and-3d-reconstruction-of-unknown-objects"> 英文题目：BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects</span></h3>
<h3><span id="中文题目bundlesdf未知物体的神经6自由度跟踪与3d重建"> 中文题目：BundleSDF：未知物体的神经6自由度跟踪与3D重建</span></h3>
<p><strong>研究背景</strong>：<br>
6自由度（6-DoF）姿态跟踪和从单目RGBD视频进行未知物体的3D重建是计算机视觉中的两个关键且紧密相关的问题，在增强现实、机器人操作等领域有广泛应用前景。但此前的研究通常将这两个问题分开处理，且存在诸多局限性。例如，神经场景表示方法在创建高质量3D物体模型时，往往依赖已知的相机姿态和/或真实物体掩模，且难以对动态相机下的静态物体进行完整3D重建；而6-DoF物体姿态估计和跟踪方法，通常需要测试物体的纹理3D模型进行预训练或在线模板匹配，类别级方法在处理分布外物体实例和未见物体类别时也存在困难。</p>
<p><strong>所存在的问题</strong>：<br>
现有方法在处理未知物体的6-DoF姿态跟踪和3D重建时，无法同时满足无需物体先验知识、能处理复杂场景（如遮挡、无纹理、镜面高光等）以及实时性好等要求。传统方法在面对纹理或几何线索不足、存在遮挡等情况时，容易出现跟踪漂移、重建不准确等问题，难以实现可靠的实时跟踪和高质量的3D重建。</p>
<p><strong>解决方法</strong>：<br>
提出一种联合解决6-DoF姿态跟踪和3D重建的方法。该方法通过在线姿态图优化、并行训练的神经物体场以及动态记忆池协同工作来实现。首先利用基于Transformer的特征匹配网络和RANSAC算法进行粗姿态初始化；接着将关键帧存储在记忆池中，通过与记忆池中的帧对比更新当前帧姿态；然后进行在线姿态图优化，选择部分记忆帧参与优化以得到更准确的姿态估计；最后，神经物体场利用记忆池中的帧学习物体的3D形状和外观，同时调整记忆帧的姿态。此外，采用混合SDF表示处理不确定自由空间问题，提高模型的鲁棒性。</p>
<p><strong>所用到的数据集</strong>：<br>
使用了三个真实世界数据集进行实验评估。HO3D数据集包含人类手部与YCB物体交互的RGBD视频；YCBInEOAT数据集包含双臂机器人操作YCB物体的以自我为中心的RGBD视频；BEHAVE数据集包含人体与物体交互的RGBD视频，评估时限制在单视图设置下。这些数据集涵盖了不同形式的交互和动态场景，用于全面测试方法在各种情况下的性能。</p>
<h3><span id="英文题目on-the-importance-of-accurate-geometry-data-for-dense-3d-vision-tasks"> 英文题目：On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks</span></h3>
<h3><span id="中文题目精确几何数据在密集3d视觉任务中的重要性研究"> 中文题目：精确几何数据在密集3D视觉任务中的重要性研究</span></h3>
<p><strong>研究背景</strong>：<br>
3D视觉在自动驾驶、机器人视觉和增强现实等领域至关重要，距离测量是其关键。当前存在多种传感器模态和深度预测方法，且有丰富的公开数据集用于评估深度估计流程。不同传感器依据不同测距原理各有利弊，但在训练模型时，人们常未充分考量其特性，直接将其测量数据当作真值，忽略了数据不准确或存在误差的问题。</p>
<p><strong>所存在的问题</strong>：<br>
不同深度传感器在面对诸如无纹理区域、反射性材料和半透明物体等场景时，会产生测量误差。而基于不准确数据训练的模型会出现偏差，泛化能力也会受到影响。若在评估时将存在误差的传感器测量值视为真值，就无法发现这些问题，进而可能导致对模型性能的错误解读。</p>
<p><strong>解决方法</strong>：<br>
构建一个独特的多模态传感器数据集，涵盖D-ToF、I-ToF、被动/主动立体视觉和单目RGB + P等多种传感器数据，并使用高精度3D扫描仪和对齐渲染获取同步捕获的高精度真值数据。通过该数据集，深入分析不同深度传感器模态对模型训练的影响，量化传感器噪声的作用，为改进密集视觉估计和针对性的数据融合提供依据。</p>
<p><strong>所用到的数据集</strong>：<br>
自主构建的室内多模态数据集，包含7个室内区域、6张桌子、4把椅子以及64个来自9个类别的 household objects。数据集中共有13个场景，分为10个训练场景和3个测试场景。该数据集与以往室内多模态深度数据集不同，它能同时提供高精度的真值（深度、表面法线、6D物体姿态、实例掩码、相机姿态、密集场景网格）以及多样的传感器数据，为研究提供了更全面准确的数据支持。</p>
<h3><span id="英文题目dexterous-grasp-transformer"> 英文题目：Dexterous Grasp Transformer</span></h3>
<h3><span id="中文题目灵巧抓取变换器"> 中文题目：灵巧抓取变换器</span></h3>
<p><strong>研究背景</strong>：<br>
机器人灵巧抓取在机器人学和计算机视觉领域极为关键，在工业生产和日常生活场景应用广泛。深度学习与大规模数据集推动了基于学习的抓取方法发展，使其在抓取质量和泛化性上取得进展。但现有方法在生成多样且高质量的抓取姿势方面存在不足，生成式模型生成的抓取姿势多样性欠佳，而传统判别式模型每次只能预测一个抓取姿势，且为获取多样抓取需多次旋转输入点云并推理，耗时且影响质量。</p>
<p><strong>所存在的问题</strong>：<br>
现有学习方法在生成可行且多样的抓取姿势时面临挑战，条件生成模型受条件限制，推理时生成的抓取姿势相似；传统判别式模型预测能力有限。同时，将灵巧抓取生成视为集合预测任务时，存在优化难题，如模型易出现崩溃或预测的抓取姿势穿透物体的情况，影响抓取效果与模型性能。</p>
<p><strong>解决方法</strong>：<br>
提出Dexterous Grasp Transformer（DGTR）这一判别式框架，将灵巧抓取生成定义为集合预测任务，利用Transformer架构和可学习的抓取查询，一次前向传递就能预测多样的可行抓取姿势。针对优化难题，提出动态 - 静态匹配训练（DSMT）策略和对抗平衡测试时自适应（AB - TTA）策略。DSMT策略通过动态训练引导模型学习合适目标，再经静态训练优化物体穿透问题，提升优化稳定性；AB - TTA策略利用一对对抗损失在灵巧手参数空间中优化预测的抓取姿势，增强抓取质量并减少穿透。</p>
<p><strong>所用到的数据集</strong>：<br>
使用DexGraspNet数据集进行实验评估，该数据集包含133万多个ShadowHand对5355个物体的抓取数据，涵盖133多个物体类别。利用此数据集对DGTR框架的生成质量和多样性进行综合评估，通过多种评估指标验证了DGTR在生成高质量、多样抓取姿势方面的有效性。</p>
<h3><span id="英文题目forecasting-of-3d-whole-body-human-poses-with-grasping-objects"> 英文题目：Forecasting of 3D Whole-body Human Poses with Grasping Objects</span></h3>
<h3><span id="中文题目基于物体抓取的3d全身人体姿势预测"> 中文题目：基于物体抓取的3D全身人体姿势预测</span></h3>
<p><strong>研究背景</strong>：<br>
在计算机视觉和人机交互领域，预测3D人体姿势对理解人类行为、提升智能系统预测能力意义重大。过往研究在该领域虽有进展，但多聚焦于预测人体主要关节，忽视了手部的精细动作以及与物体的交互，而手部动作在人机交互中起着关键作用，能更精准地表达人体姿势，反映行为意图。</p>
<p><strong>所存在的问题</strong>：<br>
现有方法主要关注人体主要关节的预测，忽略了手部精细动作和物体交互。同时，在处理人体内部的异质性（如不同身体部位运动模式差异）和外部交互性（如人体与物体交互的动态变化）方面存在不足，难以实现精确的全身动作预测。</p>
<p><strong>解决方法</strong>：<br>
提出C³HOST（cross-context cross-modal consolidation for 3D whole-body pose forecasting）方法。先将人体划分为身体、左手和右手三个部分，分别提取时空信息，利用最大平均差异（MMD）进行特征对齐，减少特征异质性；接着通过循环交叉注意力机制捕捉身体各部分间的内部交互；然后计算人体与物体间的距离信息，利用门控共享单元调整其影响，并通过分布归一化在时间域对齐物体和人体关节特征；最后使用图注意力网络聚合多模态信息，学习人体与物体的交互，从而预测未来的全身动作序列。</p>
<p><strong>所用到的数据集</strong>：<br>
使用了GRAB和BEHAVE两个数据集。GRAB数据集包含约160万帧数据，由10名演员执行29种动作，通过高精度动作捕捉技术标注全身SMPL-X参数，定义了25个身体关节和每个手部15个关节，还包含50个物体的点云数据。BEHAVE数据集包含386个样本，由4个Kinect RGB-D相机以30fps的帧率捕获，涉及8个对象、20个物体和17种人机交互类型，每个姿势由67个关节的骨架表示 。</p>
<h3><span id="英文题目generate-subgoal-images-before-act-unlocking-the-chain-of-thought-reasoning-in-diffusion-model-for-robot-manipulation-with-multimodal-prompts"> 英文题目：Generate Subgoal Images before Act: Unlocking the Chain-of-Thought Reasoning in Diffusion Model for Robot Manipulation with Multimodal Prompts</span></h3>
<h3><span id="中文题目行动前生成子目标图像在多模态提示的机器人操作扩散模型中开启思维链推理"> 中文题目：行动前生成子目标图像：在多模态提示的机器人操作扩散模型中开启思维链推理</span></h3>
<p><strong>研究背景</strong>：<br>
具身操作致力于打造能在复杂环境中感知、推理和行动的通用机器人，视觉语言模型（VLM）的发展为机器人闭环控制带来潜力，但机器人在执行长时操作任务、理解复杂多模态提示时仍困难重重。复杂操作场景仅靠文本提示难以充分准确描述，而直接利用VLM预测动作序列易因缺乏中间子目标指导产生偏差，借助大语言模型（LLMs）分解的文本子提示又过于复杂冗余，增加理解难度。</p>
<p><strong>所存在的问题</strong>：<br>
一方面，机器人在处理长时操作任务时，由于缺乏中间子目标的指导，执行过程中的小错误容易累积，导致与原始任务指令产生较大偏差。另一方面，复杂操作场景的多模态提示难以用文本精确表述，现有方法在理解和遵循这些提示时存在不足，影响任务的完成效果 。</p>
<p><strong>解决方法</strong>：<br>
提出CoTDiffusion分层框架，将扩散模型作为高层视觉规划器。设计语义对齐模块，通过三重对齐架构和掩码补丁预测，实现粗到细的训练，捕捉视觉子目标与提示间的语义关联，跟踪生成图像进度，开启扩散模型的思维链推理能力。采用双向生成和帧连接机制，增强生成子目标图像的保真度和指令跟随准确性。利用生成的子目标图像指导底层基础模型进行动作规划，降低对基础模型能力的要求，使其专注于基本单物体操作原语</p>
<h3><span id="英文题目mcd-diverse-large-scale-multi-campus-dataset-for-robot-perception"> 英文题目：MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception</span></h3>
<h3><span id="中文题目mcd用于机器人感知的大规模多校园多样本数据集"> 中文题目：MCD：用于机器人感知的大规模多校园多样本数据集</span></h3>
<p><strong>研究背景</strong>：<br>
环境感知和自我运动估计在机器人应用中至关重要，公共数据集推动了相关研究，但现有标注的多模态大规模数据集存在局限性。这些数据集大多偏向自动驾驶场景，依赖昂贵的传统传感器，且经过精心处理，与现实场景存在差距。同时，新的感知技术如低成本的非重复周转（NRE）激光雷达和超宽带（UWB）技术虽带来机遇，但也面临扫描稀疏、视野有限和非视距观测等挑战，急需相应的数据集来支持研究。</p>
<p><strong>所存在的问题</strong>：<br>
现有标注的多模态大规模数据集在环境感知和自我运动估计研究中存在不足，如KITTI数据集缺乏IMU数据和环境多样性，Newer College Dataset存在地面真值误差和标注缺失等问题。此外，大多数数据集难以提供高精度的位置地面真值，且在涵盖多种感知模态、适应不同环境以及处理真实场景中的复杂因素方面存在欠缺。同时，针对新的感知技术如NRE激光雷达，相关的语义分割研究还比较匮乏。</p>
<p><strong>解决方法</strong>：<br>
提出MCD数据集，该数据集具有多方面优势。它包含多种传感模态，如经典旋转激光雷达、MEMS NRE激光雷达、传统相机、IMU和UWB传感器等；对59k的NRE激光雷达扫描进行了逐点语义标注，涵盖29个类别，为NRE激光雷达的语义分割研究提供了数据支持；数据采集来自欧亚三个大学校园，覆盖更广泛的纬度范围，增加了环境多样性；通过基于优化的激光雷达 - 惯性数据配准，生成连续时间的地面真值，提高了准确性；将现实场景中的各种挑战因素纳入数据，如运动失真、极端光照、玻璃反射和太阳干扰等，并在标注和图像中体现这些噪声类别，以训练机器人系统应对这些特殊情况。同时，提供了数据集、数据加载脚本和基准测试指令，方便研究人员对语义分割和SLAM算法进行研究和分析。</p>
<p><strong>所用到的数据集</strong>：<br>
MCD数据集包含18个序列，涵盖三个大学校园（每个校园6个序列，其中3个白天采集，3个夜晚采集），包含超过200k的激光雷达扫描数据、1500k的相机帧数据，以及高频的IMU和UWB数据。在语义标注方面，对11个序列的Livox激光雷达点云进行了标注，共划分29个语义类。此外，还提供了基于调查级先验地图配准生成的连续时间地面真值数据，用于支持机器人感知相关的研究和算法评估。</p>
<h3><span id="英文题目cyberdemo-augmenting-simulated-human-demonstration-for-real-world-dexterous-manipulation"> 英文题目：CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation</span></h3>
<h3><span id="中文题目cyberdemo增强模拟人类演示以实现现实世界中的灵巧操作"> 中文题目：CyberDemo：增强模拟人类演示以实现现实世界中的灵巧操作</span></h3>
<p><strong>研究背景</strong>：<br>
模仿学习在机器人操作领域颇具前景，它能让机器人通过人类演示获取复杂技能，但该方法高度依赖高质量的演示数据，而收集这些数据往往需要耗费大量人力，在多手指灵巧手操作任务中，数据收集的复杂性和精确性要求更高。传统观念认为，直接从真实机器人收集特定任务的演示数据是解决问题的最佳方式，但这种“黄金标准”面临挑战。利用模拟环境收集人类演示数据具有诸多优势，如无需真实硬件、可远程并行执行，还能通过数据增强提升任务表现。然而，将模拟环境中训练的策略转移到现实世界存在“模拟到现实”（sim2real）的挑战。</p>
<p><strong>所存在的问题</strong>：<br>
收集大规模高质量的演示数据困难重重，特别是针对高自由度的灵巧手操作任务。虽然数据增强可提高策略的泛化能力，但以往的方法多在图像层面进行，未充分考虑物理现实。此外，“模拟到现实”的转移问题一直是机器人学习领域的关键难题，传统的领域随机化和领域适应方法在解决高自由度灵巧手操作任务的动力学差距方面效果不佳。</p>
<p><strong>解决方法</strong>：<br>
提出CyberDemo框架，首先在模拟环境中通过低成本设备进行远程操作，收集人类演示数据；接着利用模拟器的优势，对原始演示数据进行多维度的数据增强，包括随机化相机视图、光照和纹理、添加多样物体以及随机化物体姿态等，同时考虑了视觉和动力学的变化，以提升策略对不同条件的鲁棒性；然后，使用自动课程学习和动作聚合的方法训练操作策略，根据任务成功率逐步增加数据增强的复杂度，并将小运动的步骤聚合以减少噪声影响；最后，利用少量真实演示数据对训练好的策略进行微调，以适应现实世界的操作。</p>
<p><strong>所用到的数据集</strong>：<br>
在模拟环境中，基于SAPIEN模拟器构建真实世界任务环境，使用相同的远程操作系统收集人类演示数据。在现实环境中，利用低成本远程操作系统收集数据，每个任务仅收集三分钟的机器人轨迹数据。为评估模型性能，设计了三个操作任务，包括两个准静态任务（抓取放置、倾倒）和一个非准静态任务（旋转），并针对每个任务设计了不同的数据增强和课程学习级别 。</p>
<h3><span id="英文题目smart-help-strategic-opponent-modeling-for-proactive-and-adaptive-robot-assistance-in-households"> 英文题目：Smart Help: Strategic Opponent Modeling for Proactive and Adaptive Robot Assistance in Households</span></h3>
<h3><span id="中文题目智能家居帮助面向家庭环境中主动适应性机器人辅助的战略对手建模"> 中文题目：智能家居帮助：面向家庭环境中主动适应性机器人辅助的战略对手建模</span></h3>
<p><strong>研究背景</strong>：<br>
随着社会的发展，对于弱势群体（如老人、儿童和残疾人）在日常任务中所需的辅助技术需求显著增加。然而，当前针对这些群体的高级AI驱动辅助解决方案的研究仍然较少，传统的人机交互往往忽略了人类的能力和情感需求，比如他们的实践机会、自我提升感和自尊心等。</p>
<p><strong>所存在的问题</strong>：<br>
以往的人机交互研究主要集中在简单的合作上，而现有的辅助技术通常只是接管一切，忽视了用户的情感福祉。提供帮助时缺乏对接受者情感接受度的敏感性和考虑，这导致了一个新的挑战：如何不仅关注任务的成功完成，还要注重接收者对帮助的情感接受度。</p>
<p><strong>解决方法</strong>：<br>
提出了一个名为“Smart Help”的新挑战，旨在为具有多样化的残疾和动态目标的人类代理在不同任务和环境中提供既主动又适应的支持。通过利用AI2-THOR建立了一个新型互动3D真实家居环境，并引入了一个创新的对手建模模块来优化辅助机器人的帮助策略。该模型不仅能推断用户的目标分布，还能对其每个目标独立完成的能力进行推理，从而选择性地解决那些阻碍任务完成的关键瓶颈。</p>
<p><strong>所用到的数据集</strong>：<br>
本研究基于AI2-THOR构建了一个多智能体互动环境，这是一个专为真实的家庭模拟设计的三维互动环境。此外，为了更好地评估Smart Help任务中的性能，还提出了一系列评估指标，并建立了包括联合目标与能力推断、瓶颈推理以及帮助策略改进在内的基准模型。实验验证了模型组件的有效性，并展示了该整体方法相对于基线的优越性。所有环境、数据集和代码都可通过<a target="_blank" rel="noopener" href="https://github.com/caozh20/SmartHelp%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/caozh20/SmartHelp获取。</a></p>
<h3><span id="英文题目rapid-motor-adaptation-for-robotic-manipulator-arms"> 英文题目：Rapid Motor Adaptation for Robotic Manipulator Arms</span></h3>
<h3><span id="中文题目机器人机械臂的快速运动适应"> 中文题目：机器人机械臂的快速运动适应</span></h3>
<p><strong>研究背景</strong>：<br>
随着计算机视觉和高级规划的发展，灵巧操作物体（即低级控制技能）仍是创建可帮助完成一般操作任务机器人的主要障碍之一。经典的机器人操作方法依赖精确模型，创建模型的复杂性较高；许多强化学习方法样本效率低且泛化能力差。为解决这些问题，Kumar等人提出快速运动适应（RMA），在四足机器人locomotion方面取得成效。</p>
<p><strong>所存在的问题</strong>：<br>
将RMA应用于一般操作任务并不简单，因为操作任务目标和行为因物体特征而异，仅靠本体感觉不足以完成任务，还需要抓取前的视觉推理。此外，现有方法在处理物体变化、样本效率和泛化能力等方面存在不足。</p>
<p><strong>解决方法</strong>：<br>
提出类别和实例字典作为几何感知操作的强代理，用于学习不可跨物体转移的策略；使用深度卷积神经网络估计环境的部分特权信息，隐式地进行物体类别和实例分类；首次将快速运动适应应用于机器人手臂的一般物体操作任务；统一快速运动适应两个学习阶段的目标形式化。通过在ManiSkill2环境中训练策略，利用近端策略优化和标准反向传播分别优化策略训练阶段和适配器训练阶段的目标函数。</p>
<p><strong>所用到的数据集</strong>：<br>
YCB数据集（78个物体）、EGAD数据集（2281个物体）、PartNet-Mobility数据集（60个物体用于水龙头转动任务） 。</p>
<h3><span id="英文题目dexart-benchmarking-generalizable-dexterous-manipulation-with-articulated-objects"> 英文题目：DexArt: Benchmarking Generalizable Dexterous Manipulation with Articulated Objects</span></h3>
<h3><span id="中文题目dexart使用关节物体进行可泛化灵巧操作的基准测试"> 中文题目：DexArt：使用关节物体进行可泛化灵巧操作的基准测试</span></h3>
<p><strong>研究背景</strong>：<br>
为实现家用机器人像人类一样操作日常关节物体，灵巧操作学习至关重要。当前机器人操作多依赖平行夹爪，限制了可操作物体范围；虽强化学习在灵巧操作有进展，但多聚焦单刚体操作。同时，现有机器人操作基准测试存在不足，如MetaWorld未考虑跨物体实例的泛化，ManiSkill受平行夹爪限制任务和操作方式。</p>
<p><strong>所存在的问题</strong>：<br>
学习灵巧操作因机器人手的高自由度关节而极具挑战，操作多样关节物体增加了关节自由度的复杂性，且在测试时对未见物体的泛化存在困难，这是强化学习的主要瓶颈。此外，现有基准测试无法很好地支持研究可泛化的灵巧操作技能以及视觉感知对决策的影响。</p>
<p><strong>解决方法</strong>：<br>
提出DexArt基准测试，定义多个复杂操作任务，使用Allegro Hand在模拟环境中操作关节物体。采用强化学习结合可泛化的视觉表征学习，以3D点云作为观察数据，利用PointNet编码器提取视觉表征辅助决策。通过实验研究不同方法和设置，如改变训练物体数量、视觉骨干网络大小、预训练方法等，探究其对策略学习性能的影响。</p>
<p><strong>所用到的数据集</strong>：</p>
<ul>
<li><strong>DexArt Manipulation Dataset (DAM)</strong>：渲染的点云观测数据，包含每个物体6k个点云（包括观测和想象点云），机器人和关节物体状态随机采样，用于分割预训练时标注为4个类别。</li>
<li><strong>PartNet-Mobility Manipulation Dataset (PMM)</strong>：直接从PartNet-Mobility渲染，无任务信息，含46个物体类别，每个类别1k个点云，物体状态和相机视点随机采样，用于分类和分割任务。</li>
</ul>
<h3><span id="英文题目target-referenced-reactive-grasping-for-dynamic-objects"> 英文题目：Target-referenced Reactive Grasping for Dynamic Objects</span></h3>
<h3><span id="中文题目针对动态物体的目标参考反应式抓取"> 中文题目：针对动态物体的目标参考反应式抓取</span></h3>
<p><strong>研究背景</strong>：<br>
反应式抓取在工业领域需求大，如在人机协作场景中，机器人若能实现反应式抓取可减轻工人压力。然而，与静态环境下的抓取不同，动态任务设置对算法设计提出新挑战。此前研究多聚焦于规划时间上平滑的抓取动作，却很少关注语义一致性，即确保机器人在后续帧中抓取物体的同一部分，并且在杂乱场景中，经典方法预测的抓取位置也不一定在同一物体上。</p>
<p><strong>所存在的问题</strong>：<br>
现有方法在反应式抓取时，难以同时保证抓取动作在时间上的平滑性和语义上的一致性。6D姿态跟踪用于解决该问题时存在缺陷，一方面，其依赖实例分割，推理速度慢，无法满足实时性要求；另一方面，常需要物体的先验知识（如CAD模型），在现实中可能无法获取，且泛化能力有限。</p>
<p><strong>解决方法</strong>：<br>
提出在目标参考设置下通过跟踪生成的抓取空间来解决反应式抓取问题，具体包含两个阶段。首先，利用注意力图神经网络发现抓取姿态对应关系，并选择与目标姿态相似度最高的抓取姿态；其次，基于目标和历史信息对选定的抓取姿态进行优化。利用现成的抓取检测器生成抓取候选集，通过特定的特征表示和注意力图神经网络进行抓取特征聚合与对应估计，同时使用记忆增强的优化网络结合历史信息优化抓取姿态。</p>
<p><strong>所用到的数据集</strong>：</p>
<ul>
<li><strong>GraspNet-1Billion数据集</strong>：大规模真实世界抓取检测数据集，包含89个物体、190个场景以及每个场景256个相机视图，用于训练模型。</li>
<li><strong>Moving GraspNet数据集</strong>：自行收集的包含30个动态物体场景的测试集，记录了每帧的RGBD图像和物体6D姿态，并手动标注了第一帧的10个抓取目标，后续帧的标注通过6D姿态投影得到。</li>
</ul>
<h3><span id="英文题目arctic-a-dataset-for-dexterous-bimanual-hand-object-manipulation"> 英文题目：ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation</span></h3>
<h3><span id="中文题目arctic用于灵巧双手操作物体的数据集"> 中文题目：ARCTIC：用于灵巧双手操作物体的数据集</span></h3>
<p><strong>研究背景</strong>：<br>
人类在日常生活中频繁操作复杂物体，物体的运动或变形往往源于人类施加的外力。然而，在手部姿态估计领域，对操作过程中手和物体物理一致性动力学的研究较少。现有的手 - 物体数据集大多局限于刚性物体的抓取，很少包含对关节物体丰富且灵巧的操作示例，缺乏用于研究手和关节物体物理一致且同步运动的具有真实3D注释的数据集。</p>
<p><strong>所存在的问题</strong>：<br>
现有数据集无法满足研究手与关节物体物理一致且同步运动的需求，其局限性体现在以下方面：多聚焦于刚性物体抓取，缺乏关节物体操作数据；手部姿态变化范围有限，多为静态抓取；数据采集受遮挡影响大，难以获取高质量3D注释；难以研究双手灵巧操作和动态手 - 物体接触等。此外，现有从RGB图像估计3D手和物体的方法，以及手 - 物体接触检测方法，在处理关节物体复杂交互时存在不足。</p>
<p><strong>解决方法</strong>：<br>
构建ARCTIC数据集，通过精确的Vicon动作捕捉系统和多视图RGB系统同步采集数据，获取10名受试者与11个关节物体交互的210万帧RGB图像，并为每帧图像匹配准确的3D手和物体网格以及详细的动态接触信息。针对数据集提出两个新任务：一致运动重建，利用ArcticNet模型，通过编码器 - 解码器架构估计双手和关节物体的参数，实现从单目视频中重建3D运动，使其在时空上保持一致；交互场估计，引入InterField模型，通过CNN和PointNet估计每帧图像中手部顶点与物体间的最短距离，以此研究手 - 物体的相对空间关系。</p>
<p><strong>所用到的数据集</strong>：<br>
ARCTIC数据集包含10名受试者对11个关节物体进行灵巧操作的339个序列，210万张来自8个静态视图和1个第一人称视角的RGB图像，并配有3D手和物体网格。数据采集使用Vicon动作捕捉系统和多视图RGB系统，通过获取模板几何形状、估计旋转轴、捕捉交互、求解姿态和计算接触等步骤完成。该数据集可用于研究双手与关节物体的交互，为相关任务提供了丰富的数据支持。</p>
<h3><span id="英文题目flex-full-body-grasping-without-full-body-grasps"> 英文题目：FLEX: Full-Body Grasping Without Full-Body Grasps</span></h3>
<h3><span id="中文题目flex无需全身抓取数据的全身抓取姿态生成方法"> 中文题目：FLEX：无需全身抓取数据的全身抓取姿态生成方法</span></h3>
<p><strong>研究背景</strong>：<br>
生成与场景真实交互的3D虚拟人在AR/VR、视频游戏和机器人等领域具有重要意义。当前研究依赖大规模3D数据集来生成虚拟人，但数据收集存在诸多困难，如使用光学标记的动作捕捉系统收集数据繁琐，涉及物体和场景时更复杂，且难以涵盖所有与物理世界交互的可能方式，导致基于特定任务数据集训练的模型泛化能力差、生成的全身姿态多样性有限。</p>
<p><strong>所存在的问题</strong>：<br>
现有生成虚拟人全身抓取姿态的方法存在明显不足。一方面，依赖数据收集的方法难以扩展到更广泛场景，无法适应物体位置、方向变化以及场景中存在家具等复杂情况；另一方面，这些方法生成的全身姿态多样性严重受限，难以满足现实场景中多样化的需求。</p>
<p><strong>解决方法</strong>：<br>
提出FLEX框架，该框架利用现有的手部抓取模型和人体姿态先验，通过几何和解剖约束将两者结合，在不使用任何3D全身抓取数据的情况下生成全身抓取姿态。具体通过在手部抓取模型和人体姿态先验模型的潜在空间中搜索合适的潜在变量，同时引入障碍物避免损失、注视损失等，并结合姿态 - 地面先验来优化生成的人体姿态，使其满足自然、合理且不与障碍物相交的要求。此外，还设计了一个映射网络，将多个相互依赖的参数控制问题转化为对单个可控潜在向量的优化。</p>
<p><strong>所用到的数据集</strong>：</p>
<ul>
<li><strong>ReplicaGrasp数据集</strong>：为了使全身物体抓取任务更具挑战性和现实代表性而构建。该数据集包含来自GRAB的50个日常物体，分布在ReplicaCAD的48个容器中，通过Habitat模拟器模拟物体处于各种可行位置，共产生4800个实例。这些实例中，物体所在的家具被视为障碍物，用于评估生成的全身人类抓取姿态是否具有“场景感知”能力，即不与障碍物相交，同时还要自然可行。</li>
<li><strong>GRAB数据集</strong>：是一个通过动作捕捉收集的人体与日常物体交互的数据集，但该数据集中的物体交互场景较为单一，缺乏多样性。FLEX在该数据集上进行实验，以对比评估其在不使用全身抓取数据的情况下，与其他基于该数据集训练的方法的性能差异。</li>
</ul>
<h3><span id="英文题目robot-structure-prior-guided-temporal-attention-for-camera-to-robot-pose-estimation-from-image-sequence"> 英文题目：Robot Structure Prior Guided Temporal Attention for Camera-to-Robot Pose Estimation from Image Sequence</span></h3>
<h3><span id="中文题目基于机器人结构先验引导的时间注意力机制用于从图像序列估计相机到机器人的位姿"> 中文题目：基于机器人结构先验引导的时间注意力机制用于从图像序列估计相机到机器人的位姿</span></h3>
<p><strong>研究背景</strong>：<br>
在机器人执行自主任务过程中，相机到机器人的位姿估计是关键环节，其结果直接决定机器人在环境中的精准定位与操作能力。传统方法，如采用AR标签进行手眼校准，存在诸多弊端，包括关节配置与图像捕获过程繁琐、无法实现线上实时校准等，在需要频繁调整相机位置的下游任务中，这些问题尤为突出。基于视觉的位姿估计方法为在线手眼校准带来了新的可能，但现有的大多数基于学习的单帧估计方法，因受单视图图像模糊性和机器人自遮挡现象的影响，导致位姿估计性能不佳。</p>
<p><strong>所存在的问题</strong>：<br>
经典的相机到机器人位姿估计方法存在显著局限性，难以满足实际应用需求。基于单帧图像的视觉估计方法，在机器人发生自遮挡时，由于单视图图像信息不完整、存在模糊性，无法准确获取位姿信息。此外，如何有效融合时间序列信息，以提升位姿估计的准确性和鲁棒性，是当前研究面临的重要挑战。现有方法在处理时间信息时，往往不能充分利用图像序列中的时间关联，导致位姿估计结果不稳定，难以适应动态变化的环境。</p>
<p><strong>解决方法</strong>：<br>
提出SGTAPose方法，该方法通过引入机器人结构先验和时间注意力机制，实现从图像序列中准确估计相机到机器人的位姿。具体而言：</p>
<ul>
<li><strong>结构先验引导特征对齐</strong>：根据前一帧估计的位姿生成重投影置信图，以此引导网络聚焦于当前帧中关键点的残差信息，实现帧间特征的精准对齐，有效利用机器人结构先验知识增强特征的一致性和准确性。</li>
<li><strong>时间交叉注意力增强融合</strong>：针对不同分辨率的特征图，采用差异化的融合策略，充分挖掘时间序列中各帧之间的关联信息，通过时间交叉注意力机制实现特征的高效融合，提升对时间信息的利用效率。</li>
<li><strong>位姿精炼优化</strong>：设计位姿精炼器，通过重加权PnP问题求解，降低异常关键点对相机到机器人位姿计算的干扰，对初始位姿进行优化，进一步提高位姿估计的精度和可靠性。<br>
此外，构建大规模合成数据集，并运用域随机化技术，有效缩小模拟数据与真实数据之间的差距，为模型训练提供丰富且具有代表性的数据，提升模型的泛化能力和适应性。</li>
</ul>
<p><strong>所用到的数据集</strong>：</p>
<ul>
<li><strong>Panda Syn Training</strong>：借助Blender工具生成的大规模合成训练集，包含约60k个视频，每个视频包含3个连续帧，总计180k张图像。该数据集通过多种域随机化手段，模拟真实场景中的各种变化，包含时间序列的RGB图像、2D/3D预定义关键点位置以及部分位姿信息，主要用于模型的训练过程，帮助模型学习不同场景下的位姿估计模式。</li>
<li><strong>Panda Syn Testing</strong>：同样为作者生成的合成测试集，包含347个视频，每个视频有30个连续帧，共10k张图像，用于评估模型在合成数据上的性能表现，检验模型对合成场景的适应能力和位姿估计准确性。</li>
<li><strong>Panda 3CAM-RS</strong>：由外部安装的Intel RealSense D415相机拍摄Franka Emika Panda机械臂得到的真实世界测试集，包含约6k张图像，用于测试模型在真实场景下的位姿估计效果，验证模型从模拟到真实场景的迁移能力。</li>
<li><strong>Panda 3CAM-AK</strong>：属于真实世界测试集，由Microsoft Azure Kinect相机采集，包含约6k张图像，用于评估模型在不同相机采集数据下的性能差异，考察模型对不同传感器数据的适应性。</li>
<li><strong>Panda Orb</strong>：由Intel RealSense D415相机从27个不同视图拍摄所得，包含约32k张图像，是一个多样化的真实数据集，可用于全面评估模型性能、开展消融实验，以及测试模型在自遮挡等复杂场景下的鲁棒性 。</li>
</ul>
<h3><span id="英文题目learning-human-to-robot-handovers-from-point-clouds"> 英文题目：Learning Human-to-Robot Handovers from Point Clouds</span></h3>
<h3><span id="中文题目从点云学习人机交接"> 中文题目：从点云学习人机交接</span></h3>
<p><strong>研究背景</strong>：<br>
人机交接在人机交互中至关重要，可助力机器人在日常协作与生产制造中发挥作用。然而，实现该任务颇具挑战，机器人需依据有限的视觉输入对人类行为做出反应。当前具身人工智能倾向于在模拟环境中训练机器人，但模拟人类的难度阻碍了人机交互任务的进展。尽管已有研究引入逼真的模拟环境（如HandoverSim），但尚未充分探索在该环境中进行带人类参与的策略训练。</p>
<p><strong>所存在的问题</strong>：<br>
一方面，传统基于模型的抓取规划方法依赖物体的3D形状模型，难以处理未见物体；另一方面，基于学习的方法在处理动态交互场景时存在不足，如需要复杂的手工设计成本函数，且在机器人和人类同时运动的情况下，难以通过开环运动规划器获得有效的专家演示来指导训练，同时现有方法在模拟到现实的迁移方面也表现不佳。</p>
<p><strong>解决方法</strong>：<br>
提出一种基于视觉的人机交接学习框架，通过两阶段师生训练框架进行训练。在第一阶段，固定人类为静止状态，利用运动和抓取规划获取专家演示，辅助强化学习策略训练；第二阶段，在人类和机器人同时运动的动态场景中，以预训练的策略为教师，对下游策略进行微调。网络输入为手腕摄像头获取的点云，经感知模块处理后，由视觉控制模块预测机器人动作和抓取时机。采用PointNet++进行点云特征编码，利用TD3算法进行策略学习，并结合行为克隆损失、标准演员 - 评论家损失和辅助目标损失来优化网络。</p>
<p><strong>所用到的数据集</strong>：</p>
<ul>
<li><strong>DexYCB数据集</strong>：包含大量人类与物体交互序列，HandoverSim利用其轨迹在模拟环境中驱动虚拟人类的运动，为训练和评估人机交接任务提供了多样化的人类手部运动和物体交互数据。</li>
<li><strong>HandoverSim基准测试场景</strong>：包含1000个独特的人机交接场景，分为训练集、验证集和测试集。每个场景都有独特的人类交接动作，用于评估不同方法在模拟环境中的人机交接性能，作者利用该基准测试场景对提出的方法进行训练和评估。</li>
</ul>
<h3><span id="英文题目markerless-camera-to-robot-pose-estimation-via-self-supervised-sim-to-real-transfer"> 英文题目：Markerless Camera-to-Robot Pose Estimation via Self-supervised Sim-to-Real Transfer</span></h3>
<h3><span id="中文题目通过自监督模拟到真实迁移的无标记相机到机器人位姿估计"> 中文题目：通过自监督模拟到真实迁移的无标记相机到机器人位姿估计</span></h3>
<p><strong>研究背景</strong>：<br>
在基于视觉的机器人控制中，相机到机器人的位姿估计是关键环节，位置基视觉伺服（PBVS）依赖它将环境信息转换到机器人坐标系以实现操作任务。传统方法通过在机器人上附加标记来校准位姿，但存在无法在线校准、易受环境因素影响等问题，限制了机器人在复杂现实环境中的应用。深度学习的发展为无标记相机到机器人位姿估计带来新途径，主要分为基于关键点和基于渲染的方法，但前者受模拟到现实差距的限制，后者计算时间长，不适用于动态场景。</p>
<p><strong>所存在的问题</strong>：<br>
基于关键点的方法，其性能受限于在模拟环境中训练的关键点检测器，难以适应真实场景；基于渲染的方法，迭代渲染和比较的过程耗时耗能，不适合动态场景下的在线估计。此外，获取真实世界数据的3D标注成本高、工作量大，主流深度学习方法依赖合成数据和域随机化来缩小模拟与真实之间的差距，但效果有限。</p>
<p><strong>解决方法</strong>：<br>
提出CtRNet，一种端到端的位姿估计框架。先在合成数据上预训练网络，学习分割机器人和估计位姿的知识。接着，利用前景分割和可微渲染进行自监督训练，实现从模拟到真实的迁移。自监督训练通过最小化渲染的机器人轮廓图像与分割掩码之间的差异来优化神经网络参数，其中前景分割为位姿估计提供监督。在推理时，CtRNet利用关键点检测的快速速度和基于渲染方法的高性能，通过ResNet50作为骨干网络提取特征，经一系列操作生成机器人掩码和关键点，再结合机器人关节角度通过PnP求解器估计位姿，并利用隐函数定理实现PnP求解器的反向传播以训练关键点检测器。</p>
<p><strong>所用到的数据集</strong>：</p>
<ul>
<li><strong>DREAM-real数据集</strong>：包含约50K张由Azure Kinect、XBOX 360 Kinect和RealSense这3种不同相机拍摄的Franka Emika Panda机械臂的RGB图像，分辨率为(640×480)，每个图像帧都提供了相机到机器人位姿的真实值。使用平均距离（ADD）和曲线下面积（AUC）指标评估方法的准确性，用于评估CtRNet在该数据集上的性能，并与其他先进方法进行比较。</li>
<li><strong>Baxter数据集</strong>：包含100张由Azure Kinect相机拍摄的Rethink Baxter左臂的RGB图像，分辨率为(2048×1526)，提供了相对于相机帧的2D和3D末端执行器位置的真实值。采用ADD指标评估末端执行器的位姿估计性能，使用正确关键点百分比（PCK）指标评估末端执行器的重投影误差，用于研究不同预训练样本数量对自监督训练的影响，以及评估CtRNet在该数据集上的2D和3D性能表现。</li>
</ul>
<h3><span id="英文题目weakly-supervised-posture-mining-for-fine-grained-classification"> 英文题目：Weakly Supervised Posture Mining for Fine-grained Classification</span></h3>
<h3><span id="中文题目用于细粒度分类的弱监督姿态挖掘方法"> 中文题目：用于细粒度分类的弱监督姿态挖掘方法</span></h3>
<p><strong>研究背景</strong>：<br>
细粒度分类任务具有挑战性，因其数据集中不同子类间的视觉差异细微，难以识别。以往研究可分为三类，早期完全监督方法在训练和测试阶段都依赖边界框标注，成本高且易出错；后续方法仅在训练阶段使用标注；近期部分方法虽无需标注，但现有基于局部区域创建图的方法不易移植，且难以感知具有正确上下文信息的判别区域及区域间关系。</p>
<p><strong>所存在的问题</strong>：<br>
现有细粒度分类方法存在不足，完全监督方法标注成本高，弱监督方法在感知判别区域及其关系方面存在缺陷，难以有效挖掘姿态信息，导致分类性能受限。此外，传统交叉熵（CE）损失函数在细粒度分类中，因忽视负标签中的类间差异信息，无法充分发挥模型的分类能力。</p>
<p><strong>解决方法</strong>：<br>
提出PMRC（posture mining and reverse cross-entropy）框架，该框架可与不同骨干网络结合。利用深度导航器（Deep Navigator）从图像生成判别区域，构建图结构，并通过消息传递聚合图以获取分类结果。设计了一种新的训练范式，使深度导航器和消息传递相互通信和训练，以学习挖掘姿态信息。提出反向交叉熵（RCE）损失函数，通过反转softmax输出层的标签分数，学习类间差异信息，提升模型在细粒度分类上的性能。</p>
<p><strong>所用到的数据集</strong>：</p>
<ul>
<li><strong>CUB-200-2011</strong>：包含200种鸟类的11,788张图像，训练集5,994张，测试集5,794张。每张图像有详细标注，因其每个物种训练图像较少，被认为是竞争较为激烈的数据集。</li>
<li><strong>Stanford Cars</strong>：由196类汽车的16,185张图像组成，训练集和测试集近似50-50划分，类别通常为生产年份和型号级别。</li>
<li><strong>FGVC Aircraft</strong>：有102种不同飞机模型变体的10,200张图像，每个变体100张，图像中的飞机有紧密边界框和分层模型标签标注。</li>
<li><strong>Stanford Dogs</strong>：包含120类狗的20,580张图像，训练集12,000张，测试集8,580张 。</li>
</ul>
<h3><span id="英文题目nerf-in-the-palm-of-your-hand-corrective-augmentation-for-robotics-via-novel-view-synthesis"> 英文题目：NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis</span></h3>
<h3><span id="中文题目掌间nerf通过新视图合成实现机器人的矫正增强"> 中文题目：掌间NeRF：通过新视图合成实现机器人的矫正增强</span></h3>
<p><strong>研究背景</strong>：<br>
在基于视觉的机器人控制中，6自由度（6-DoF）物体抓取是关键问题。从专家演示中进行模仿学习是有前景的方法，但存在误差累积问题，复杂视觉任务常需在线专家监督或环境交互，成本高昂。同时，离线“反馈增强”方法未充分应用于视觉观测，而利用安装在机器人手腕上的手眼相机虽能提升性能，但仍无法解决根本问题。此外，深度传感器在处理透明或反光物体时存在局限性，基于视觉的6-DoF抓取策略发展面临挑战。</p>
<p><strong>所存在的问题</strong>：<br>
传统6-DoF抓取管道采用开环轨迹执行，无法利用感知反馈进行精确抓取。模仿学习在处理复杂视觉任务时，因协变量偏移导致策略易偏离数据分布，且纠正错误困难。现有离线反馈增强方法未涉及视觉观测，标准图像增强方法不修改动作标签，难以有效提升视觉策略性能。</p>
<p><strong>解决方法</strong>：<br>
提出SPARTN（Synthetic Perturbations for Augmenting Robot Trajectories via NeRF），这是一种基于NeRF的离线数据增强技术。利用NeRF为每个演示场景训练神经辐射场，通过向演示中的相机姿态注入噪声，并使用NeRF渲染新视图，同时计算矫正动作标签，将增强数据与原始演示数据结合训练反应式实时策略。该方法将矫正反馈增强扩展到视觉领域，能生成仅基于RGB的6-DoF抓取策略，且完全离线，无需额外专家监督或环境交互。</p>
<p><strong>所用到的数据集</strong>：</p>
<ul>
<li><strong>模拟6-DoF抓取基准数据集</strong>：包含2500个抓取1500个ShapeNet物体的演示，用于评估SPARTN和相关方法。策略在该数据集上接收来自手腕相机的RGB、RGBD或点云观测，并控制夹爪在6-DoF末端执行器空间中进行相对姿态变化。评估时使用YCB或ShapeNet数据集中未在训练中出现的物体，每个物体在不同初始机器人配置和物体初始姿态下测试10次。</li>
<li><strong>真实世界实验数据集</strong>：使用Franka Emika Panda机器人在8个不同环境中进行抓取实验，任务是抓取具有不同几何形状、反射性、透明度和径向对称等特征的目标物体。每个环境中收集少量专家抓取演示，用于训练和评估SPARTN和普通行为克隆（BC）策略，评估时随机化物体和机器人的初始配置。</li>
</ul>
<h3><span id="英文题目pypose-a-library-for-robot-learning-with-physics-based-optimization"> 英文题目：PyPose: A Library for Robot Learning with Physics-based Optimization</span></h3>
<h3><span id="中文题目pypose基于物理优化的机器人学习库"> 中文题目：PyPose：基于物理优化的机器人学习库</span></h3>
<p><strong>研究背景</strong>：<br>
深度学习在机器人感知领域成果显著，但在泛化能力上存在不足，易受训练数据、环境变化和计算资源的限制。物理优化方法虽具有良好的泛化能力和较高精度，但依赖手动参数调整且缺乏高级语义信息。目前，学习方法和物理优化通常在机器人系统的不同模块中单独使用，这种解耦的范式难以达到最优解，限制了系统性能和泛化能力。同时，开发端到端可微的集成方法成为趋势，但现有的实现方式多为特定问题编写，缺乏统一框架，不同语言库的混合使用增加了系统复杂性，阻碍了研究进展。</p>
<p><strong>所存在的问题</strong>：<br>
深度学习和物理优化方法各自存在局限性，两者结合的现有方式多为特定问题编写，缺乏统一框架，不同语言库的混合使用导致系统复杂，开发周期长。此外，现有的相关开源库也存在不足，如LieTorch仅支持一阶可微操作，CvxpyLayer不支持李群操作和二阶优化器，Theseus采用旋转矩阵表示变换，内存效率低。</p>
<p><strong>解决方法</strong>：<br>
提出PyPose，这是一个基于PyTorch的面向机器人的开源库，旨在将深度感知模型与基于物理的优化相结合。通过定义LieTensor来表示3D变换，解决了在学习模型中3D变换的梯度计算和数值问题，支持任意阶梯度自动求导和并行计算，且内存效率高。利用PyTorch的Function和Module概念实现可微的机器人相关功能，提供多种实用模块。集成多种优化器，支持二阶优化，如Levenberg-Marquardt算法，并引入FastTriggs校正器提高稳定性，同时支持多种策略来限制优化步长。</p>
<p><strong>所用到的数据集</strong>：</p>
<ul>
<li><strong>TartanAir数据集</strong>：用于自监督学习视觉SLAM前端的实验。在实验中，利用估计的姿态和地标位置作为伪标签进行监督，对在MegaDepth上预训练的CAPS网络在TartanAir数据集上进行微调，以评估PyPose在视觉SLAM后端优化中的可行性，通过实验验证了自监督训练后CAPS模型在两帧匹配任务中的匹配精度提高。</li>
<li><strong>KITTI数据集</strong>：用于IMU预积分和IMU-centric PGO（姿态图优化）的实验展示。通过在该数据集上的实验，展示了PyPose的IMUPreintegrator模块在处理IMU数据时，结合PGO能够有效减少轨迹漂移，验证了其在惯性导航中的有效性。</li>
<li><strong>EuRoC数据集</strong>：用于训练和评估IMU校准网络。通过在该数据集上的实验，验证了使用PyPose的IMUPreintegrator模块训练的IMU校准网络，在姿态估计的精度上相较于传统方法有显著提升，证明了PyPose在基于学习的多模态SLAM和惯性导航系统研究中的优势。</li>
</ul>
<h3><span id="英文题目unidexgrasp-universal-robotic-dexterous-grasping-via-learning-diverse-proposal-generation-and-goal-conditioned-policy"> 英文题目：UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy</span></h3>
<h3><span id="中文题目unidexgrasp通过学习多样提案生成和目标条件策略实现通用机器人灵巧抓取"> 中文题目：UniDexGrasp：通过学习多样提案生成和目标条件策略实现通用机器人灵巧抓取</span></h3>
<p><strong>研究背景</strong>：<br>
机器人抓取是机器人与环境交互的基础能力，在平行夹爪抓取算法取得进展的同时，灵巧抓取因能实现复杂和精细的物体操作而愈发重要。但灵巧手的高自由度使得生成有效抓取姿势和规划执行轨迹难度增大，现有工作在解决该问题时，或依赖理想输入，或难以在现实任务设置中实现通用化和多样化的灵巧抓取。</p>
<p><strong>所存在的问题</strong>：<br>
现有方法在生成高质量且多样的灵巧抓取姿势以及执行通用的灵巧抓取任务方面存在不足。在抓取姿势生成阶段，基于学习的方法难以兼顾质量和多样性，如使用条件变分自编码器（CVAE）的方法易出现模式崩溃问题。在抓取执行阶段，传统分析方法需对模型进行简化，强化学习和模仿学习方法依赖物体网格的全知知识，难以在现实任务设置中部署，且现有方法均无法在原始视觉输入下对大量物体实现通用化抓取。</p>
<p><strong>解决方法</strong>：<br>
提出UniDexGrasp，将任务分解为两个阶段。在抓取提案生成阶段，设计一种条件抓取姿势生成模型，将旋转与平移和关节角度解耦，分别使用GraspIPDF和GraspGlow进行建模，再通过ContactNet优化采样的抓取姿势，以生成多样且高质量的抓取提案。在目标条件抓取执行阶段，采用教师 - 学生学习框架，先使用近端策略优化（PPO）算法结合状态规范化、对象课程学习等技术学习一个能获取理想状态输入的教师策略，再使用DAgger算法将其蒸馏为仅依赖现实输入的学生策略。</p>
<p><strong>所用到的数据集</strong>：<br>
通过合成的方式生成了大规模的数据集，包含133个类别中5519个物体实例的112万个有效抓取。这些物体被划分为训练实例（3251个）、已见类别未见实例（754个）和未见类别实例（1514个），用于训练和评估模型在不同情况下的抓取性能。</p>
<h3><span id="英文题目affordances-from-human-videos-as-a-versatile-representation-for-robotics"> 英文题目：Affordances from Human Videos as a Versatile Representation for Robotics</span></h3>
<h3><span id="中文题目从人类视频中学习可供性作为机器人的通用表示"> 中文题目：从人类视频中学习可供性作为机器人的通用表示</span></h3>
<p><strong>研究背景</strong>：<br>
机器人领域期望构建能通过观察人类进行理解和学习交互的机器人，这推动了视觉领域对相关问题的研究。从人类视频中提取可供性知识的研究虽在静态数据集上取得一定成果，但在与机器人系统集成方面存在不足，如不清楚如何定义和表示可供性，也缺乏在真实机器人和野外环境中的实验评估。而机器人学习方法在面对新任务或新环境时，通常需要从头开始学习，即便有视觉预训练，也难以应对复杂的状态空间和确定具体的操作方式。</p>
<p><strong>所存在的问题</strong>：<br>
目前，视觉可供性学习和机器人学习之间存在明显差距。一方面，现有可供性学习方法多在人类视频数据集上测试，未与机器人系统集成，无法明确可供性的有效定义和表示，也难以评估其性能；另一方面，机器人学习在面对新环境和任务时，缺乏有效的先验知识，难以从视觉输入中直接获取如何操作物体的信息，且复杂的状态空间使得学习难度增大。</p>
<p><strong>解决方法</strong>：<br>
提出Vision-Robotics Bridge (VRB)方法，旨在弥合视觉与机器人之间的差距。采用接触点和接触后轨迹作为视觉可供性的可操作表示，这种表示以机器人为中心，便于机器人使用。利用大规模以自我为中心的人类交互视频，通过机器人优先的方法提取可供性，包括利用现成工具估计自我运动、人体姿态和手 - 物体交互来获取监督信息，并处理相机运动和视觉领域转移问题。将学习到的可供性与多种机器人学习范式（离线模仿学习、探索、目标条件学习和动作参数化强化学习）无缝集成，以加速机器人在野外的学习。</p>
<p><strong>所用到的数据集</strong>：<br>
文中未明确提及具体的数据集名称，但提到利用互联网上的人类行为视频进行模型训练。在实验阶段，使用两个不同的机器人平台（Franka Emika Panda手臂和Hello Stretch移动操作器）在多个环境中进行实验，涵盖多种任务场景，如与橱柜、刀具、蔬菜、架子、锅、门、盖子、抽屉、洗碗机、垃圾桶等物体的交互任务。针对每个任务，通过估计任务空间图像裁剪区域，并使用校准的机器人 - 相机系统（Intel RealSense D415i）将预测的接触点和轨迹投影到3D空间用于机器人控制。</p>

      
    </div>
    <div class="article-footer">
      <!-- <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="http://example.com/2025/04/21/paper_Robot_cvpr_list/" title="PaperReading-Robot_list-CVPR" target="_blank" rel="external">http://example.com/2025/04/21/paper_Robot_cvpr_list/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote> -->


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://github.com/chandlerye" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/favicon.ico" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://github.com/chandlerye" target="_blank"><span class="text-dark">Chandler</span><small class="ml-1x">一头牛马</small></a></h3>
        <div>江上两条红船，寒风斜雨摇摆</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/2025/04/21/paper_Robot_tro_list/" title="PaperReading-Robot_list-TRO"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/2025/04/07/coding_RL__isaacsim_demo_of_car_and_robot/" title="Coding-isaacsim-控制-Franka和Carter机器人"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  
  <div class="bar-right">
    
  </div>
  </div>
</nav>
  


</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/chandlerye" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>



  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   




   






</body>
</html>