{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"分类","date":"2023-08-30T09:13:13.000Z","updated":"2023-08-30T09:26:25.168Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2023-08-30T09:14:13.000Z","updated":"2023-08-30T09:27:18.175Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"Sylar项目-日志系统","date":"2024-01-30T02:57:51.676Z","updated":"2024-01-02T07:43:16.629Z","comments":true,"path":"draft/coding_sylar_data_system.html","permalink":"http://example.com/draft/coding_sylar_data_system.html","excerpt":"","text":"A Review on Cross-modality Synthesis from MRI to PET 1.摘要 2.引言 3.论文结构 4.总结"},{"title":"Note-ALL","date":"2024-01-30T09:21:20.595Z","updated":"2024-01-30T09:21:20.595Z","comments":true,"path":"draft/note-all.html","permalink":"http://example.com/draft/note-all.html","excerpt":"","text":"JS散度 一般地，JS散度是对称的，其取值是 0 到 1 之间。如果两个分布 P,Q 离得很远，完全没有重叠的时候，即使两个分布的中心离的很近，而JS散度值是一个常数。这在学习算法中是比较致命的，这就意味这这一点的梯度为 0。梯度消失了。"},{"title":"衡量分布差异性的损失函数","date":"2024-01-30T10:00:11.726Z","updated":"2024-01-30T10:00:11.726Z","comments":true,"path":"draft/note-distant.html","permalink":"http://example.com/draft/note-distant.html","excerpt":"","text":"EMD 对于离散的概率分布，Wasserstein距离也被描述为推土距离(EMD)。如果我们将分布想象为两个有一定存土量的土堆，那么EMD就是将一个土堆转换为另一个土堆所需的最小总工作量。"},{"title":"Note-特征融合代码","date":"2024-02-04T01:47:09.506Z","updated":"2024-02-04T01:47:09.506Z","comments":true,"path":"draft/note-feature_fusion.html","permalink":"http://example.com/draft/note-feature_fusion.html","excerpt":"","text":"特征不融合 Feature不融合，多尺度的feture分别进行预测，然后对预测结果进行综合，如Single Shot MultiBox Detector (SSD) , Multi-scale CNN(MS-CNN) 多尺度特征融合（Multi-scale Feature Fusion） 金字塔池化（Pyramid Pooling） 论文：Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition 特征金字塔网络（Feature Pyramid Networks） 论文：Feature Pyramid Networks for Object Detection 短连接（Short Connections） 论文：Deeply Supervised Salient Object Detection with Short Connections 密集连接的卷积网络（Densely Connected Convolutional Networks） 论文：Densely Connected Convolutional Networks 双路径网络（Dual Path Networks） 论文：Dual Path Networks 残差连接（Residual Connections） 论文：Deep Residual Learning for Image Recognition 注意力机制（Attention Mechanism） 论文：Show, Attend and Tell: Neural Image Caption Generation with Visual Attention Non-local Neural Networks CBAM: Convolutional Block Attention Module 胶囊网络（Capsule Networks） Dynamic Routing Between Capsules 图卷积网络（Graph Convolutional Networks） Semi-Supervised Classification with Graph Convolutional Networks Graph Convolutional Networks"},{"title":"Note-1922年《劳动法大纲》","date":"2025-07-11T03:13:12.547Z","updated":"2024-03-09T06:46:31.000Z","comments":true,"path":"draft/note-LaoDongFaDaGang.html","permalink":"http://example.com/draft/note-LaoDongFaDaGang.html","excerpt":"","text":"中国劳动组合书记部拟定的劳动法案大纲 （一九二二年八月） （一）承认劳动者之集会结社权。 （二）承认劳动者之同盟罢工权。 （三）承认劳动者之团体的契约缔结权。 （四）承认劳动者之国际的联合。 （五）日工不得过八小时，夜工不得过六小时，每星期连续四十二小时〈休息〉。 （六）十八岁以下青年男女工人及吃力的工作，不得过六小时。 （七）禁止超过法定的工作时间。如在特别情形须得工会同意才得增加工作时间。 （八）农工的工作时间虽可超过八小时，但所超过之工作时间的工值，须按照八小时制的基础计算。 （九）须以法律担保一般不掠夺别人劳动之农人的农产品价格，此项价格由农人代表提出，以法律规定之。 （十）吃力的工作及有碍卫生的工作，对于十八岁以下的男女工人，绝对禁止超过法定时间。绝对禁止女工及十八岁以下男工作夜工。 （十一）体力的女工产前产后各八星期休工；其他工作之女工，产前产后各六星期休工，均照常领取工资。 （十二）禁止雇用十六岁以下之男女童工。 （十三）为保障工人适当以至低限度的工钱，国家须制定这种保障法律。当立此项法律时，须准全国总工会代表出席。无论公私企业或机关的工资，均不得低于此项法律保障的至低限度。 （十四）各种工人，由他们产业组合或职业组合保障，可选举代表参加政府经济机关，及选举代表参加政府企业机关，及政府所管理的私人企业或机关之权。 （十五）国家对于全国公私各企业均须设立劳动检查局。 （十六）国家保障工人有完全参加国家所设劳动检查局之权。 （十七）一切保险事业，须由工人参加规定之，以保障所有在政府的，公共的，私人的企业和机关内的工人之损失或危险。保险费完全由雇主或国家出之，受保险者决不分担。 （十八）各种工人和雇用人，一年工作中有一月之休息，半年中有两星期之休息，并领薪之权。 （十九）国家须以法律保证男女工人有受补习教育的机会。 （附白）工友们！这是本部斟酌各国劳动法拟定的，我们认为是最低的限度，并不过高，我们是非要国会都要通过不可的。但不知各位对于这十九条认为满足不满足？完备不完备？如有认为要增加或更改的请快快来函示知，以便修改。这是关于我们劳动阶级切身的利害，我们不可忽视呀！ 根据一九二二年九月三日出版的《先驱》第十一期刊印 引自：中国劳动组合书记部拟定的劳动法案大纲"}],"posts":[{"title":"Coding-量化交易-qlib workflow","slug":"coding_Quant_Qlib_start","date":"2025-08-29T08:17:46.152Z","updated":"2025-08-29T09:06:06.940Z","comments":true,"path":"2025/08/29/coding_Quant_Qlib_start/","link":"","permalink":"http://example.com/2025/08/29/coding_Quant_Qlib_start/","excerpt":"","text":"摘要 Csv数据转化 workflow主函数 配置函数 摘要 基于qlib的量化交易workflow框架实现，包括数据，因子，模型，回测部分。 Csv数据转化 convert_data.py 1234567891011121314151617# 从scripts.dump_bin模块导入DumpDataAll类from scripts.dump_bin import DumpDataAll# 当脚本作为主程序运行时执行以下代码if __name__ == &#x27;__main__&#x27;: # 创建DumpDataAll类的实例，用于数据导出 dump_util = DumpDataAll( data_path=&quot;./mydata/five_stock&quot;, # 原始数据存放路径 qlib_dir=&quot;./my_qlib_dir/five_stock&quot;, # QLib格式数据的输出目录 # include_fields=&#x27;open,high,low,close,volume,position&#x27;, # 注释掉的字段配置 include_fields=&#x27;volume,high,low,close,open&#x27;, # 需要导出的字段列表 date_field_name=&quot;date&quot;, # 日期字段的名称 symbol_field_name=&quot;symbol&quot;, # 股票代码字段的名称 freq=&quot;day&quot; # 数据频率，这里是日线数据 ) # 调用dump方法执行数据导出操作 dump_util.dump() workflow主函数 main.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798# Copyright (c) Microsoft Corporation.# Licensed under the MIT License.&quot;&quot;&quot;Qlib提供了两种类型的接口：(1) 用户可以通过简单的配置定义量化研究工作流(2) Qlib以模块化方式设计，支持像搭积木一样通过代码创建研究工作流(1)的接口是`qrun XXX.yaml`，(2)的接口是类似本脚本的形式，其功能与`qrun XXX.yaml`几乎相同&quot;&quot;&quot;# 导入qlib库及相关组件import qlibfrom qlib.constant import REG_CN # 导入中国市场常量from qlib.utils import init_instance_by_config, flatten_dict # 工具函数：通过配置初始化实例、字典扁平化from qlib.workflow import R # 实验记录与管理组件from qlib.workflow.record_temp import ( # 记录模板：用于生成各种分析记录 SignalRecord, # 信号记录 PortAnaRecord, # 组合分析记录 SigAnaRecord # 信号分析记录)# 从自定义配置文件导入基准指数和任务配置from mycode.myconfig import CSI300_BENCH, CSI5_GBDT_TASKif __name__ == &quot;__main__&quot;: # 数据存储路径配置 provider_uri = &quot;./my_qlib_dir/five_stock&quot; # 目标数据目录 # 初始化qlib # 注：GetData().qlib_data(...)用于获取数据，这里假设数据已存在 qlib.init(provider_uri=provider_uri, region=REG_CN) # 初始化并指定数据路径和市场区域（中国） # 根据配置初始化模型和数据集 model = init_instance_by_config(CSI5_GBDT_TASK[&quot;model&quot;]) # 从任务配置中初始化模型 dataset = init_instance_by_config(CSI5_GBDT_TASK[&quot;dataset&quot;]) # 从任务配置中初始化数据集 # 组合分析配置（回测相关设置） port_analysis_config = &#123; &quot;executor&quot;: &#123; # 执行器配置：控制回测执行逻辑 &quot;class&quot;: &quot;SimulatorExecutor&quot;, # 模拟器执行器 &quot;module_path&quot;: &quot;qlib.backtest.executor&quot;, # 执行器所在模块 &quot;kwargs&quot;: &#123; &quot;time_per_step&quot;: &quot;day&quot;, # 每步执行时间单位（按天） &quot;generate_portfolio_metrics&quot;: True, # 生成组合指标 &#125;, &#125;, &quot;strategy&quot;: &#123; # 策略配置：定义交易策略逻辑 &quot;class&quot;: &quot;TopkDropoutStrategy&quot;, # TopkDropout策略（选取排名前k的股票，剔除部分） &quot;module_path&quot;: &quot;qlib.contrib.strategy.signal_strategy&quot;, # 策略所在模块 &quot;kwargs&quot;: &#123; &quot;signal&quot;: (model, dataset), # 信号源：使用模型和数据集生成信号 &quot;topk&quot;: 50, # 选取排名前50的股票 &quot;n_drop&quot;: 5, # 每次调仓剔除5只股票 &#125;, &#125;, &quot;backtest&quot;: &#123; # 回测配置：定义回测的基本参数 &quot;start_time&quot;: &quot;2019-11-28&quot;, # 回测开始时间 &quot;end_time&quot;: &quot;2020-09-23&quot;, # 回测结束时间 &quot;account&quot;: 100000000, # 初始资金（1亿元） &quot;benchmark&quot;: CSI300_BENCH, # 基准指数（沪深300） &quot;exchange_kwargs&quot;: &#123; # 交易所相关参数（手续费等） &quot;freq&quot;: &quot;day&quot;, # 交易频率（按天） &quot;limit_threshold&quot;: 0.095, # 涨跌停阈值（9.5%） &quot;deal_price&quot;: &quot;close&quot;, # 成交价格（收盘价） &quot;open_cost&quot;: 0.0005, # 开仓手续费率（0.05%） &quot;close_cost&quot;: 0.0015, # 平仓手续费率（0.15%） &quot;min_cost&quot;: 5, # 最低手续费（5元） &#125;, &#125;, &#125; # 注意：以下代码是可选的 # 用于演示数据集可以单独使用 example_df = dataset.prepare(&quot;train&quot;) # 获取训练集数据 print(example_df.head()) # 打印训练集前5行 # 开始实验（使用R组件记录实验过程） with R.start(experiment_name=&quot;workflow&quot;): # 实验名称为&quot;workflow&quot; # 记录参数：将任务配置扁平化后存入日志 R.log_params(**flatten_dict(CSI5_GBDT_TASK)) # 模型训练 model.fit(dataset) # 保存模型参数 R.save_objects(** &#123;&quot;params.pkl&quot;: model&#125;) # 预测阶段 recorder = R.get_recorder() # 获取记录器 sr = SignalRecord(model, dataset, recorder) # 创建信号记录器 sr.generate() # 生成信号记录 # 信号分析 sar = SigAnaRecord(recorder) # 创建信号分析记录器 sar.generate() # 生成信号分析结果 # 回测分析 # 注：如果用户想基于自己的预测结果进行回测， # 请参考https://qlib.readthedocs.io/en/latest/component/recorder.html#record-template par = PortAnaRecord(recorder, port_analysis_config, &quot;day&quot;) # 创建组合分析记录器 par.generate() # 生成组合分析结果 配置函数 myconfig.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257# 导入qlib数据处理相关组件from qlib.data.dataset.handler import DataHandlerLP # 数据处理基类from qlib.data.dataset.loader import QlibDataLoader # 数据加载器基类from qlib.data.dataset.processor import Processor # 数据处理器基类from qlib.utils import get_callable_kwargs # 工具函数：获取可调用对象的参数from qlib.data.dataset import processor as processor_module # 数据处理器模块from inspect import getfullargspec # 用于获取函数参数信息# 基准指数配置CSI300_BENCH = &quot;SH000300&quot; # 沪深300指数代码CSI5_MARKET = &quot;all&quot; # 市场范围（全部股票）DATASET_A4_CLASS = &quot;A4&quot; # 数据集类名称# 默认的学习阶段数据处理器配置_DEFAULT_LEARN_PROCESSORS = [ &#123;&quot;class&quot;: &quot;DropnaLabel&quot;&#125;, # 移除标签为空的数据 &#123;&quot;class&quot;: &quot;CSZScoreNorm&quot;, &quot;kwargs&quot;: &#123;&quot;fields_group&quot;: &quot;label&quot;&#125;&#125;, # 对标签进行Z-Score标准化 &#123;&quot;class&quot;: &quot;Fillna&quot;&#125;, # 填充缺失值]def check_transform_proc(proc_l, fit_start_time, fit_end_time): &quot;&quot;&quot; 检查并处理数据处理器配置，为需要的处理器添加拟合时间范围 参数: proc_l: 处理器配置列表 fit_start_time: 拟合开始时间 fit_end_time: 拟合结束时间 返回: 处理后的处理器配置列表 &quot;&quot;&quot; new_l = [] for p in proc_l: if not isinstance(p, Processor): # 获取处理器类和参数 klass, pkwargs = get_callable_kwargs(p, processor_module) # 获取类的参数列表 args = getfullargspec(klass).args # 如果处理器需要拟合时间范围参数，则添加 if &quot;fit_start_time&quot; in args and &quot;fit_end_time&quot; in args: # 确保拟合时间范围已设置 assert ( fit_start_time is not None and fit_end_time is not None ), &quot;Make sure `fit_start_time` and `fit_end_time` are not None.&quot; pkwargs.update( &#123; &quot;fit_start_time&quot;: fit_start_time, &quot;fit_end_time&quot;: fit_end_time, &#125; ) # 构建处理器配置 proc_config = &#123;&quot;class&quot;: klass.__name__, &quot;kwargs&quot;: pkwargs&#125; # 如果原始配置包含模块路径，则保留 if isinstance(p, dict) and &quot;module_path&quot; in p: proc_config[&quot;module_path&quot;] = p[&quot;module_path&quot;] new_l.append(proc_config) else: new_l.append(p) return new_lclass A4DL(QlibDataLoader): &quot;&quot;&quot; 自定义数据加载器：用于获取指定因子（MACD、成交量、价格波动范围、3日均值） &quot;&quot;&quot; def __init__(self, config=None, **kwargs): &quot;&quot;&quot;初始化数据加载器&quot;&quot;&quot; _config = &#123; &quot;feature&quot;: self.get_feature_config(), # 设置特征配置 &#125; if config is not None: _config.update(config) super().__init__(config=_config,** kwargs) @staticmethod def get_feature_config(config=&#123;&#125;): &quot;&quot;&quot; 创建指定的因子配置：MACD、成交量、价格波动范围、3日均值 参数: config: 额外配置（预留） 返回: 特征表达式和名称的元组 &quot;&quot;&quot; # 1. 定义MACD因子 # MACD计算公式：(EMA(12)-EMA(26))/收盘价 - EMA((EMA(12)-EMA(26))/收盘价, 9)/收盘价 macd_exp = &#x27;(EMA($close, 12) - EMA($close, 26))/$close - EMA((EMA($close, 12) - EMA($close, 26))/$close, 9)/$close&#x27; # 2. 定义其他特征 volume = &#x27;$volume&#x27; # 成交量 price_range = &#x27;$high-$low&#x27; # 最高价-最低价（价格波动范围） ma3 = &#x27;Mean($close, 3)&#x27; # 3日收盘价均值 # 组合特征字段和名称 fields = [macd_exp, volume, price_range, ma3] names = [&#x27;MACD&#x27;, &#x27;VOLUME&#x27;, &#x27;RANGE&#x27;, &#x27;MA3&#x27;] return fields, namesclass A4(DataHandlerLP): &quot;&quot;&quot; 自定义数据处理器类，继承自DataHandlerLP 用于处理数据加载、特征工程和标签生成 &quot;&quot;&quot; def __init__( self, instruments=&quot;all&quot;, # 股票池 start_time=None, # 数据开始时间 end_time=None, # 数据结束时间 freq=&quot;day&quot;, # 数据频率（按天） infer_processors=[], # 推理阶段数据处理器 learn_processors=_DEFAULT_LEARN_PROCESSORS, # 学习阶段数据处理器 fit_start_time=None, # 处理器拟合开始时间 fit_end_time=None, # 处理器拟合结束时间 process_type=DataHandlerLP.PTYPE_A, # 处理类型 filter_pipe=None, # 过滤管道 inst_processors=None, # 股票处理器 **kwargs, ): # 检查并处理数据处理器配置 infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time) learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time) # 数据加载器配置 data_loader = &#123; &quot;class&quot;: &quot;QlibDataLoader&quot;, &quot;kwargs&quot;: &#123; &quot;config&quot;: &#123; &quot;feature&quot;: self.get_feature_config(), # 特征配置 &quot;label&quot;: kwargs.pop(&quot;label&quot;, self.get_label_config()), # 标签配置（默认使用内置标签） &#125;, &quot;filter_pipe&quot;: filter_pipe, # 过滤管道 &quot;freq&quot;: freq, # 数据频率 &quot;inst_processors&quot;: inst_processors, # 股票处理器 &#125;, &#125; # 调用父类构造函数初始化 super().__init__( instruments=instruments, start_time=start_time, end_time=end_time, data_loader=data_loader, infer_processors=infer_processors, learn_processors=learn_processors, process_type=process_type,** kwargs, ) def get_feature_config(self): &quot;&quot;&quot;获取特征配置（复用A4DL的数据加载器）&quot;&quot;&quot; return A4DL.get_feature_config() def get_label_config(self): &quot;&quot;&quot; 获取标签配置 这里定义的标签是：下一期的收盘价（用于预测） &quot;&quot;&quot; return [&quot;Ref($close, -1)&quot;], [&quot;LABEL0&quot;] # 表达式和标签名称def get_data_handler_config( start_time=&quot;2019-11-28&quot;, # 数据开始时间 end_time=&quot;2020-09-23&quot;, # 数据结束时间 fit_start_time=&quot;&lt;dataset.kwargs.segments.train.0&gt;&quot;, # 拟合开始时间（引用训练集开始时间） fit_end_time=&quot;&lt;dataset.kwargs.segments.train.1&gt;&quot;, # 拟合结束时间（引用训练集结束时间） instruments=CSI5_MARKET, # 股票池): &quot;&quot;&quot; 获取数据处理器配置 返回: 数据处理器配置字典 &quot;&quot;&quot; return &#123; &quot;start_time&quot;: start_time, &quot;end_time&quot;: end_time, &quot;fit_start_time&quot;: fit_start_time, &quot;fit_end_time&quot;: fit_end_time, &quot;instruments&quot;: instruments, &#125;def get_dataset_config( dataset_class=DATASET_A4_CLASS, # 数据集类名称 train=(&quot;2019-11-28&quot;, &quot;2020-07-23&quot;), # 训练集时间范围 valid=(&quot;2020-07-24&quot;, &quot;2020-08-23&quot;), # 验证集时间范围 test=(&quot;2020-08-24&quot;, &quot;2020-09-23&quot;), # 测试集时间范围 handler_kwargs=&#123;&quot;instruments&quot;: CSI5_MARKET&#125;, # 数据处理器参数): &quot;&quot;&quot; 获取数据集配置 返回: 数据集配置字典 &quot;&quot;&quot; return &#123; &quot;class&quot;: &quot;DatasetH&quot;, # qlib自带的DatasetH类（用于包装数据处理器） &quot;module_path&quot;: &quot;qlib.data.dataset&quot;, # DatasetH所在模块 &quot;kwargs&quot;: &#123; &quot;handler&quot;: &#123; # 数据处理器配置 &quot;class&quot;: dataset_class, # 自定义数据处理器类名 &quot;module_path&quot;: &quot;mycode.myconfig&quot;, # 自定义数据处理器所在模块 &quot;kwargs&quot;: get_data_handler_config(**handler_kwargs), # 数据处理器参数 &#125;, &quot;segments&quot;: &#123; # 数据集分割（训练/验证/测试） &quot;train&quot;: train, &quot;valid&quot;: valid, &quot;test&quot;: test, &#125;, &#125;, &#125;# GBDT模型配置（使用LightGBM）GBDT_MODEL = &#123; &quot;class&quot;: &quot;LGBModel&quot;, # qlib中的LGBModel类 &quot;module_path&quot;: &quot;qlib.contrib.model.gbdt&quot;, # LGBModel所在模块 &quot;kwargs&quot;: &#123; &quot;loss&quot;: &quot;mse&quot;, # 损失函数（均方误差） &quot;colsample_bytree&quot;: 0.8879, # 特征采样比例 &quot;learning_rate&quot;: 0.0421, # 学习率 &quot;subsample&quot;: 0.8789, # 样本采样比例 &quot;lambda_l1&quot;: 205.6999, # L1正则化系数 &quot;lambda_l2&quot;: 580.9768, # L2正则化系数 &quot;max_depth&quot;: 8, # 树的最大深度 &quot;num_leaves&quot;: 210, # 叶子节点数量 &quot;num_threads&quot;: 20, # 线程数量 &#125;,&#125;def get_gbdt_task(dataset_kwargs=&#123;&#125;, handler_kwargs=&#123;&quot;instruments&quot;: CSI5_MARKET&#125;): &quot;&quot;&quot; 获取GBDT任务配置（模型+数据集） 参数: dataset_kwargs: 数据集额外参数 handler_kwargs: 数据处理器额外参数 返回: 任务配置字典 &quot;&quot;&quot; return &#123; &quot;model&quot;: GBDT_MODEL, # 模型配置 &quot;dataset&quot;: get_dataset_config(** dataset_kwargs, handler_kwargs=handler_kwargs), # 数据集配置 &#125;# 最终的CSI5 GBDT任务配置CSI5_GBDT_TASK = get_gbdt_task(handler_kwargs=&#123;&quot;instruments&quot;: CSI5_MARKET&#125;)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-量化交易","slug":"Code-量化交易","permalink":"http://example.com/tags/Code-%E9%87%8F%E5%8C%96%E4%BA%A4%E6%98%93/"}]},{"title":"环境配置","slug":"env_config","date":"2025-04-28T03:05:09.549Z","updated":"2025-08-29T14:56:05.328Z","comments":true,"path":"2025/04/28/env_config/","link":"","permalink":"http://example.com/2025/04/28/env_config/","excerpt":"","text":"代理服务器设置 Ubuntu anaconda环境变量设置 Ubuntu cuda环境变量设置 换conda清华源 换pip清华源 Ubuntu22.04 开启3389端口远程连接 vscode python调试功能的launch.json配置设置 vscode python调试功能的settings.json配置设置 linux配置免密登录 screen命令 挂起窗口 nignx正向代理配置 windows指令 代理服务器设置 windows 12set http_proxy=http://192.168.1.140:1080set http_proxys=http://192.168.1.140:1080 ubuntu 12set http_proxy=socks5://192.168.1.140:1080set https_proxy=socks5://192.168.1.140:1080 Ubuntu anaconda环境变量设置 12345sudo gedit ~/.bashrcexport PATH=&quot;~/anaconda3/bin&quot;:$PATHsource ~/.bashrc Ubuntu cuda环境变量设置 123456789sudo nano ~/.bashrcexport PATH=/usr/local/cuda/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATHsource ~/.bashrcnvcc --version 换conda清华源 123conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes 换pip清华源 1pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple Ubuntu22.04 开启3389端口远程连接 12firewall-cmd --add-port=3389/tcp --permanentfirewall-cmd --reload vscode python调试功能的launch.json配置设置 参数配置 工作区Python路径设置 12345678910111213141516171819202122232425&#123; &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ &#123; &quot;name&quot;: &quot;Python: Run Image Detection Demo&quot;, &quot;type&quot;: &quot;debugpy&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;$&#123;workspaceFolder&#125;/demo/image_demo.py&quot;, &quot;console&quot;: &quot;integratedTerminal&quot;, &quot;args&quot;: [ // 参数设置 &quot;demo/demo.jpg&quot;, &quot;configs/mask_rcnn/mask-rcnn_r101_fpn_1x_coco.py&quot;, &quot;--weights&quot;, &quot;mymodel/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth&quot;, &quot;--texts&quot;, &quot;bench&quot;, &quot;--device&quot;, &quot;cuda:0&quot; ], &quot;env&quot;: &#123; &quot;PYTHONPATH&quot;: &quot;$&#123;workspaceFolder&#125;&quot; // 关键设置：将工作区路径添加到PYTHONPATH &#125;, &#125; ]&#125; vscode python调试功能的settings.json配置设置 123&#123; &quot;python.analysis.extraPaths&quot;: [&quot;./mycode&quot;] //本地模块添加&#125; linux配置免密登录 本机运行 1ssh-keygen -t rsa 将本机.SSH文件夹下的id_rsa.pub追加到远程主机.ssh文件夹下的authorized_keys文件中 1cat id_rsa.pub &gt;&gt; authorized_keys screen命令 挂起窗口 1234567891011121314# 新建窗口screen -S name# 挂起 [detached] ctrl + a + d # 列出窗口列表screen -ls# 杀死多余窗口kill -9 threadnum# 清除死去的窗口screen -wipe nignx正向代理配置 123456789101112131415161718192021#正向代理转发http请求 server &#123; #指定DNS服务器IP地址 # resolver 114.114.114.114; #监听80端口，http默认端口80 listen 80; #服务器IP或域名 server_name xxx.xxxxx.top; #正向代理转发http请求 location / &#123; proxy_pass http://xxxxx.top:20003; proxy_set_header HOST $host; proxy_buffers 256 4k; proxy_max_temp_file_size 0k; proxy_connect_timeout 30; proxy_send_timeout 60; proxy_read_timeout 60; proxy_next_upstream error timeout invalid_header http_502; &#125;&#125; windows指令 Windows转发wsl 123netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=7777 connectaddress=localhost connectport=8888netsh interface portproxy show allnetsh interface portproxy delete v4tov4 listenport=7777 listenaddress=0.0.0.0 protocol=tcp","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"编程基础知识","slug":"编程基础知识","permalink":"http://example.com/tags/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}]},{"title":"PaperReading-Robot_list-ICRA","slug":"paper_Robot_icra_list","date":"2025-04-21T08:28:48.661Z","updated":"2025-04-29T11:22:46.725Z","comments":true,"path":"2025/04/21/paper_Robot_icra_list/","link":"","permalink":"http://example.com/2025/04/21/paper_Robot_icra_list/","excerpt":"","text":"英文题目：CloudGripper: An Open Source Cloud Robotics Testbed for Robotic Manipulation Research, Benchmarking and Data Collection at Scale 中文题目：CloudGripper：一个用于机器人操作研究、基准测试和大规模数据收集的开源云机器人测试平台 研究背景：随着深度学习模型在机器人领域的发展，机器人操作研究面临训练数据获取的瓶颈。与计算机视觉和自然语言处理领域相比，机器人操作研究的数据相对匮乏，且各研究组的实验设置差异大，给基准测试和研究成果复现带来困难。同时，现有机器人操作研究实验室的建设和运营成本高，大规模部署机器人系统超出了大多数研究组的预算，数据收集也耗费大量人力，这些都限制了机器人操作领域大规模实验的开展。虽然仿真和模拟到现实的转换被提出作为解决大规模真实训练数据缺乏的方案，但模拟复杂物理交互仍面临挑战，尤其是对于可变形物体和易碎物体的操作。 所存在的问题 数据获取瓶颈：机器人操作研究难以获取大规模训练数据，限制了深度学习模型在该领域的潜力挖掘。 实验设置差异大：不同研究组的机器人操作实验设置各不相同，包括工作单元、机器人系统和实验对象等，导致基准测试和研究复现困难。 实验规模受限：建设和运营大规模机器人操作研究实验室成本高昂，数据收集过程繁琐且人力需求大，使得实际大规模实验进展缓慢，多局限于企业支持的项目。 仿真存在挑战：模拟复杂物理交互，特别是针对可变形和易碎物体的操作，难以达到与真实实验数据相同的丰富度，无法完全替代真实实验。 解决方法 设计CloudGripper测试平台：构建了一个基于机架的可扩展云机器人测试平台CloudGripper，每个机架包含32个小型机器人手臂工作单元，具备空间优化、成本高效的特点。每个工作单元配备独立的照明、低成本笛卡尔机器人手臂、可旋转平行夹爪和双摄像头设置，还提供5V和12V电源连接、1Gbit/s以太网网络和USB-C电缆连接。 开发软件栈：开发了简单易用的软件栈，每个机器人的树莓派4运行Raspberry Pi OS，通过基于令牌认证的REST API服务器，实现远程用户对机器人状态查询、图像获取、运动命令发送以及实时视频流查看等功能。所有机器人通过10Gbit/s网络连接，保障数据传输和远程控制。 开源生态建设：将CloudGripper作为开源项目发布，硬件和软件均采用宽松的许可协议，希望吸引国际用户共同参与，形成一个活跃的测试平台，推动新型算法的测试和发展。 所用到的数据集：CloudGripper - Rope - 100数据集，该数据集包含19个机器人进行随机绳索操作的实验数据，约110小时的机器人运动数据和400万张相机图像。数据集中还包含元数据、机器人动作指令以及不同分辨率的相机图像（顶部相机1280×720像素，底部相机640×480像素），未压缩形式约占半TB存储空间。 所进行的实验 重复性分析：对23个机器人进行运动重复性测试，在XY轴和Z轴方向分别选取五个均匀采样的路径点，每个机器人在每个点重复测试10次。结果显示XY轴位移偏差范围在−0.084mm到0.070mm之间，标准差约0.027mm；Z轴偏差范围在−0.419mm到0.376mm之间，标准差约0.109mm。虽然Z轴存在一定变异性，但整体系统重复性仍适用于多种实际机器人实验任务。 网络压力测试：通过本地计算服务器向CloudGripper发送API请求，从单个机器人开始逐渐增加到32个机器人，并发请求获取相机静态图像（10fps）。测试结果表明，CPU和内存利用率随请求线程增加而稳步上升，网络带宽使用线性增长至约80MB/s，仅占理论最大带宽（10Gbit/s）的不到十分之一，所有请求和机器人的最大延迟保持在100ms以下，95%的延迟约为70ms ，显示出系统处理高并发负载的潜力。 英文题目：CMG-Net: An End-to-End Contact-based Multi-Finger Dexterous Grasping Network 中文题目：CMG-Net：一种基于端到端接触的多指灵巧抓取网络 研究背景：从杂乱环境中抓取未知物体是自主机器人操作的一个基本问题，在工业自动化、商业场所和家庭等领域有着广泛应用需求。平行夹爪虽结构简单、计算效率高，但在处理复杂形状物体时存在局限性。多指灵巧手因自由度高、灵活性强，能更好地适应物体形状，然而其高维搜索空间和不连续的抓取空间，使得寻找有效的手部配置和抓取姿势变得极具挑战性。 所存在的问题 传统方法计算复杂：传统分析方法通过随机搜索和采样探索潜在抓取空间，计算成本高昂，每次处理物体通常需要数十甚至数百次迭代，并且严重依赖精确的物体表示，难以适用于杂乱环境中的未知物体。 数据驱动方法存在局限：数据驱动方法中，一类是传统采样方法的扩展，虽能直接从训练的深度模型估计抓取质量指标，但仍依赖已知物体模型，且存在采样和搜索空间巨大的问题；另一类端到端方法虽能有效生成抓取姿势且对未知物体有一定鲁棒性，但很多只能处理单个物体，并且由于未考虑夹爪与环境的潜在碰撞，抓取常常失败。还有部分方法仅对抓取类型进行分类，未充分考虑多指抓取的特性。 解决方法 提出新的抓取表示方法：基于接触点将10自由度的抓取投影到仅6自由度，定义了一种新颖的抓取表示方式。通过将指尖与物体的接触用圆来拟合，计算指尖中心、相关向量和矩阵，进而得到手部姿势和配置，有效减少了潜在的抓取搜索空间，提高了抓取质量。 构建CMG-Net网络：设计了一个端到端的深度神经网络CMG-Net，该网络包含抓取点分割、初步抓取姿势预测和抓取姿势优化三个阶段。利用PointNet++提取点云特征，预测接触点、手指投影和关节角度，最终输出多指手部配置和抓取姿势。 生成合成抓取数据集：构建了包含5000个杂乱场景、80个物体类别和2000万个注释的合成抓取数据集。先为单个物体生成高质量抓取，再通过在模拟环境中放置多个物体并筛选无碰撞的有效抓取，最后从特定视点获取点云，完成数据集的生成。 所用到的数据集：自主生成的合成抓取数据集，涵盖80个物体类别，分布在5000个不同场景中，包含大量的抓取注释（手部姿势和手部配置）以及从特定视点获取的点云数据。这些数据用于训练CMG-Net网络，以提高其在不同场景下对未知物体抓取的性能。 所进行的实验 仿真实验：使用Pytorch在NVIDIA GPU上实现CMG-Net，以深度相机捕获的深度图转换后的点云作为输入，并随机下采样。采用抓取成功率（SR）、抓取完成率（CR）和抓取质量作为评估指标，对比GraspIt!和Multi-FinGan等方法。结果表明，CMG-Net在各项指标上均有显著提升，能很好地处理小物体并为复杂场景生成高质量的抓取姿势。此外，通过消融实验验证了网络中相关模块的有效性。 真实世界实验：在真实环境实验中，使用配备三指灵巧手的Franka Emika Panda机器人和Intel RealSense D435相机。通过深度图像分割去除背景和桌面点，将物体点云输入CMG-Net获取最终抓取姿势。在不同数量物体的场景中进行实验，结果显示在6物体场景中成功率可达74.4%，完成率可达86.1%，在9物体场景中也有相似表现，证明了该方法在杂乱环境中对未知物体的良好适应性。 英文题目：Fast Collision Checking for Dual-Arm Collaborative Robots Working in Close Proximity 中文题目：用于近距离协同工作的双臂协作机器人的快速碰撞检测方法 研究背景：协作机器人在工业领域的应用日益广泛，因其能在紧凑环境中灵活部署并执行复杂任务，如ABB YuMi双臂协作机器人可完成穿针、折纸等精细操作。然而，当协作机器人近距离工作时，确保其运动过程中无碰撞是一大挑战。碰撞检测与避免是确定机器人模型在空间中是否重叠的问题，在机器人学、虚拟现实等多领域有广泛应用，现有动态碰撞检测算法主要有代数方程求解和扫描体积法两种，但都存在一定局限性。 所存在的问题 代数方程求解法：将动态碰撞检测问题转化为高阶（非线性）代数方程求解，计算速度相对较慢，且仅适用于三角形、椭圆或椭球等简单形状的物体。 扫描体积法：通过分析机器人路径的扫描体积来检测碰撞，虽能保持模型间的保守分离，但在协作机器人快速、精细操作且距离接近时，基于扫描体积的重叠测试过于保守，在实际应用中效果不佳。 解决方法 基于泰勒模型的正向运动学：利用泰勒模型（函数的紧密包围）和DH（Denavit-Hartenberg）约定，对协作机器人手臂的正向运动学进行建模，计算得到机器人手臂上任意点的位置和方向的紧密边界，从而确定机器人手臂间的干涉情况。 计算分离距离函数的下界：通过泰勒模型计算分离距离函数的下界，以此判断机器人手臂在给定时间间隔内是否发生碰撞。若分离距离函数的下界大于用户指定的阈值，则机器人手臂在运动过程中无碰撞。 结合包围体技术加速计算：采用包围体技术，如使用有限的包围球集合及其泰勒模型进行碰撞检测，减少了需要计算的点对数量，从而加速了整个碰撞检测的计算过程。 所用到的数据集：本文未使用公开数据集。实验中自定义了相关数据，如在分析机器人手臂运动时，设置了机器人手臂的关节参数（包括关节角度、连杆长度等）；在测试算法性能时，设定了机器人手臂的运动场景，如双臂逐渐远离、靠近再远离的运动过程等。 所进行的实验 算法实现与设置：使用C++在ROS（机器人操作系统）中实现算法，在配备Intel Core i7-6700 3.40GHz CPU和32GB内存的PC上进行性能测试。采用三阶（三次）泰勒模型进行代数计算，将时间间隔细分为多个小子区间，并在每个子区间的起始点进行泰勒展开。 实验结果与对比：使用ABB YuMi双臂协作机器人进行实验，用变分球集近似机器人手臂的连杆。在第一个实验中，测量双臂末端执行器间距离函数的泰勒模型与实际距离的差异，结果表明泰勒模型的下界足够紧密，可用于精确的碰撞检测与避免。在第二个实验中，将本文算法与先进的碰撞检测库FCL（Flexible Collision Library）进行对比，结果显示在双臂近距离操作（距离小于1cm和1mm）时，本文算法在计算时间上具有优势。 英文题目：InterRep: A Visual Interaction Representation for Robotic Grasping 中文题目：InterRep：一种用于机器人抓取的视觉交互表示方法 研究背景：预训练视觉模型在机器人学习任务中得到应用，但多数研究聚焦于模型训练本身，对基于现有预训练模型提取更有效表示的研究不足。现有交互表示方法在处理复杂形状物体抓取或涉及丰富接触的灵巧抓取任务时，存在忽略物体形状信息或依赖裁剪整个物体区域等问题，影响性能和泛化能力。 所存在的问题 忽视有效表示提取：大多数利用预训练进行机器人学习的研究，仅关注如何训练更好的预训练模型，而在下游任务中，主要使用整个图像的潜在视觉表示，未深入探索更有效的表示形式。 现有方法有局限：现有的交互表示方法，如VIOLA和GraphIRL，在处理复杂形状物体的抓取任务或灵巧抓取任务时，难以取得令人满意的性能和泛化能力，无法很好地适应新物体。 解决方法 提出InterRep表示方法：结合预训练视觉模型的优势和抓取过程中的动态交互特征，在抓取过程的每一步，先用预训练模型提取图像特征，再通过选择物体上最靠近机器人手的区域的局部特征，集成动态交互信息，捕捉手与物体间的距离信息和物体的局部形状细节。 设计策略学习框架：基于演示增强策略梯度（DAPG）构建强化学习策略学习框架，先通过行为克隆进行初始化，再用强化学习进行微调。在观察空间中纳入InterRep和本体感受信息；定义包含接近奖励、提起奖励和成功奖励的奖励函数，促进抓取效果。 所用到的数据集 GRAB数据集：用于获取人类演示数据，从中提取右手数据并进行重定位，得到机器人抓取轨迹的演示数据，用于行为克隆阶段训练策略网络。 3DNet数据集：包含28个新颖的物体网格，用于测试模型对未见物体的泛化能力。 所进行的实验 模拟实验：在MuJoCo模拟器中构建抓取模拟环境，对多种物体进行抓取实验。对比不同基线方法，结果表明InterRep在训练速度和泛化能力上表现更优，能有效捕捉不同物体的共享特征，对新物体的抓取成功率超过60%。同时验证了动态交互信息、局部几何特征、物体区域提取以及全局和局部交互特征组合对抓取性能的影响。 不同预训练模型实验：使用不同架构的预训练模型（MVP和ResNet50）进行实验，结果显示InterRep在不同预训练模型下均有效，证明了该方法的通用性。 不同机器人手和任务实验：在不同形态的机器人手（Allegro Hand）上进行抓取实验，以及在MetaWorld环境中的抽屉打开任务实验，结果表明InterRep在不同机器人手和任务中具有良好的泛化性。 真实机器人实验：将训练好的策略应用于由Allegro Hand和Unitree Zl arm组成的真实机器人系统，对香蕉和相机进行抓取测试，InterRep的抓取成功率达到70%，验证了其在真实场景中的有效性。 英文题目：Learning Deep Visuomotor Policies for Dexterous Hand Manipulation 中文题目：学习用于灵巧手操作的深度视觉运动策略 研究背景：机器人在复杂环境中执行任务时，需要具备感知世界和利用丰富感官信息的能力，多手指灵巧手和视觉、触觉传感器为实现这一目标提供了可能，但高维的观察和行动空间使控制器合成面临挑战。传统基于强化学习训练策略的方法效率较低，难以满足需求。因此，探索更有效的算法来训练能直接处理原始视觉和触觉反馈的策略具有重要意义。 所存在的问题 高维空间挑战：多手指灵巧手和视觉、触觉传感器带来的高维观察和行动空间，增加了控制器合成的难度。 强化学习效率低：基于强化学习从视觉空间直接训练策略的方法效率低下，即使对于紧凑状态空间的操作任务，也需要大量计算资源和时间。 解决方法 采用模仿学习：通过模仿学习训练深度视觉运动策略，利用专家演示数据为策略优化提供更稳定的目标，减少探索负担。根据不同情况，可以从模拟器优化算法、紧凑状态表示训练或人类演示等途径获取演示数据。 设计策略架构：构建深度神经网络作为传感器运动策略架构，将RGB相机观察、本体感受特征向量和接触传感器信息作为输入。采用卷积层处理图像，全连接层处理本体感受和触觉传感器信息，通过实验发现晚融合方式效果更佳。 所用到的数据集：本文未使用公开数据集。实验中的数据主要通过在MuJoCo模拟器中模拟任务场景获取，利用训练好的计算专家策略生成演示轨迹，这些轨迹包含了机器人在执行任务过程中的状态和动作信息，用于模仿学习训练深度视觉运动策略。 所进行的实验 对比不同算法效率：比较直接在视觉空间进行强化学习、行为克隆（BC）和DAgger算法的学习曲线。结果表明，强化学习在视觉空间进展缓慢，而模仿学习方法能够训练出成功的策略，且DAgger在多数任务上表现略优于BC，BC则可利用人类演示数据，对基础设施要求较低。 探究触觉传感优势：对比有无接触传感器时策略的性能，发现接触传感器对物体重定位任务有显著帮助，能提高成功率，在其他任务中也有一定增益。对于遮挡较多的任务和视角，接触传感器加速学习并提升渐近性能。 研究不同融合方式效果：实验对比了早期融合和晚期融合两种方式，发现晚期融合在处理当前任务时表现稍好，因此在实验中采用晚期融合方式。 英文题目：Learning Dexterous Grasping with Object-Centric Visual Affordances 中文题目：基于以物体为中心的视觉感知学习灵巧抓取 研究背景：机器人抓取是复杂操作任务的重要前提，灵巧拟人化机器人手具备执行精细操作的潜力，但因其自由度高，学习操作极具挑战。传统强化学习方法在处理高维状态和动作空间时样本复杂度高，而依赖人类演示轨迹的方法存在成本高、通用性差等问题。因此，探索更有效的学习方法来实现机器人灵巧抓取具有重要意义。 所存在的问题 高维空间挑战：灵巧手的高自由度导致状态和动作空间维度高，传统强化学习方法面临巨大的样本复杂性问题，学习效率低下。 传统演示学习的局限：依赖人类演示轨迹的学习方法，如使用动作捕捉手套获取演示数据，需要耗费大量人力，可能需要专业设备，且演示轨迹与特定对象紧密耦合，难以泛化到新对象。 解决方法 提出GRAFF模型：将以物体为中心的视觉感知模型嵌入深度强化学习循环中，训练机器人的抓取策略。该模型由两个阶段组成，首先训练一个网络从静态图像中预测感知区域，然后利用学习到的感知信息训练动态抓取策略。 设计奖励函数：在奖励函数中结合成功抓取奖励、手部与感知区域接触距离奖励以及熵最大化项，引导智能体探索物体的可抓取区域，使智能体在学习过程中更关注有利于抓取的区域。 所用到的数据集 ContactDB数据集：包含50个家用物体的3D扫描以及通过热像仪记录的人类接触地图，从中选取16个单手抓取的物体，用于训练视觉感知模型和评估抓取策略。 3DNet数据集：一个CAD模型数据库，包含多个物体类别，每个类别有多个网格模型。从该数据集中选取24个与ContactDB中物体大致对齐的网格模型，用于测试模型对未见物体的泛化能力。 所进行的实验 对比实验：设计了两个纯强化学习基线（NO PRIOR和COM）以及一个结合模仿学习和强化学习的方法DAPG进行对比。结果表明，GRAFF在所有指标上均优于纯强化学习基线，在与DAPG的比较中也具有竞争力，且在处理未见物体时表现更优。 鲁棒性实验：通过对物体施加不同方向的干扰力，评估抓取的稳定性；改变物体的质量和尺度，测试模型对物体物理属性变化的鲁棒性。实验结果显示，GRAFF在面对干扰力和物体物理属性变化时，仍能保持较高的抓取成功率和稳定性。 泛化性实验：使用训练好的策略对3DNet数据集中的未见物体进行抓取实验，GRAFF在抓取成功率和稳定性方面大幅优于基线方法，证明其具有良好的泛化能力。 消融实验：逐步添加模型的不同组件进行对比，验证了完整模型的有效性。同时，对比训练时间发现，GRAFF学习速度更快，达到相同成功率所需的训练样本更少。 英文题目：Learning Hierarchical Control for Robust In-Hand Manipulation 中文题目：学习用于稳健手内操作的分层控制方法 研究背景：机器人手内操作是一个长期存在的挑战，其复杂性体现在手部与物体接触建模以及复杂操作序列中手指运动的协调上。传统基于模型的方法在处理复杂操作序列时存在局限，而基于无模型的深度强化学习方法通常需要针对每个物体实例学习策略，且在保持物体稳定抓取方面表现不佳。此外，在许多实际操作任务中，物体需在手中持续稳定抓取，现有方法难以满足这一需求。 所存在的问题 传统基于模型的方法：虽能利用已知的运动学、动力学和接触模型设计控制器来执行基本操作，但在处理需要复杂操作序列以达到较远目标姿态的任务时存在困难，且对噪声和不准确模型的鲁棒性较差。同时，这些方法常基于强假设，如无限摩擦的粘性接触、完全已知的确定性转换模型和有限状态空间等，难以应对现实世界的挑战。 无模型的深度强化学习方法：此类方法无需手部和物体属性的先验知识，但样本复杂度高，且学习过程通常不区分低级和中级控制。多数相关研究中的任务不需要机器人在操作过程中稳健地抓取物体，若要学习在操作时稳定抓取物体的策略，需要大量训练集和精心设计的奖励函数。 解决方法：提出一种分层控制结构，结合传统基于模型的低级控制器和基于深度强化学习（DRL）的中级策略。低级控制器利用扭矩控制实现三种操作原语（重新定位、滑动、翻转），能在接触时提供鲁棒性；中级策略通过DRL学习，用于选择合适的操作原语及其参数，以引导物体达到目标姿态。同时，设计了可行性过滤器来筛选无效动作，并为成功且有用的翻转动作设置奖励，提高策略学习效率。 所用到的数据集：为训练中级策略，收集了物体初始姿态(x_{0})和目标姿态(X_{g})的综合数据集，并将其分为“Easy”“Medium”“Hard Goals”三组。“Easy Goal”组内目标可仅通过重新定位操作达到；“Medium Goal”组内目标可能需要滑动和重新定位操作；“Hard Goals”组包含任意((X_{0}, X_{g}))对，部分需要翻转操作才能成功到达目标。 所进行的实验 对比实验：将该方法与基于深度确定性策略梯度（DDPG）的端到端策略和基于搜索的基线方法进行对比。在移动杆状物体至目标姿态的任务中，该方法在成功率和降低掉落率方面表现更优，尤其在处理需要复杂操作序列的“Hard Goals”任务时优势明显。虽然端到端方法的操作序列更快，但在实际机器人应用中可能因不稳定导致物体频繁掉落，而基于搜索的基线方法存在计算复杂度高和规划性能受分辨率影响的问题。 鲁棒性实验：通过添加物体姿态观测噪声以及改变物体的几何和惯性参数，评估该方法的鲁棒性。实验结果表明，该方法对物体模型的不准确性具有较高的鲁棒性，在观测噪声存在的情况下，成功率虽有下降，但仍能保持良好性能。这得益于中级控制器的状态反馈机制和扭矩控制对不确定性的固有鲁棒性。 泛化性实验：以立方体为对象进行实验，测试该方法对不同形状物体的泛化能力。实验中禁用了对立方体操作增益有限的滑动操作，并将翻转操作分解为两个阶段。结果显示，该方法在处理立方体操作时成功率为71.4%，掉落率为20.8% ，表明其能有效推广到不同形状的物体上。 英文题目：Learning Task-Oriented Dexterous Grasping from Human Knowledge 中文题目：从人类知识中学习面向任务的灵巧抓取 研究背景：机器人灵巧性在工业自动化中至关重要，可助力完成复杂任务。但现有机器人系统在依据物体功能、任务指定确定合适抓取策略方面能力欠缺。研究表明，灵巧操作与物体功能、任务指定、物体间空间关系和场景理解相关，虽有对抓取拓扑的定义研究，但在实现面向任务的抓取和利用人类知识制定抓取策略方面进展较少。 所存在的问题：现有机器人系统难以在物体功能和任务指定的背景下，确定合适的抓取策略，缺乏对人类抓取知识的有效利用以及面向任务抓取的实现方法。 解决方法 建立数据集与学习网络：基于耶鲁人类抓取数据集，开发面向任务的物体抓取数据集，设计包含三个子网的深度学习网络，以从数据集中学习人类抓取知识，预测抓取策略。其中，抓取拓扑选择网络(g(f))、OppoType选择网络(o(f))和PIP选择网络(p(f))分别用于预测抓取类型、对立类型和抓握力度等参数。 强化学习实现自适应抓取：采用近端策略优化（PPO）强化学习算法，结合演员-评论家策略，训练机器人自适应抓取。通过定义奖励函数，使机器人在抓取过程中，根据接触点数量和总力的情况获得奖励或惩罚，从而实现优化抓取。 所用到的数据集 耶鲁人类抓取数据集：以此为基础开发新数据集，记录了人员的常规工作活动视频，包含任务指定、物体功能和抓取参数等信息。对其进行精炼，得到74个独特任务、15种抓取拓扑、255个独特物体。 自建3D模型数据集：收集157个与耶鲁数据集中物体相似的3D模型，用于后续实验。在实验中，因机器人手的物理限制，使用其中103个3D物体，86个用于训练自适应抓取部署网络，17个用于测试算法。 所进行的实验 抓取策略学习实验：使用650个全新数据样本测试抓取选择网络，结果显示其命中率达100%，前3匹配率为98.6%。OppoType选择网络和PIP选择网络的准确率分别为85.9%和86.3%。 模拟任意物体抓取实验：利用AR10机器人手和Sawyer机器人臂在模拟环境中进行实验，对9种预定义抓取拓扑，每种用3个新物体测试，每个物体抓取100次，共进行2700次抓取实验。最终成功率为85.6%，实验结果受测试数据新颖性、机器人手灵巧度和3D模型准确性等因素影响。 英文题目：Learning to Rock-and-Walk: Dynamic, Non-Prehensile, and Underactuated Object Locomotion through Reinforcement Learning 中文题目：通过强化学习实现动态、非抓取式和欠驱动的物体移动：学会摇摆行走 研究背景：在搬运过重或过大而难以抓取或抬起的物体时，利用物体与支撑面的相互作用及其在重力下的自然动力学进行机器人操作具有重要意义。传统动态操作方法多基于物理模型，而强化学习在动态操作和运动控制领域逐渐得到应用，为解决此类问题提供了新途径。 所存在的问题：对于搬运过重或过大物体的任务，传统的抓取 - 抬起 - 搬运或推动等操作方法不再适用，需要探索新的、更有效的物体搬运方式，以应对复杂的物体搬运场景。 解决方法：提出一种强化学习框架，将物体搬运任务建模为强化学习问题。通过定义状态、动作和奖励函数，使用“软演员 - 评论家”（SAC）算法在模拟环境中训练智能体，学习控制物体的策略。状态包括物体与地面接触点的位置和欧拉角及其时间变化率；动作定义为控制作用于物体上某点的速度；奖励函数鼓励物体向前移动，并惩罚过度调整物体姿态的行为。 所用到的数据集：未使用公开数据集。在模拟训练过程中，为提升策略的鲁棒性，针对锥形物体模型，在训练过程中让智能体接触250种略有不同形状的物体，以模拟真实场景中的变化、误差和不确定性，但未明确这些物体形状数据的具体来源和性质。 所进行的实验 仿真实验：对锥形物体模型和摩艾石像模型进行仿真实验。在训练过程中，智能体学习到的策略能使物体产生周期性步态，通过调节物体的机械能和姿态实现向前移动。例如，锥形物体模型在训练初期学习效率较高，且智能体能够控制物体的滚动和前进运动；摩艾石像模型的仿真实验中，其运动轨迹和姿态控制也符合预期，不过与锥形物体相比，其最大滚动幅度设置较小，运动轨迹更为保守。 实际实验：使用定制的四旋翼飞行器和UR10机器人手臂进行实验，将在模拟环境中学习到的策略应用于实际场景。机器人手臂实验中，成功实现了物体在实验室地面的搬运，且在受到扰动时能恢复稳定步态，较大的末端执行器运动幅度可使物体移动速度更快；四旋翼飞行器实验分别在实验室地面和泡沫垫上进行，尽管四旋翼飞行器定位精度较差且存在气动干扰，但仍能成功搬运物体，且泡沫垫因能量耗散导致物体侧向运动幅度较小。 英文题目：Mechanical Intelligence for Prehensile In-Hand Manipulation of Spatial Trajectories 中文题目：用于空间轨迹抓握式手内操作的机械智能 研究背景：机器人在日常生活中的应用日益广泛，但在形状多样和形状不确定的情况下，使用机器人手进行可靠的灵巧操作仍是一个未解决的问题。在机器人手的设计方面，从简单的单自由度平行夹爪到复杂的拟人化手，设计多样。欠驱动手虽能通过简单控制抓取多种物体，但在复杂任务和手内操作能力上存在局限。目前，针对prehensile in-hand helical manipulation轨迹的研究较少，相关操作的实现仍面临挑战。 所存在的问题 现有机器人手的局限：多数欠驱动手只能在平面内进行手内操作，难以完成复杂或空间的灵巧手内操作。而复杂的拟人化手虽然具备一定能力，但存在控制复杂、对物体几何形状依赖大等问题。 特定操作研究不足：对于prehensile in-hand helical manipulation（如拧、拉等操作）轨迹的研究在灵巧操作领域仍有待探索，现有解决方案存在不足，如需要复杂的传感器反馈、冗余控制系统或大量的气动腔等。 解决方法：提出一种基于机械智能的技术，通过设计特殊的机器人手结构和控制策略，实现抓握式手内的螺旋操作。以三指两驱动的欠驱动螺旋手为例，利用数学模型对螺旋运动进行深入分析，确定运动轨迹和控制关系。通过两个电机分别控制近端和远端关节，基于速度调节实现对不同尺寸和形状物体的螺旋操作，并设计了实际控制算法来实现该操作。 所用到的数据集：未使用公开数据集。在研究过程中，通过对12个不同尺寸和形状（三角形、正方形、圆柱形）的物体进行实验来验证方法的有效性。这些物体是自行准备用于实验测试的，文中未提及关于这些物体数据的具体来源和性质。 所进行的实验 螺旋运动从目标位置开始的实验：在速度调节控制方案下，使用螺旋手抓取不同尺寸的物体并进行操作。实验对比了模拟和实际的旋转角度与平移距离，结果表明平移实验结果与模拟范围接近，但旋转实验结果部分小于模拟结果。三角形物体表现相对较好，而方形和圆柱形物体因抓取策略不同，在操作过程中存在稳定性和抓取力不均的问题。同时，尝试了不使用速度调节控制方案的实验，该方案虽提高了旋转范围，但平移范围变差。 带偏移的螺旋运动实验：将物体放置在三个偏移位置进行实验，测试螺旋手的操作容差。实验结果显示，不同物体在不同偏移位置的表现不稳定，但平均结果与直接抓取接近。在平移范围方面，实验结果与模拟结果更一致，表明螺旋手在平移操作上具有较高的能力和容差。 英文题目：Model-Based Reinforcement Learning for Closed-Loop Dynamic Control of Soft Robotic Manipulators 中文题目：基于模型的强化学习用于软机器人操纵器的闭环动态控制 研究背景：软机器人的建模和控制复杂性限制了其在现实场景中的应用。当前软机器人操纵器大多采用基于运动学模型或关节空间线性度的静态或准动态控制器，无法充分利用软体系统的丰富动力学特性。开发准确的动态模型难度较大，基于模型的传统方法存在依赖恒定曲率近似等问题，直接学习闭环控制策略在实际平台上耗时且易陷入局部最优，从模拟模型学习又会放大 inaccuracies。 所存在的问题 现有控制方法的局限性：静态控制器依赖稳态假设，限制了软机器人操纵器的速度、效率和可达性；基于模型的动态控制方法，如基于恒定曲率近似的方法，理论上仅在稳态条件下有效，且难以开发能在所需控制周期内运行的模型预测控制器（MPC）以实现闭环控制。 策略学习的挑战：直接策略学习在高维系统中存在数据需求大、模型偏差影响策略性能以及局部极小值和探索等问题。 解决方法：提出一种基于模型的策略学习算法实现软机器人操纵器的闭环预测控制。使用递归神经网络（NARX）学习前向动态模型，通过轨迹优化和监督学习推导闭环策略。具体步骤包括：先通过电机随机运动采样数据训练NARX网络得到前向动态模型；然后利用单射击轨迹优化算法在真实平台上采样开环控制策略；最后使用多层感知器对新获得的实验轨迹进行监督学习，得到闭环预测控制器。 所用到的数据集：未使用公开数据集。在研究过程中，针对模拟模型，通过电机随机运动收集7000个样本用于学习前向动态模型；对于实际的两部分气动驱动软操纵器，收集12000个样本学习前向模型。这些样本数据主要用于训练模型和学习控制策略，未提及具体来源和数据性质。 所进行的实验 仿真实验：进行了四项仿真研究。在全局动态到达实验中，验证了控制器动态到达静态目标的准确性；在有外部干扰的到达实验中，证明了控制器对强外部干扰具有鲁棒性；在多点到达实验中，展示了控制器能从任意给定状态到达工作空间中任何期望的静止目标；在可变控制频率实验中，发现所学闭环策略对控制频率具有鲁棒性，控制频率变化对跟踪误差和到达时间影响不大，但控制输入的平滑性会受影响。 实际实验：在两部分气动驱动软操纵器上进行实验。在全局动态到达实验中，控制器在不同评估时间段内有不同的跟踪误差和到达时间表现；在低频到达实验中，降低控制频率会增加跟踪误差和到达时间，且部分目标在低频下不可达；在有负载到达实验中，即使附加负载且未进行适应阶段，初始闭环策略仍能执行到达任务，但到达时间显著增加。 英文题目：Multiagent Reinforcement Learning: Rollout and Policy Iteration for POMDP With Application to Multirobot Problems 中文题目：多智能体强化学习：用于部分可观测马尔可夫决策过程的滚动算法和策略迭代及其在多机器人问题中的应用 研究背景：部分可观测多智能体序贯决策问题在实际场景中面临诸多挑战，如部分状态观测、大状态空间、大控制空间以及智能体间的不完美通信等。传统的动态规划方法难以求解此类复杂问题，因此需要探索更有效的次优解决方案。多智能体强化学习在解决这类问题方面具有潜力，受到广泛关注。 所存在的问题 计算复杂度高：在多智能体环境下，标准滚动算法的前瞻优化计算成本极高，其计算复杂度随智能体数量呈指数增长，难以应用于实际大规模问题。 通信不完美：现实场景中智能体间通信往往不完美，这会导致策略改进属性丧失、学习策略可能无法有限终止等问题，影响多智能体系统的性能和决策效果。 现有方法的局限性：现有的部分可观测马尔可夫决策过程（POMDP）求解方法，如POMCP、MADDPG等，在处理大规模多智能体问题时存在局限性，如Q因子估计不准确、难以扩展到更多智能体等。 解决方法 多智能体截断滚动算法：提出一种多智能体截断滚动算法，通过简化智能体逐个进行前瞻优化的方式，将计算复杂度从标准滚动算法的(O(C^{m}))降低到(O(Cm))，同时保持策略改进属性。该算法利用轨迹截断和终端成本近似，在减少计算量的同时保证了一定的性能。 近似策略迭代：将多智能体截断滚动算法融入近似策略迭代框架，通过迭代训练神经网络来近似策略，进一步优化策略。同时提出了基于标准滚动、单智能体逐个滚动和顺序优化滚动的三种近似策略迭代算法，以适应不同场景需求。 在线执行策略：引入在线执行策略，以离线训练的策略为基础，通过在线前瞻优化和终端成本近似，提高策略在动态变化环境中的适应性和性能。该策略能有效应对系统参数变化，弥补离线训练策略的不足。 应对不完美通信的策略：针对不完美通信情况，提出多种扩展方法。如在控制不共享时，使用随机化策略解决有限终止问题；通过间歇性通信架构，利用云服务器实现控制共享，恢复策略改进属性。 所用到的数据集：未使用公开数据集。在研究过程中，针对多机器人修复问题构建了模拟环境，通过设置不同的图拓扑结构（如32顶点、500顶点的图）、机器人数量（4个、8个、10个、50个）、损伤水平及转移概率等参数生成实验数据，用于测试和验证各种算法的性能。这些数据是根据研究中的问题设定生成，并非来自已有的公开数据集。 所进行的实验 多机器人修复问题实验：在部分可观测的多机器人修复问题上进行实验，对比了不同算法的性能。结果表明，单智能体逐个滚动和顺序优化滚动算法在计算成本大幅降低的情况下，性能与标准滚动算法相当；近似策略迭代方法能在多次迭代中不断改进策略；在线执行策略在利用离线训练的策略和成本近似时，性能优于其他滚动算法和近似策略迭代算法。 不完美通信实验：考虑智能体间不完美通信的情况，研究了多种近似多智能体滚动算法在不同通信架构下的性能。实验结果显示，不同通信架构各有优劣，如AMR - LC依赖通信半径，AMR - ILC在连接概率较高时表现较好等。同时，与其他方法（如A3C3）对比，本文提出的方法在不完美通信情况下能产生更优的策略 英文题目：RBO Hand 3: A Platform for Soft Dexterous Manipulation 中文题目：RBO Hand 3：用于软灵巧操作的平台 研究背景：机器人手的设计多样，但在实现灵巧操作、借鉴人类灵巧性以及满足真实世界实验需求方面存在不足。传统刚性机器人手缺乏内在柔顺性，难以适应复杂环境；欠驱动软手虽在一定程度上有改进，但在重新配置和模仿人类策略方面存在局限。开发具有高度通用性、鲁棒性且能实现灵巧操作的机器人手成为研究重点。 所存在的问题 传统机器人手的局限：刚性机器人手依赖复杂力学和精确的接触动力学建模，缺乏内在柔顺性，在频繁与环境接触的场景中可靠性不足，难以实现高度灵巧的操作。 欠驱动软手的不足：欠驱动软手自由度有限，无法以多种方式重新配置，难以形成多样的操作漏斗和施加不同的力模式，不利于实现灵巧的手内操作和借鉴人类操作策略。 综合能力欠缺：现有的机器人手难以同时满足高度灵巧操作、有效借鉴人类操作策略以及在真实世界实验中长时间稳定运行的要求。 解决方法：设计RBO Hand 3，通过整合多个设计特点来解决上述问题。采用基于软材料的气动驱动，实现机械柔顺性与多自由度的结合；模仿人类手的功能和结构，以促进人类操作策略的转移；进行模块化设计并改进制造工艺，提高手的鲁棒性，满足真实世界实验的需求。 所用到的数据集：未使用公开数据集。在研究过程中，通过对RBO Hand 3进行各项实验来获取数据，如测量两腔手指的工作空间和力、波纹管致动器的扭矩，以及进行抓握测试、模仿人类抓握策略实验和手内操作实验等，这些实验数据用于评估和展示RBO Hand 3的性能，并非来自公开的数据集。 所进行的实验 拇指对向性实验：使用Kapandji测试评估拇指设计的功能，RBO Hand 3的拇指能够触碰到手的十个指定位置，获得最高分，证明了其拇指设计的灵巧性以及对实现多样操作漏斗的重要性。 抓握姿势实验：依据GRASP分类法，测试RBO Hand 3实现不同抓握姿势的能力。它能够重复执行全部33种抓握姿势，展现出高度的灵巧性和通用性，相比前代手有明显改进。 抓握强度实验：通过测量在不同方向上拔出物体所需的力，展示RBO Hand 3的抓握强度。实验结果表明，它能承受较大的外力，最大拉力可达39N，抓握强度超过了类似的气动手，与“硬”拟人化手相当。 模仿人类抓握策略实验：复制人类最常见的三种桌面抓握策略，RBO Hand 3能够可靠地再现这些策略，尽管在翻转抓握时方式与人类略有不同，但仍遵循相同的基本策略，体现了其对人类操作功能的模仿能力和柔顺性对操作的重要贡献。 手内操作实验：通过在手中旋转不同物体，展示RBO Hand 3的手内操作灵巧性。相同的驱动轨迹能对不同尺寸、形状和重量的物体成功进行操作，体现了柔顺性在操作中的重要作用，也表明该手能形成可靠的操作漏斗。 作为研究平台的评估实验：在实验室中使用RBO Hand 3进行手内操作实验，评估其作为研究平台的性能。估计两腔手指和波纹管致动器的连续使用时间，并记录主要的故障类型和修复时间。结果表明，该手具有较高的可用性，修复简单快捷，是一个可靠的研究平台。 英文题目：Real2Sim2Real: Self-Supervised Learning of Physical Single-Step Dynamic Actions for Planar Robot Casting 中文题目：真实-模拟-真实：用于平面机器人抛投的物理单步动态动作自监督学习 研究背景：在机器人操作领域，对刚性物体的弹道投掷或击打运动研究较多，但对可变形物体（如电缆、织物）的动态操作研究较少。对这类物体进行动力学建模面临诸多挑战，包括物体变形、弹性和摩擦的不确定性，以及系统运动时间长导致的状态估计和动力学建模复杂性增加。同时，模拟与现实之间的差距也是机器人学习中一个长期存在的难题，尤其在处理可变形物体时更为突出。 所存在的问题 动力学建模困难：可变形物体的动力学建模复杂，其变形、弹性和摩擦的不确定性，以及运动时间长的特点，使得精确建模难度大。 模拟与现实差距：传统使用模拟来替代物理实验收集数据的方法，存在模拟与现实之间的差距，难以将在模拟环境中学习到的策略直接应用于现实场景。 现有方法局限：现有的基于学习的可变形物体操作方法，大多采用准静态动力学和拾取-放置动作，在自由端可达性方面存在限制，难以满足高速动态运动的需求。 解决方法：提出Real2Sim2Real（R2S2R）自监督学习框架，应用于平面机器人抛投（PRC）任务。该框架首先自动收集少量物理轨迹示例，利用差分进化算法调整动力学模拟器的参数，使其与物理环境相匹配；然后使用调整后的模拟器生成大量模拟示例；最后结合物理和模拟数据训练策略。同时，定义了重置过程和参数化轨迹函数，以支持该框架的运行。 所用到的数据集： 物理数据集：通过对参数化动作进行网格采样，生成522条轨迹，去除碰撞或超出关节限制的轨迹，形成物理数据集 (D_{phys}) ，并从中抽取20条轨迹组成模拟器调优数据集 (D_{tune}) 。 模拟数据集：使用调优后的模拟器，对相关参数进行网格采样，生成21,450条模拟轨迹，形成模拟数据集 (D_{sim}) 。 所进行的实验 比较模拟器调优方法：使用贝叶斯优化（BO）和差分进化（DE）两种算法对NVIDIA Isaac Gym的模拟器进行调优测试。结果表明，DE在将参数调整到接近真实值方面表现更优，其能将参数调整到与真实值相差在1%以内，因此后续选择DE进行模拟器调优。 评估模拟器性能：使用DE对PyBullet、NVIDIA Isaac Gym的两种模拟模型（分段模型和混合模型）进行调优，结果显示分段模型在最小化模拟与现实差异方面表现最佳。但对于不同电缆，各模拟器的表现有所不同，如电缆3较硬，混合模型和分段模型对其模拟效果相近。 物理实验评估策略：在物理实验中，对两种基线策略和三种基于不同数据集训练的前向动力学模型策略进行评估。结果表明，“Cast and Pull”基线策略表现最差；基于物理数据集训练的高斯过程（GP）基线在某些指标上略优于仅在物理数据集上训练的策略 (\\pi_{RD}) ，但总体误差较大；结合物理和模拟数据集训练的策略 (\\pi_{R2S2R}) 表现最佳，其中位误差相比其他策略有显著下降，但存在最大误差上升的情况，这可能是由于现实差距导致的。 英文题目：Real-Time Coordination of Multiple Robotic Arms With Reactive Trajectory Modulation 中文题目：基于反应式轨迹调制的多机器人手臂实时协调 研究背景：在自动化和先进机器人领域，多机器人手臂在共享工作空间中的协调研究至关重要，其应用广泛。但目前的协调方法存在不足，如集中式规划方法计算负担重，随着机器人和自由度数量增加，难以适用于实时场景；解耦方法则因孤立规划单个机器人运动，易忽略机器人间的细微交互，导致次优或不可行的解决方案。此外，现有方法在处理异步复杂任务时，编程或预规划轨迹的复杂性给用户带来较大负担，因此将人机交互融入多臂协调的需求愈发迫切。 所存在的问题 计算负担重：集中式多臂运动规划方法在处理大量机器人和自由度时，计算成本呈指数级增长，限制了其在实时场景中的应用。 适应性差：现有方法无论是集中式还是解耦式，对不可预见的情况适应性不足。集中式方法依赖预规划轨迹，难以应对突发状况；解耦式方法在处理机器人间复杂交互时存在局限性，可能导致任务执行效率低下或失败。 缺乏实时性和准确性：在多机器人协作任务中，现有学习示范方法在轨迹调制的实时性、准确性和处理紧密时间输入的多点方面存在缺陷，难以满足多臂协调任务的要求。 碰撞避免策略不完善：现有的碰撞避免方法，规划方法计算需求大，执行时间长；反应式方法虽计算效率高，但存在生成的运动不稳定、易陷入局部最优以及缺乏引导机器人到达目标的能力等问题。 解决方法：提出一种创新的多臂协调框架，包含基于模糊模型的运动原语（FMP）的实时轨迹调制方法和将该方法与扩展反应式方法相融合的实时多臂协调策略。基于FMP的轨迹调制方法通过从单个示范学习，能让机器人在线调整运动，精确到达可变目标位置；实时多臂协调策略使多个机器人在共享工作空间中能同时执行任务，实时避免相互碰撞。 所用到的数据集：未使用公开数据集。在研究过程中，通过机器人执行任务的演示收集数据。如在单臂轨迹调制实验中，记录机器人绘制字母的演示数据；在双臂和三臂协调实验以及复杂 construction work实验中，记录机器人执行任务过程中的相关数据，这些数据用于训练和测试所提出的方法，未提及数据的具体来源和性质。 所进行的实验 单臂轨迹调制测试：将提出的轨迹调制方法与ProMP、KMP、GP和T2FMP等四种先进的LfD方法进行比较。实验结果表明，所提方法在调制轨迹通过所有指定路径点、保持轨迹平滑性、接近原始演示轨迹以及计算效率等方面表现更优，能实现实时轨迹调制。 双臂协调测试：使用两个7自由度的Franka Emika机器人进行实验，对比所提实时多臂协调策略与文献[7]中的反应式方法和文献[8]中的规划方法。结果显示，所提策略能使机器人在执行任务时实时避免碰撞，高效完成任务；而对比方法存在任务完成时间长、机器人运动易受干扰等问题。 三臂协调测试：利用所提方法驱动三个机器人执行插销入孔任务，实验展示了机器人能根据任务阶段自动切换身份，有效避免冲突，顺利完成任务，验证了所提方法在多机器人协调执行任务方面的有效性。 复杂construction work测试：让两个机器人执行建筑装配任务，实验表明所提方法仅需用户进行简单的单个演示，就能使机器人实时规划和调整轨迹，避免碰撞和避开环境障碍物，成功完成复杂任务，相比其他方法具有明显优势。 英文题目：Robot Fine-Tuning Made Easy: Pre-Training Rewards and Policies for Autonomous Real-World Reinforcement Learning 中文题目：机器人微调变得轻松：用于自主现实世界强化学习的预训练奖励和策略 研究背景：在机器学习众多领域，预训练和微调范式通过利用现有数据或模型能快速学习新任务，这对数据收集成本高昂的机器人学习极具吸引力。但在机器人强化学习中应用该范式面临挑战：一是现成机器人数据集与本地机器人平台在物体、环境等方面存在差异，导致预训练和在线微调数据分布不一致，影响策略微调效果；二是现实世界中训练或微调策略需大量人力，包括手动重置环境和设计奖励函数。 所存在的问题 数据分布差异：不同来源的机器人数据集在诸多方面与本地机器人平台不同，使得在预训练和在线微调时，数据分布存在较大差异，难以有效微调机器人策略。 人力需求大：传统机器人强化学习训练或微调策略时，需要人工手动重置环境和设计奖励函数，耗费大量人力。 现有系统不完善：现有的机器人学习系统无法同时利用多样的演示数据集并在最少人工监督下学习，部分方法虽解决了部分问题，但仍存在不足，如未充分利用多样数据、未有效处理奖励函数学习等。 解决方法：提出ROBOFUME系统，该系统包含离线预训练和在线微调两个阶段。在预训练阶段，利用校准离线强化学习技术（CalQL）从多样的先验数据集学习语言条件多任务策略，同时通过微调预训练的视觉语言模型（VLM）构建奖励模型；在在线微调阶段，机器人基于VLM奖励模型自主交替执行目标任务和重置任务，实现策略的在线微调，减少人工干预。 所用到的数据集 Bridge数据集（BridgeDataV2）：用于预训练语言条件策略，从中选取约1000条与各任务相关行为的轨迹。 目标任务演示数据（(D_{f})）：包含目标任务的少量演示，用于帮助学习目标任务。 目标任务重置演示数据（(D_{b})）：来自将环境从目标任务结束状态重置到初始状态的任务，辅助学习。 目标任务失败状态数据（(D_{\\odot})）：由对应失败状态的图像观察组成，用于VLM奖励模型的学习，提升模型对失败状态的判断能力。 所进行的实验 真实机器人实验：在5种不同的真实机器人操作任务上评估ROBOFUME系统，包括布料折叠、覆盖、海绵抓取放置、锅盖放置和锅具放置等任务。结果表明，经过30k步自主在线交互（2 - 4小时），该方法相对于仅离线训练的性能提升了51%，平均比行为克隆（BC）基线方法高出58%，且在处理场景干扰物时表现更鲁棒。 仿真实验和消融实验：在3种模拟机器人操作环境中进行实验，对比了ROBOFUME系统与多种基线方法。结果显示，ROBOFUME系统在所有模拟任务中均优于先前方法，在200k步在线微调内成功率至少比所有基线高出20%。此外，通过消融实验验证了CalQL算法、VLM奖励模型、多样先验数据和语言条件策略对系统性能的重要性。例如，训练中使用CalQL算法至关重要，其他方法可能导致训练失败或性能不佳；VLM奖励模型比其他自动奖励函数选择表现更优；多样先验数据和语言条件策略能显著提升系统离线性能。 英文题目：Reactive Human-to-Robot Dexterous Handovers for Anthropomorphic Hand 中文题目：用于拟人化手的反应式人对机器人灵巧交接 研究背景：人机协作能力对机器人融入人类生产生活至关重要，物体交接是其中关键环节。与机器人到人的交接相比，人到机器人的交接面临更多挑战，如需应对物体的任意姿态、手部遮挡和碰撞风险等。现有研究多基于平行夹爪机器人，对拟人化手的研究较少。虽然已有研究分别在运动控制和抓握规划方面取得进展，但仍存在不足，如前者忽视机器人手的通用性和灵巧性，后者假设人类在交互中保持静态姿势。 所存在的问题 现有研究局限性：多数现有研究使用平行夹爪，在处理复杂操作时受限，且针对拟人化手的反应式交接研究较少。已有的拟人化手相关研究，要么在运动控制中忽略手的通用性和灵巧性，要么在抓握规划时假设人类静态姿势，无法满足现实需求。 实际应用挑战：现实环境中，人到机器人的物体交接存在诸多困难，包括精确的抓握规划、可靠的感知以及实时的运动控制等，同时还要考虑遮挡、碰撞和控制复杂性等问题。 解决方法：提出一种反应式人对机器人灵巧交接框架，利用拟人化手使机器人能够对人类手部运动做出反应并执行可靠的交接。该框架包含跟踪、抓握和响应三个子模块。通过基于轨迹插值的碰撞检测和结合抓握置信度、可达性和手部分类的抓握选择方法，实现安全且反应灵敏的交接；引入基于Transformer的抓握网络，以单视图点云为输入预测密集的抓握候选，实现从模拟到现实的低成本迁移。 所用到的数据集：自行合成了包含200个不同形状和大小的家用物体的数据集，涵盖5K个单视图场景、20K个点云以及超过100万个抓握注释。该数据集通过对物体进行姿态采样、静态抓握模拟、动态过滤、单视图场景渲染和配置广播等步骤生成，用于训练抓握网络。 所进行的实验 真实机器人实验：使用30个新的家用物体进行实验，验证了系统对不同物体的通用性和鲁棒性。实验结果表明，该框架的交接成功率达到80.67%，能够有效处理各种物体的交接任务，且路径规划失败率和非直接接近率较低。 消融实验：对抓握选择模块的三个组件进行消融实验，验证了各组件对系统性能的重要性。结果表明，考虑可达性和抓握分类能够提高路径规划成功率和交接自然性，合理设置各组件权重可提升系统整体性能。 框架比较实验：与之前的工作相比，新系统显著减少了视觉输入处理时间，提高了可重复性，降低了非直接接近情况的发生频率，缩短了执行时间。尽管任务复杂度增加导致成功率略有下降，但仍展示出了较高的可靠性。 用户研究：邀请8名参与者进行用户研究，从客观和主观两方面评估系统性能。客观结果显示，系统在与不同用户交互时表现出鲁棒性和可靠性；主观结果表明，参与者对系统在抓握分类选择、感知当前阶段、理解用户意图和协作等方面给予了较高评价，同时也指出了系统在安全性和运动流畅性方面的改进方向。 英文题目：Robotic Fastening with a Manual Screwdriver 中文题目：使用手动螺丝刀的机器人紧固操作 研究背景：机器人在抓取和灵巧操作方面虽有进展，但在精细操作任务上仍存在局限。工具使用对机器人在人类环境中的工作和协助至关重要，其中紧固操作在工业和家庭任务中都很常见。以往对机器人工具使用的研究在处理摩擦、顺应性和控制等方面存在不足，且较少涉及手动工具的操作。本文聚焦于机器人手臂使用手动螺丝刀进行螺丝紧固操作的研究。 所存在的问题：机器人使用手动工具进行操作面临诸多挑战，如在操作过程中需要复杂的手指动作和精细控制，现有研究对摩擦、顺应性和控制的考虑不足，导致展示出的灵巧性较低。同时，视觉在基于接触的任务中控制作用有限，且易受遮挡影响，机器人手臂的控制不准确也增加了在狭小空间操作螺丝刀的难度。此外，以往研究多针对特定形状的物体，对于螺丝与螺丝刀的适配操作研究较少，难以满足实际需求。 解决方法 螺丝刀安装到螺丝的策略：将螺丝刀安装到螺丝的操作分为建立接触、搜索螺丝槽和旋转插入三个步骤。在建立接触时，通过PID位置控制器使螺丝刀尖端下降接触螺丝头，接触后切换为阻抗控制以软化接触；搜索螺丝槽时，采用在螺丝头上滑动的方式，结合力控制和阻抗控制，防止螺丝刀尖端滑出螺丝头边界，并通过简单测试区分螺丝槽边缘和头部边界；插入尖端时，通过三次旋转操作，逐步使螺丝刀尖端完全插入螺丝槽并对准。 螺丝驱动控制：通过建立螺丝与螺纹孔的相互作用模型，计算螺丝在拧入过程中受到的力和扭矩。采用混合控制策略，在轴向方向进行位置/力控制，使螺丝前进，在其他四个方向进行导纳控制，防止螺丝刀和螺丝偏离孔的轴线。同时，利用零空间控制防止关节超出极限。 所用到的数据集：未使用公开数据集。在研究过程中，自行设定了螺丝和孔的相关参数，如螺丝的螺距、驱动宽度、大半径、小半径，孔的参数则根据螺丝参数按比例缩放。同时设定了控制参数，包括位置控制、力控制、阻抗控制和导纳控制的相关参数，用于模拟实验中的控制设定，未提及数据的具体来源和性质。 所进行的实验：在MuJoCo平台上使用7自由度的KUKA LBR iiwa机器人进行模拟实验。实验包括螺丝刀安装和螺丝拧紧的顺序执行，在安装过程中，记录了驱动搜索、尖端插入等步骤的相关数据，如接触力、扭矩等；在拧紧过程中，记录了扭矩变化、正常力以及执行时间等数据。共进行了40次模拟试验，其中20次安装试验中有15次成功插入尖端，5次因关节达到极限而失败；20次拧紧试验均完成了三次迭代操作，平均每次迭代时间为15秒。实验还验证了零空间控制的作用，未激活时多数试验容易在旋转时失败。 英文题目：Throwing Objects into A Moving Basket While Avoiding Obstacles 中文题目：避障环境下将物体投掷到移动篮子中 研究背景：机器人具备投掷能力可显著拓展其功能，如快速将物体放置到自身运动学空间外的目标位置。然而，精确投掷物体十分复杂，受物体形状、质量分布等多种因素影响，此前研究常局限于预设物体和初始条件。当环境中存在障碍物且目标篮子移动时，投掷任务难度更大，此前尚无研究解决此类问题。 所存在的问题： 分析方法的局限性：早期基于分析模型的投掷系统，依赖手工制作或机械分析来优化控制参数，难以精确建模物体、夹具和环境的物理特性，在面对动力学变化和不同物体时，泛化能力较差。 学习方法的不足：基于学习的方法虽能直接通过成功或失败信号学习任务，但以往多数研究假设投掷物体的属性已知，且未考虑存在障碍物和目标移动的情况，无法满足复杂场景需求。 解决方法： 基于强化学习的问题建模：将物体投掷问题构建为马尔可夫决策过程（MDP），并使用离策略强化学习框架求解。定义状态空间，包括机器人本体感受、障碍物和目标的位姿、释放时间、轨迹执行时长以及物体与目标和障碍物的距离等；动作空间包含初始和最终肩关节值、轨迹执行时长和释放物体时间；通过执行采样参数后的投掷轨迹确定转移函数；根据物体是否落入篮子、是否碰撞障碍物等情况设置奖励函数。 感知系统设计：在模拟环境中，利用Gazebo的服务提供必要信息；在实际机器人实验中，通过RGB - D Asus Xiton相机感知环境，利用粒子滤波器跟踪目标物体的位姿和速度，并提供世界模型服务，获取物体的相关信息，如唯一ID、位姿、速度、标签和抓取合成等。 训练与应用策略：由于在现实世界中强化学习的探索阶段不安全，先在与真实机器人设置相似的Gazebo模拟环境中训练机器人，然后将学习到的策略直接应用于真实机器人，避免了在现实环境中直接训练的风险。 所用到的数据集：在模拟实验中，使用10个具有不同材料、形状、大小和重量的日常物体，其中5个用于训练（牛奶盒、可乐罐、香蕉、瓶子、苹果），5个用于测试（啤酒罐、桃子、肥皂、品客薯片罐、芥末瓶）。在真实机器人实验中，使用5个与训练阶段模拟物体大小和形状不同的家用物体（丑陋玩具、凯蒂猫玩偶、小盒子、果汁盒、洗手液）。 所进行的实验： 实验设置：搭建模拟和真实机器人实验环境，均包含Asus xtion相机、两个配备Robotiq 2F - 140夹爪的Universal Robots (UR5e) 以及用户界面。设计三个难度递增的任务：任务1为将物体投掷到静态篮子中且无障碍物；任务2为将物体投掷到移动篮子中且无障碍物；任务3为在有障碍物阻挡路径的情况下将物体投掷到篮子中。 对比方法：采用深度确定性策略梯度（DDPG）、软演员 - 评论家（SAC）两种离策略强化学习算法，以及行为克隆（BC）方法作为基线进行对比实验。 实验结果：在所有任务中，SAC策略表现最佳。任务1中，SAC策略对已见物体的投掷成功率在模拟和真实机器人实验中分别达到94%和90%；任务2中，SAC策略对已见物体的投掷成功率在模拟实验中为91%；任务3中，SAC策略对已见和未见物体在模拟实验中的投掷准确率分别为86%和83%，在真实机器人实验中为80%。实验表明，随着任务难度增加，SAC策略与其他方法的差距更明显，且学习到的策略在真实机器人上具有良好的泛化能力 英文题目：Touch-Based Manipulation with Multi-Fingered Robot using Off-policy RL and Temporal Contrastive Learning 中文题目：基于离线策略强化学习和时间对比学习的多指机器人触觉操作 研究背景：多指机器人实现类似人类的复杂灵巧操作是一个挑战，在操作任务中，机器人需在接触和非接触状态间切换，精确感知物体状态至关重要。触觉信息虽有潜力提升机器人操作能力，但触觉传感器存在部分可观测性问题。传统基于信念状态管理的方法在高维空间效果不佳，基于循环神经网络（RNN）的强化学习（RL）虽有潜力，但在离线策略RL中存在局限性，如随着观测和动作维度增加，样本效率低、稳定性和鲁棒性差，且存在收敛问题，因此需要新的方法来解决这些问题。 所存在的问题： 部分可观测性问题：在机器人操作任务中，基于部分可观测马尔可夫决策过程（POMDP）框架，机器人难以获取环境的完整状态信息，传统方法在处理高维空间的部分可观测性问题时表现不佳。 RNN与离线策略RL结合的问题：将RNN集成到依赖价值估计自举进行学习的离线策略RL算法中，随着观测和动作维度增加，学习过程变得复杂，同时提取任务相关表示和价值函数估计变得困难，容易导致不稳定，且RNN在高维情况下有效性降低，需要大量样本和超参数调整，还存在收敛问题。 解决方法：提出一种专为离线策略RL设计的时间对比学习（TCL）方法，将其与TD3算法相结合（TD3+TCL） 。采用不同参数化的编码器处理查询和关键观测编码，利用InfoNCE损失最大化查询和关键之间的互信息，并引入正则化损失来稳定编码器输出的表示。在训练演员 - 评论家时，将时间编码器生成的表示纳入演员和评论家的输入，使策略和评论家在离线策略更新时能够结合时间信息。在离线学习设置中，使用TD3+BC作为基础RL算法，并结合TCL，记为TD3+BC+TCL。 所用到的数据集： 仿真实验：选用OpenAI Gym中的Pendulum (v1)、Lunar Lander (v2)，Adroit中的Pen (v0)，DMC Vision中的Quadruped Walk等任务，通过修改观测值引入部分可观测性。其中，Pen任务包含21维触觉信息，Quadruped Walk使用图像观测。 真实机器人实验：在“Hook pull-tab”和“Pick Screwdriver”任务中，使用本田研发的多指机器人，该机器人每个手指配备触觉传感器和6轴力 - 扭矩传感器。任务中不提供物体姿态和视觉信息，依赖机器人的触觉和本体感受数据。 所进行的实验： 仿真测试：以近端策略优化（PPO）、软演员 - 评论家（SAC）和TD3算法为基线，对比不同算法在多个任务上的表现。结果表明，基于MDP的算法在低维任务（如Pendulum任务）中表现有效；在维度稍大的Lunar Lander任务中，TD3+LSTM和SAC-LSTM表现良好；在高维任务（如Pen和Quadruped Walk任务）中，只有TD3+TCL方法能够成功学习。在离线学习实验中，TD3+TCL方法在较大维度环境中性能相对稳定。 潜在表示对比分析：在Pen(touch)任务中，对比TD3+TCL和TD3+LSTM的潜在表示与物体姿态的相关性。训练一个MLP回归模型，用两种方法的LSTM输出表示预测物体姿态。结果显示，TD3+TCL的表示与物体姿态信息相关性更强，证明其在捕捉关键空间细节方面更有效。 真实机器人测试：使用本田研发的多指机器人进行“Hook pull-tab”和“Pick Screwdriver”任务测试。在仅依赖触觉和本体感受数据的情况下，TD3+BC+TCL方法在真实机器人实验中表现优于其他算法。此外，将训练好的策略应用于不同的圆柱形物体，验证了该方法对相似形状物体的泛化能力，不同物体的成功率与训练物体相近。 英文题目：Towards Real-World Efficiency: Domain Randomization in Reinforcement Learning for Pre-Capture of Free-Floating Moving Targets by Autonomous Robots 中文题目：迈向现实世界的高效性：强化学习中用于自主机器人预捕捉自由漂浮移动目标的域随机化方法 研究背景：早期捕捉自由漂浮物体的研究多依赖预测和离线规划技术，需提前知晓目标的诸多信息，模型精度不足时效果不佳。传统抓取研究侧重静态物体，而如今对抓取自由漂浮物体的先进自主技术需求渐长，如处理受风力影响的轻质物体或微重力环境下的空间碎片等。深度强化学习（DRL）在机器人抓取任务上有一定成果，但在处理自由漂浮物体时面临挑战，如模拟与现实的差距。很多抓取系统设计为桌面场景，3自由度的抓取姿态难以满足自由漂浮物体的抓取需求，需扩展到6自由度，这带来了跟踪目标和预测触觉传感器影响等新挑战。 所存在的问题： 传统方法依赖精确模型：依赖目标形状、结构、运动轨迹等先验知识的方法，在模型不精确时效果差。 模拟与现实差距：DRL应用于机器人抓取自由漂浮物体时，难以将模拟训练的策略直接应用于现实机器人，存在模拟到现实的差距问题。 抓取任务复杂度增加：从桌面场景抓取静态物体转变为抓取自由漂浮物体，需扩展机器人动作空间到6自由度，增加了任务复杂度，包括跟踪目标和预测触觉传感器对抓取成功率的影响等难题。 解决方法：提出基于DRL的控制系统，结合触觉反馈传感器定位机器人抓手。采用软演员 - 评论家（SAC）算法，这是一种结合离策略学习和演员 - 评论家方法的DRL算法，通过熵正则化平衡探索与利用。设计包含状态空间和动作空间的学习环境，状态空间涵盖抓手和目标的位姿、速度、相对位姿和速度、最小距离及接触力等信息；动作空间表示抓手的移动和旋转。构建结合密集奖励和稀疏奖励的新型奖励函数，引导抓手准确接近并抓取目标，同时避免不必要接触。 所用到的数据集：在模拟实验中，未使用公开数据集，自行设定目标物体为尺寸(0.2m×0.04_{m})的扁平盒子 ，在训练过程中随机设置目标的初始位置和速度，每个训练episode包含500个时间步，共进行40,000个episode的训练。在真实实验中，未使用公开数据集，使用真实的Fanuc M20id机器人、Robotiq 3F抓手、Intel RealSense相机和ATI F/T传感器等设备，通过相机和传感器获取数据。 所进行的实验： 模拟实验：训练阶段，使用40,000个episode训练策略，每个episode有500个时间步，目标随机定位且速度相对抓手最大为(0.4m/s)。成功标准为在200个连续步骤中获得超过两个正奖励，训练在24,000个episode后收敛，模型取得了0.91的成功率。评估阶段，在多个随机场景测试训练好的智能体，如在两个不同初始状态的随机episode中，智能体均成功完成任务。还研究了不同最大相对速度对抓取结果的影响，发现降低最大相对速度通常能提高平均奖励和成功率。 真实实验：将训练好的智能体部署到真实机器人上，使用Intel RealSense相机和YOLOX算法确定目标的抓取点，结合机器人和触觉传感器数据，利用机器人的逆运动学计算关节角度来控制抓手运动。由于实验安全等因素，仅对静止的自由漂浮目标进行测试，通过ATI F/T传感器实现目标的自由漂浮特性。 英文题目：Bio-Inspired Rapid Escape and Tight Body Flip on an At-Scale Flapping Wing Hummingbird Robot Via Reinforcement Learning 中文题目：基于强化学习的仿蜂鸟扑翼机器人快速逃逸与紧凑机身翻转 研究背景：昆虫和蜂鸟等生物具有出色的飞行能力，能做出快速转向、360°翻转等敏捷机动动作。受生物启发的扑翼微型飞行器（FWMAVs）在实现类似动作时面临诸多挑战。尽管扑翼飞行有助于FWMAVs在紧凑空间内悬停和机动，但目前其在控制方面存在难题，导致与生物的机动性能差距较大。 所存在的问题 动力学不确定性：扑翼飞行的动力学高度非线性，且受独特的内在动态机制和不稳定的空气动力学影响，在不同飞行模式下差异显著，除悬停外很多情况的动力学特性尚不明确。 驱动限制：与飞行生物复杂强大的翼 - 胸驱动系统相比，机器人的驱动器数量和功率密度有限，在敏捷机动时控制力严重不足。 缺乏控制参考：传统飞行控制依赖明确的参考来引导飞行器，但在激进机动中，精确建立轨迹进行跟踪并不可行。在某些机动（如机身翻转）中，追求稳定性的传统飞行控制会与机动目标相悖，导致控制失效。 解决方法：提出将强化学习（RL）融入传统基于模型的飞行控制中。针对不同的激进机动动作，设计了两种RL集成方法：在传统稳定控制器能处理的机动动作中，RL辅助原有控制律提升机器人机动性；在传统控制器无法处理的动作中，RL完全接管飞行控制。设计了两种奖励函数，分别用于激励机器人完成快速逃逸和360°机身翻转动作。利用深度确定性策略梯度（DDPG）算法在高保真模拟环境中训练RL策略，同时在训练过程中采用动力学随机化方法，注入多种噪声来模拟真实系统中的误差。 所用到的数据集：未使用公开数据集。在研究过程中，自行创建模拟环境来生成训练数据，模拟环境包含完整的系统动力学，如扑翼飞行空气动力学、机翼驱动胸部动力学和飞行器机身动力学等。在训练过程中，随机化模拟飞行器的物理参数，如质量、惯性、驱动噪声、初始条件和传感噪声等，这些数据用于训练RL策略以提高其对真实环境的适应性。 所进行的实验 蜂鸟式快速逃逸实验：在模拟和真实环境中对搭载RL辅助控制策略的蜂鸟机器人进行测试，并与传统控制方法对比。实验结果显示，RL辅助的混合控制策略能使机器人完成类似蜂鸟的逃逸动作，尽管由于自身限制完成动作所需翼拍数比蜂鸟多，但相比传统控制方法，平均完成时间缩短了约0.088s，节省了约18%的时间成本。不过，RL方法的性能存在波动，而传统控制方法性能更稳定。 紧凑360°机身翻转实验：同样在模拟和真实环境中测试机器人的360°机身翻转动作。实验发现，训练后的机器人倾向于侧翻，能在约0.15s内完成翻转，且垂直位移约为一个翼展长度，接近真实动物的表现。在8次实验飞行中，6次成功完成任务，2次在恢复阶段失去稳定性。实验还表明，模拟机器人在上升能力和高度控制性能上与真实系统存在差异 。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Paper-Robot_list","slug":"Paper-Robot-list","permalink":"http://example.com/tags/Paper-Robot-list/"},{"name":"Paper-ICRA","slug":"Paper-ICRA","permalink":"http://example.com/tags/Paper-ICRA/"}]},{"title":"PaperReading-Robot_list-TRO","slug":"paper_Robot_tro_list","date":"2025-04-21T08:26:15.932Z","updated":"2025-04-29T11:22:55.793Z","comments":true,"path":"2025/04/21/paper_Robot_tro_list/","link":"","permalink":"http://example.com/2025/04/21/paper_Robot_tro_list/","excerpt":"","text":"英文题目：Design Principle of a Dual-Actuated Robotic Hand With Anthropomorphic Self-Adaptive Grasping and Dexterous Manipulation Abilities 中文题目：具有拟人自适应抓取和灵巧操作能力的双驱动机器人手的设计原理 研究背景：机器人设计的核心挑战之一是创造出具备人类抓取功能、灵巧操作能力且结构紧凑的机器人手，这在自动化工厂、服务机器人、危险环境作业和康复工程等领域有着迫切需求。早期科学家尝试开发全驱动多指灵巧手，但因使用过多的驱动器，导致此类手体积大、重量大、电路复杂且成本高昂，限制了其在实际场景中的应用。后来，自适应欠驱动机制成为欠驱动机器人手设计的重要部分，一系列基于不同技术的欠驱动机器人手不断涌现，但现有设计仍存在一些问题，如难以执行需要拇指和手指复杂协调运动的操作，且基于整体手协同的设计原则与人类手的内在分组运动特征不符，存在功能和控制差距。 所存在的问题 全驱动设计的局限：全驱动多指灵巧手虽理论上能实现与人类手相同的灵巧性，但过多的驱动器带来了尺寸、重量、电路和成本等方面的问题，限制了其实际应用。 欠驱动设计的不足：自适应欠驱动机制虽减少了电机数量，提高了手指抓取物体的自适应能力，但现有欠驱动设计存在固定的运动模式，难以实现人手的一些灵巧操作和抓取动作，且基于整体手协同的设计原则与人类手的运动特征不一致，导致机器人手与人类手在功能和控制上存在差距。 解决方法 提出双驱动设计原则：构建一种设计理论，使拇指和手指能够独立驱动，同时四个手指协同驱动，确保设计的手具有人类手的固有运动特征，为丰富的抓取功能和灵巧操作能力奠定基础。 设计单驱动差动机构：发明一种新颖的单驱动差动手掌机构，将人类的两种主要特征抓取模式融入其中，通过独特的差动元件（齿轮、拖轴和滑块元件），确保无论在抓取过程中进行何种差动过程，该机构都能复制人类手的抓取行为和姿势。 采用混合驱动方式：对于拇指的驱动，采用混合驱动方案，即拇指的屈伸由电机主动控制，外展/内收由手动调节，在保证拇指主要功能的同时，考虑到了假肢手对尺寸的严格要求。 所用到的数据集：文中未提及使用公开数据集。实验中使用的人体运动数据是从包含30名受试者和33种抓取类型的研究中提取而来，用于获取人体运动的相关特征，以验证所设计机器人手的运动传输关系与人类手的相似性。 所进行的实验 机器人手的实现与运动传输关系验证：根据双驱动设计原则，制作了集成度高的mini X-hand，并列出了关键机构参数。通过对比机器人手与人类手的运动传输矩阵，发现两者高度接近，差异极小（(\\left|U_{primary }^{robot }-U_{primary }^{human }\\right|{F}^{2} /\\left|U{primary }^{human }\\right|_{F}^{2}=0.0008)），证明设计的机器人手能在实践中复制人类的抓取特征。 拟人运动能力评估：让机器人手和人类手佩戴数据手套进行抓取实验，实验对象包括球、圆柱体和小方块等不同形状和大小的物体，涵盖多种抓取类型。通过记录关节角度数据并计算相关性，结果显示在五种抓取中，最高相关系数均超过0.91，表明机器人手在典型自适应抓取过程中能保持拟人化的抓取行为。 抓取和操作能力测试：依据Feix分类法对mini X-hand的日常抓取能力进行评估，结果表明它能完成29种抓取类型。实验还验证了机器人手可以用不同的抓取力稳定抓取物体，并且在操作能力方面，虽然存在一定限制，但能够实现部分人类手的操作功能，如操作镊子、球以及拧瓶盖等，证明了双驱动设计原则赋予了机器人手一定的操作能力。 英文题目：A Dexterous and Compliant (DexCo) Hand Based on Soft Hydraulic Actuation for Human-Inspired Fine In-Hand Manipulation 中文题目：基于软液压驱动的灵巧柔顺（DexCo）手实现仿人精细手部操作 研究背景： 机器人操作在解决劳动力短缺、降低成本等方面有广阔前景，但在灵巧操作方面与人类仍存在较大差距。人类能利用手指间及手指与环境的交互完成各种精细手部操作，而机器人要实现这些操作颇具挑战。过去几十年，机器人手的发展主要集中在高灵巧度但复杂的拟人化机器人手和简单专用的抓手上，如何在控制复杂性、柔顺性、灵巧性、运动精度和系统复杂性之间取得平衡仍是难题。 所存在的问题： 现有机器人手设计：传统两指抓手虽简单但功能有限；增加手指和关节数量的设计在任务多样性上有提升，但存在控制复杂、建模困难等问题；拟人化机器人手受驱动、结构、控制复杂性和硬件便携性等因素制约，难以获取训练数据和定义任务；现有柔顺手在复杂操作任务中优势不明显。 手驱动方式：电机驱动的机器人手控制复杂，流体驱动的虽有优势，但也存在完全驱动系统硬件和控制复杂、欠驱动系统算法和控制简单但应用受限的问题。 手感知方面：灵巧手的位置和力感知影响算法可靠性，现有外部感知方法依赖外部设备，内部感知方法受安装位置和尺寸限制，新兴感知方法存在精度、滞后和耐久性问题。 解决方法： 提出DexCo手系统，包括以下关键部分： DexCo手设计：模仿人类拇指和食指的操作特征，设计具有拟人化结构和运动学的手指，每个手指有三个自由度（两个用于弯曲，一个用于外展/内收），手掌可线性运动，增加操作灵巧性；采用软液压驱动，利用折纸式执行器，实现精确、柔顺和双向驱动，通过弹性力和静水力建模优化设计；使用线性滑动电位器和IMU进行本体感知，获取关节角度信息。 DexCo手建模：从单指和整手的运动学角度对灵巧性和可操作性进行建模，通过定义可操作性指标评估手部运动性能；对DexCo手的柔顺性进行建模，分析在抓取和操作过程中的变形和力的关系。 实验验证与控制：通过实验验证DexCo手的硬件和本体感知系统性能，包括液压执行器局部柔顺性、本体感知准确性和稳定性、抓取力、抓取周期时间、手指力、手指重复性和扭转力等方面的测试；设计速度和位置遥操作控制器，实现对DexCo手的有效控制。 所用到的数据集： 文中未提及专门用于训练或评估的公开数据集，主要是通过自行设计实验，对DexCo手在不同任务和测试中的表现进行数据收集和分析，如在抓取力实验中，使用3D打印的不同形状和尺寸的工件进行测试，并记录相关数据；在手指强度实验中，对手指1和手指2在不同运动类型下的力量数据进行收集和对比等。 所进行的实验： 液压执行器局部柔顺性验证：将折纸式执行器按在机器人手中的锁定方式固定，一端连接单轴力传感器，另一端连接线性滑块。由注射器泵驱动执行器在一定长度范围内运动，线性滑块在不使执行器严重屈服的情况下对其进行拉伸和压缩，重复测试三次，记录各位置的平均受力。 本体感知验证：分为准确性和稳定性测试。准确性测试中，DexCo手摆出不同姿势，收集传感器数据和运动捕捉系统数据，将运动捕捉系统数据作为真实值，与IMU数据对比评估IMU估计手部姿势的准确性；稳定性测试中，DexCo手保持不同静止姿势，收集2分钟IMU数据，分析数据漂移情况。 抓取力验证：使用3D打印的工件，分为捏合和包裹抓取两种方式，在工件两端固定力传感器，通过位置控制使手达到期望配置，多次循环抓取，去除每个周期非零数据的首尾10%，计算中间80%数据的平均合力等统计量。 抓取周期时间验证：实验设置和过程与抓取力测试相同，记录每次抓取中从完全打开到抓取完成再打开的时间，计算每个工件的抓取周期时间。 手指力验证：每个手指推压固定在地面的捏合盒，通过位置控制使手达到期望配置，多次重复打开和关闭动作，收集准静态期的平均力，对比新制造的手指1和使用两天且液压执行器有气泡的手指2的力量表现。 手指重复性验证：使用线性位移传感器测试手指从相同方向到达相同位置的一致性，手指在工作空间内移动到四个独特位置后返回起始位置，多次循环，记录位移数据评估重复性。 DexCo手扭转力验证：引入扭转力基准测试，DexCo手先完全打开，再完全关闭达到最大抓取力后进行扭转运动，使用扭矩传感器测量准静态条件下的最大扭矩，测试不同尺寸工件的扭转力。 精细手部操作实验：展示DexCo手在多种精细手部操作任务中的能力，如组装灯泡、打开卡片盒、分拣药丸、在杂乱环境中拾取物品、数卡片、打开塑料袋、进行包络操作和手部内旋转等任务。 英文题目：A Multitentacle Gripper for Dynamic Capture 中文题目：用于动态捕获的多触手抓手 研究背景：在自然界，生物捕获动态目标颇具挑战，像猎豹、猎鹰等捕食成功率并不高，人类也难以保证接住高速飞行的球。在工程领域，随着机器人应用场景向复杂非结构化环境拓展，动态捕获的需求日益增长，如机场捕捉入侵无人机、水下采集生物样本、太空清理碎片等。实现动态目标捕获的关键在于目标估计以及捕获装置设计。 所存在的问题 刚性抓手的局限：刚性抓手和机械手臂虽借助精确传感与复杂算法能主动捕捉运动目标，但碰撞时的冲击力易使物体反弹，定位或定向的微小误差也会致使任务失败。 现有柔性捕获方式的不足：基于网或系绳的捕获方法虽能扩大安全半径，却降低了可控性，难以应对目标轨迹变化；受大象鼻子启发的触手状软连续体操纵器在控制上存在难题，其适应性和可靠性有待提升；软机器人手和抓手在抓取来自不同方向的飞行物体时适应性欠佳。 解决方法 设计多触手抓手：从海葵的捕获原理获取灵感，设计出包含12个连续体臂（分主动臂和被动臂）和可展开基座的多触手抓手。主动臂位于外层，用于主动包围捕获目标；被动臂在内层，负责碰撞目标、吸收动能并适应目标形状，辅助完成捕获。 创新可展开基座：借鉴折纸和Sarrus机构设计可展开基座，其能同步运动，调整连续体臂的相对位置和角度，扩大工作空间，辅助手臂挤压目标。 搭建控制系统：在可展开基座内安装惯性测量单元（IMU），用于检测与动态目标碰撞产生的加速度突变。控制系统在接收到碰撞信号后，驱动气缸完成捕获动作。 所用到的数据集：文章未提及使用公开数据集。实验采用自行准备的多种运动状态的目标物体，如网球（直径68mm，质量58g）、篮球（儿童用，直径130mm，质量126g）、30g的纸巾包、装有半瓶水的饮料瓶以及活金鱼等。 所进行的实验 高容错性实验：通过调整KUKA机器人末端执行器与自由落体球的相对位置和角度，改变球与抓手的接触位置、速度和角度，用网球和篮球进行实验，评估抓手对不同运动状态球的捕获成功率，以此验证其对动态目标的高容错性。 强鲁棒性实验：在部分连续体臂被移除的极端情况下开展实验，改变移除臂的数量和位置，用网球和篮球进行自由落体实验，测试抓手的捕获成功率，进而验证其鲁棒性。 动态捕获适应性实验：利用KUKA机器人搭载抓手，对随机抛出的纸巾包、饮料瓶以及水下的活金鱼进行捕获实验，展现抓手对不同形状和随机轨迹目标的良好适应性。 英文题目：Adaptive Fingers Coordination for Robust Grasp and In-Hand Manipulation Under Disturbances and Unknown Dynamics 中文题目：在干扰和未知动力学下用于稳健抓握和手内操作的自适应手指协调 研究背景：在机器人领域，手内操作是指在手中重新定位物体的能力，使用多自由度机器人手进行更复杂物体的抓握和操作成为研究热点。然而，在实际手内操作中，确保物体稳定和任务成功依赖于手指动力学的精确同步，面对系统机械属性的不确定和不精确估计时，稳健地执行手内操作颇具挑战。例如，当物体质量分布不均匀或变化时，传统的抓握和操作方法难以应对，同时外部干扰也会增加操作难度。 所存在的问题 手内操作面临的挑战：在实际执行手内操作时，由于对物体属性、接触力学和机器人动力学的了解不精确，导致任务规划复杂且反馈控制律效率低下。 现有方法的不足：在规划抓握和操作方面，如Shi等人控制手指加速度的方法推导非层流物体加速度曲线困难；Sundaralingam和Hermans的方法依赖物体形状知识，在实际应用中面临挑战。在控制抓握和操作方面，基于物体动力学调制手指阻抗的方法，存在需要确保零空间、精确估计抓握矩阵等问题。数据驱动方法虽能应对部分挑战，但在处理扰动时存在困难，且依赖完美模拟环境。此外，手内操作还需确保物体在手指可及范围内，以往研究大多考虑单步操作，对于质量分布不均匀或变化的物体，重新抓握几乎不可能。 解决方法 基于耦合动力系统的手指同步：提出一种基于耦合动力系统（DSs）的控制策略，通过引入中间动力学调节手指相对速度，实现手指运动同步。设计中间变量z及其动力系统，使手指能根据物体状态调整运动，确保所有手指遵循特定期望轨迹，使物体达到期望姿态。 关节空间自适应控制器：采用基于扭矩的控制方法，提出关节级自适应扭矩控制器，通过估计和训练自适应控制增益，在线调整关节扭矩，以跟踪手指期望轨迹并调节关节阻抗增益，补偿机器人动力学、物体 - 手指相互作用和物体物理属性的不确定性。 抓握和操作优化：在抓握阶段，通过启发式估计稳定抓握配置，利用接触扳手构建物体级阻抗，优化接触力以稳定物体；在手内操作阶段，根据抓握矩阵和期望物体速度重新定位吸引子，实现物体的操作。同时，通过触觉观察估计接触框架，为抓握优化提供信息。 所用到的数据集 接触框架估计数据集：收集BioTac电极阻抗、指尖基部方向和接触平面方向数据，用于训练人工神经网络（ANN）模型，以估计接触框架。数据集包含40000个样本，按75%和25%的比例划分为训练集和测试集。 从人类演示学习数据集：记录人类演示中物体的姿态、手指位置和接触力，通过OptiTrack运动捕捉系统以250Hz记录位置信息，用TPS触觉传感器以50Hz记录接触力信息。这些数据用于识别任务段、抓握位置和手指角色。 所进行的实验 协调手指控制评估：通过让指尖从静止位置移动到准备位置，再跟踪特定吸引子序列，测试控制器性能，观察到跟踪误差降低且关节刚度能调节到期望值。将该控制算法与基于阻抗的控制器对比，在不同手部姿态下进行跟踪任务，结果表明该控制器在精度上更具优势。 不确定环境下的抓握适应评估：使用Allegro手抓取不同属性的物体，在手腕旋转时，通过OptiTrack系统记录物体质心相对于手部框架的运动，评估抓握鲁棒性。实验发现控制器能适应物体惯性变化，稳定抓握物体，并能通过重新估计接触法线向量调整接触力。 手内操作准确性和鲁棒性评估：对不同形状和质量属性的物体进行手内平移和旋转操作实验，以齿轮箱装配任务为例展示手内操作应用。实验结果表明该控制方法跟踪误差低，相比之前研究，在位置和方向误差上表现更优。 手指步态实验：通过学习人类演示，利用HMM提取手指角色和任务段，使用该控制方法进行手内物体滚动实验。实验在真实机器人手上实现了类似人类的操作，虽存在部分失败情况，但整体成功率较高，且在面对多种干扰时表现出鲁棒性。 英文题目：Autonomous Learning of Page Flipping Movements via Tactile Feedback 中文题目：通过触觉反馈自主学习翻页动作 研究背景：操作技能是人类的重要能力，在需要灵巧操作的任务中，触觉对于预测操作过程中的关键状态转换起着重要作用。当视觉被遮挡时，触觉的重要性更加凸显。为使机器人具备类似人类的灵巧操作能力，给机器人配备触觉传感器至关重要。然而，过去由于触觉传感硬件技术的限制，解决机器人在操作过程中的学习挑战存在困难。传统触觉传感器变形能力低，难以满足需要详细了解手指 - 表面相互作用的任务需求。尽管近年来开发了一些可变形的触觉传感器，但在更一般形式的物体操作，尤其是对高度可变形物体（如纸张）的操作中，其应用仍较为罕见。 所存在的问题 触觉传感器与物体相互作用建模困难：当触觉传感器和被操作物体都可变形时，它们之间的动力学相互作用难以精确建模。 基于触觉信息量化操作任务功能性能困难：在大多数情况下，仅依靠本质上局限于手指 - 物体相互作用的触觉信息，很难量化操作任务的整体功能性能。 现有方法应用局限：目前对可变形物体的操作主要依赖视觉传感，触觉传感多应用于物体属性分类或形状估计，在可变形物体操作方面应用有限。且现有方法要么需要复杂的物体内部物理状态模型，难以在实际机器人实验中部署；要么需要精确的辅助传感机制，而非单纯依靠触觉传感。 解决方法 基于强化学习学习名义轨迹：采用无模型强化学习过程，通过动觉教学由人类演示引导，利用基于触觉信号和运动跟踪数据的奖励函数，学习翻页任务的名义轨迹。具体使用动态运动原语（DMPs）作为轨迹的参数表示，通过调整DMPs的参数来重现演示轨迹，并使用基于模型的相对熵策略搜索（MORE）算法对轨迹进行优化。 基于感知耦合适应轨迹：在学习了名义翻页轨迹后，基于感知耦合的原理学习一个额外的触觉反馈项，以将名义轨迹适应不同大小的笔记本页面（不同任务情境）。假设不同页面大小的功能行为对应的名义传感轨迹相似，通过最小化当前传感轨迹与名义传感轨迹之间的差异来学习适应策略。 选择合适的触觉传感轨迹表示：考虑了两种触觉传感轨迹的表示方法，一种是基于生物启发触觉传感器人工顶毛的部分通道表示，另一种是基于主成分分析（PCA）特征值的表示。通过实验对比，选择能在有效学习的触觉信息丰富度和在实际机器人上部署的计算可处理性之间达到平衡的表示方法。 所用到的数据集 用于学习名义轨迹的演示数据：通过动觉教学记录人类演示的关节角度和关节角速度数据，采样频率为50Hz，用于初始化DMPs的参数。 触觉传感器数据：使用BioTac触觉传感器记录触觉数据，包括低频压力（(P_{dc})）和19个阻抗电极（(E)）的数据，采样频率为100Hz。在实验中，通过分析这些数据来评估不同翻页轨迹下的触觉信号特征，以及用于定义奖励函数和学习适应策略。 标记跟踪数据：使用被动运动捕捉标记和六个T - Series相机以100Hz的频率跟踪装有笔记本页面的刚性装订器的位移，用于评估任务性能和在学习名义轨迹时定义奖励函数。 所进行的实验 学习名义轨迹实验：使用7自由度机器人手臂和配备BioTac触觉传感器的夹爪，对两种不同大小（小页面：8.5”×11”；大页面：11”×11”）的笔记本页面进行翻页实验。通过动觉教学获取初始轨迹，然后使用MORE算法和设计的奖励函数优化轨迹。结果表明，机器人能够学习到避免页面翘曲和折断的翻页轨迹，且触觉状态在学习过程中起主要作用。 触觉传感轨迹表示影响实验：在简化的轨迹适应学习子问题中，研究两种触觉传感轨迹表示方法（人工顶毛表示和PCA特征值表示）对奖励函数样本值的影响。实验结果表明，PCA特征值表示能够更好地区分功能翻页轨迹和非功能轨迹，且使用该表示方法学习到的适应轨迹更温和，对装订器的位移影响更小。 完整轨迹适应实验：采用PCA特征值表示法，对学习到的名义轨迹进行完整的适应实验，将为大页面学习的名义轨迹应用于小页面，且不提供小页面目标位置的先验信息。实验结果表明，通过触觉驱动的感知耦合，机器人能够成功地将大页面的名义轨迹适应小页面，学习到合适的目标位置和翻页轨迹，使触觉传感轨迹收敛到理想情况。 英文题目：Do You Need a Hand? – A Bimanual Robotic Dressing Assistance Scheme 中文题目：需要帮忙吗？——一种双机器人穿衣辅助方案 研究背景：全球护理人员短缺，人口老龄化加剧，辅助机器人有望缓解这一问题，其中机器人辅助穿衣是极具挑战的任务。该任务涉及与柔软衣物和人体的直接物理交互，与传统人机交互任务不同，其存在交互力难以获取、手臂运动估计困难等问题。以往研究多采用单机器人进行穿衣辅助，常假设手臂静态姿势，且大多依赖视觉跟踪手臂姿势，但在穿衣过程中，视觉易受遮挡，导致现有视觉算法失效。 所存在的问题 单机器人辅助的局限性：多数现有机器人穿衣策略采用单机器人，难以充分应对穿衣任务的复杂性，且通常假设手臂处于静态姿势，这在实际中很难满足。 手臂姿势跟踪困难：穿衣过程中存在严重遮挡，基于视觉的深度学习姿势跟踪算法难以准确跟踪手臂姿势，额外的传感模块虽能辅助，但仍存在问题。 交互力获取难题：衣物柔软易变形，使得在穿衣过程中难以获取直接的交互力反馈，这增加了穿衣策略制定的难度。 解决方法 提出双机器人协作方案：采用两个机器人的协作框架，一个交互机器人与人类牵手，支持和引导人类手臂运动，辅助穿衣；另一个穿衣机器人负责执行穿衣任务，通过这种方式解决单机器人辅助的局限，并利用交互机器人实现仅通过本体感受传感器就能跟踪手臂姿势。 设计基于肘部角度的最优伸展策略：分析发现肘部角度是影响穿衣策略的关键特征，根据该特征为交互机器人设计最优伸展控制器，通过笛卡尔阻抗控制，使引导力方向与连接手和肩膀的方向一致，以增大肘部角度，便于穿衣。 定义穿衣坐标并学习穿衣策略：定义依赖于手臂姿势的穿衣坐标，将机器人在笛卡尔坐标中的运动转换到该坐标下，利用高斯混合模型（GMM）和高斯混合回归（GMR）从专家演示中学习穿衣策略，提高策略的灵活性和适应性。 所用到的数据集：实验中使用了多种数据，包括通过运动捕捉设备（如XSens运动捕捉套装）收集的人体手臂运动数据，用于计算姿势估计方案中的权重矩阵Q；通过Franka机器人进行示教收集的不同手臂姿势下的穿衣路径和手臂姿势数据，用于研究肘部角度对穿衣策略的影响；在不同实验中记录的机器人穿衣过程中的数据，如穿衣路径、手臂姿势、交互力等，用于评估和改进穿衣辅助方案。 所进行的实验 肘部角度对穿衣策略的影响实验：通过Franka机器人进行运动学示教，收集不同手臂姿势下的穿衣路径和手臂姿势数据，将穿衣路径投影到手臂平面上进行分析。实验结果表明，当肘部角度较小时，专家演示更倾向于采用外策略，验证了肘部角度对穿衣策略的影响。 姿势估计方案评估实验：使用XSens运动捕捉套装收集手臂运动数据作为真实值，计算不同情况下的肘部估计误差。结果显示，该方案的最大误差约为3cm，与其他方法相比，性能较好，能够满足穿衣任务的需求。 交互式穿衣实验：使用三种不同类型的衣服（短袖刚性衬衫、长袖刚性衬衫和长袖柔软衬衫）在人体和假人上进行实验，对比基于穿衣坐标的方法和传统的TPGMM方法。实验结果表明，基于穿衣坐标的方法成功率更高，且在人体不完全配合的情况下仍能成功穿衣，展示了该框架的鲁棒性和灵活性。 英文题目：Dual-Critic Deep Reinforcement Learning for Push-Grasping Synergy in Cluttered Environment 中文题目：用于杂乱环境中推抓协同的双评论家深度强化学习 研究背景：机器人抓取在非杂乱环境中已取得不错成果，但在密集杂乱环境中仍面临诸多挑战。在这种环境下，物体相互遮挡，可操作空间减少，严重影响机器人抓取效率。为解决这些问题，研究者引入了推抓协同策略，即通过额外的推行动作辅助抓取，但现有方法仍存在一些未解决的问题。 所存在的问题 推行动作冗余：现有推抓协同方法中，推行动作存在冗余现象，导致动作效率低下，不能有效辅助抓取，例如有些推行动作只是为了执行而执行，没有真正为抓取创造有利条件。 未能充分利用视觉特征：部分方法在处理视觉特征时不够全面，手动设计的规则在特定条件下可能失效，导致错误的指令，影响抓取成功率。 目标导向任务难度大：在目标导向的抓取任务中，从环境中区分出目标物体较为困难，现有方法在处理此类任务时，难以高效地实现对目标物体的抓取。 解决方法 提出双评论家深度强化学习框架：引入两个不同的深度Q学习评论家，Critic I基于视觉解释选择最佳行动方案，Critic II评估当前状态 - 行动配对的成功率，以此优化推抓协同，提高机器人动作的准确性和效率。 设计双步学习和多阶段训练机制：通过双步学习过程优化推行动作的训练奖励函数，增强推行动作的目的性。多阶段训练包括抓取训练、推行动作双步学习训练以及协同动作训练，逐步提升模型性能。 改进网络结构和训练设置：采用预训练的DenseNet - 121作为特征提取器，PushNet和GraspNet使用特定的全卷积网络结构，Critic II使用三层CNN。训练过程中采用Huber损失和BCEloss作为损失函数，利用优先经验回放提高训练效率，并设置了合适的超参数。 所用到的数据集：文中未提及使用公开数据集。模拟实验中使用了9种不同形状和颜色的3D模型作为实验对象；真实世界实验中使用了玩具积木和多种标准办公物品进行测试，通过Intel RealSense D435相机获取分辨率为1280×720的RGB - D图像作为感知数据。 所进行的实验 模拟实验：在CoppeliaSim中使用UR5服务机器人和RG - 2夹爪进行实验，设定了目标无关和目标导向两种任务类型。目标无关任务又分为随机设置和挑战案例，随机设置包含15、20、25和30个随机放置的物体，挑战案例设计了9个不同复杂度的案例；目标导向任务同样分为随机和挑战案例，随机案例有30个物体且随机选择目标，挑战案例指定特定目标。实验结果表明，该框架在抓取成功率和动作效率等指标上优于对比方法，尤其在挑战案例中优势明显。 真实世界实验：使用UR5服务机器人和ROBOTIQ - 85夹爪，通过Intel RealSense D435相机获取数据。实验集中于目标无关抓取任务，使用玩具积木和新的办公物品测试模型的泛化能力。结果显示，该框架在真实场景中同样表现出色，在抓取成功率和动作效率方面优于VPG方法。 英文题目：Dual-Fingered Stable Grasping Control for an Optimal Force Angle 中文题目：用于优化力角的双指稳定抓取控制 研究背景：机器人抓取研究需满足力/扭矩闭合以及力角在摩擦锥范围内这两个条件，以确保物体被稳定抓取。早期多指机器人手的研究多关注手部模型开发和空间约束，忽视了力角条件；部分考虑力角的方法对物体形状有限制，或依赖精确的物体模型和参数；基于触觉传感器的研究虽能实现稳定抓取，但存在未考虑力角问题或无法保证系统动态稳定性等不足。因此，开发适用于任意形状物体的抓取控制方法具有重要意义。 所存在的问题 早期研究忽视力角条件：早期多指机器人手的研究主要关注类似人类手部外观的多关节手指模型开发，采用开环控制系统，仅考虑力/扭矩平衡，假定力角条件自然满足，未直接关注力角问题。 部分方法对物体形状有局限：一些考虑力角的控制方法，如针对具有特定形状（如平面、圆形）物体的方法，无法应用于任意形状物体或形状未知的物体。 现有方法存在其他缺陷：基于触觉传感器的方法，有的忽略动态行为，有的虽考虑动态但未考虑力角问题，有的无法保证系统动态稳定性和提供抓取力角的最优解。 解决方法 提出新型控制方法：提出一种双指机器人的抓取控制方法，通过优化力角提高抓取稳定性，使其适用于任意形状物体。该方法基于指尖与物体表面的滚动接触约束，仅要求物体在接触点附近具有光滑曲率。 设计控制器：设计的控制器为(u=-k_{v}\\dot{q}-f_{d}J_{f}^{T}(q)[cos\\phi_{0} cos\\phi_{0} sin\\phi_{0} sin\\phi_{0}]^{T}) ，其中(k_{v})和(f_{d})为正常数，第一项用于提供阻尼，第二项用于建立期望抓取力并抵消物体的转动力矩。该控制律仅需测量关节角度、关节角速度、接触角和手指运动学参数，无需物体信息、预规划和力传感器。 进行稳定性分析：运用Lyapunov直接方法证明所提控制器的稳定性。通过构造合适的Lyapunov函数，并对其求导分析，得出在满足一定条件时，系统的平衡点是一致渐近稳定的，从而保证了抓取的稳定性。 所用到的数据集：本文未提及使用公开数据集。在仿真实验中，自定义了相关参数来模拟双指机器人抓取任意形状物体的场景，包括设置物体的形状参数（如曲率中心位置、曲率半径）、机器人手指的参数（如各关节长度、指尖半径、质量、转动惯量）以及初始状态参数（如初始关节角度、初始力角）等。 所进行的实验：通过仿真实验对比验证所提控制方法的性能。实验设置两个具有3个自由度的平面手指，采用半球形指尖抓取一个具有两个曲率表面的任意形状物体。在仿真中，设定了物体、手指的相关参数以及控制器的参数（如阻尼增益(k_{v}=0.008) ，期望抓取力(f_{d}=0.8N) ）。结果显示，所提方法能够实现动态力/扭矩平衡，且力角渐近收敛到等效值，相比之前的控制方法，能更好地实现稳定抓取。 英文题目：Enhancing the Universality of a Pneumatic Gripper via Continuously Adjustable Initial Grasp Postures 中文题目：通过连续可调的初始抓取姿势提升气动抓手的通用性 研究背景：机器人操纵器和抓手在工业自动化和日常生活辅助中应用广泛。传统刚性抓手在抓取时需精确计算目标位置和几何形状，成本高且通用性受限，还易损坏易碎物品。软机器人因具有重量轻、安全性高、控制复杂度低和自由度高等优点成为新兴领域，其中气动软抓手因成本低、驱动简单而备受关注。近年来虽有众多研究致力于提升气动软抓手的抓取性能，但在抓取尺寸范围广泛的物体方面仍存在局限。 所存在的问题：现有软气动抓手在抓取不同尺寸物体时存在局限性，部分研究虽对抓手的某些性能进行了改进，如增加运动范围、提升抓取力等，但仍无法用同一抓手有效抓取尺寸跨度大的各类物体。一些改进方法需要手动调整，难以满足自动化需求。 解决方法 设计新型气动软-硬混合多指抓手：提出一种气动驱动的软-硬混合多指抓手，通过调整初始抓取姿势来扩大抓取范围。该抓手由四个模块组成，每个模块包含距离调节、角度调节和手指三个气动执行器及刚性连接器。 优化执行器设计：距离调节执行器采用软-硬混合设计，通过内部软执行器和外部刚性外骨骼增强轴向和弯曲刚度；角度调节执行器使用织物增强的弯曲弹性体执行器，保证弯曲刚度；手指执行器采用宽度递减设计，增加与物体的接触面积，提高抓取性能。 采用模块化设计：模块化设计便于维护和更换组件，通过3D打印的固定结构将各模块组合在一起，使抓手结构紧凑，易于安装在机械臂上。 所用到的数据集：本文未使用公开数据集。实验中自定义了相关数据，如通过3D打印制作不同尺寸的半圆形结构，组合成直径为6cm、10cm和14cm的圆柱体，用于测量抓手的抓取力；从Yale - Columbia - Berkeley（YCB）物体集中选择工具类和形状类物体，以及一些日常生活中的物体，用于测试抓手的实际抓取能力。 所进行的实验 执行器性能测试实验：对距离调节执行器、角度调节执行器和手指执行器分别进行性能测试。包括测量距离调节执行器的伸长量与充气压力的关系、轴向及弯曲扭转刚度；测量角度调节执行器的弯曲角度与压力的关系和弯曲刚度；通过模拟和实验对比四种不同类型手指执行器的曲率变化和指尖力。 抓取能力测试实验：通过设置不同的初始抓取姿势，测试抓手对不同尺寸、形状和重量物体的抓取能力。使用自制的测量装置，定量测量抓手对不同直径圆柱体的横向抓取力和最大垂直拉力；对比具有锥形手指（type 4）和直手指（type 1）的抓手的抓取能力；对YCB物体集和日常生活中的物体进行抓取实验，验证抓手在实际应用中的抓取能力。 英文题目：From Simulation to Reality: A Learning Framework for Fish-Like Robots to Perform Control Tasks 中文题目：从仿真到现实：一种用于仿鱼机器人执行控制任务的学习框架 研究背景：仿鱼机器人作为典型的仿生自主水下航行器，凭借其生物启发的结构和仿生运动方式，在机动性和低噪音方面具有优势，在水下探测等领域有广泛应用前景。然而，由于其在游泳过程中存在复杂的流固相互作用，难以建立精确的动力学模型，导致基于模型的控制方法效率低下。强化学习（RL）虽为解决机器人控制问题提供了新途径，但传统RL在复杂控制任务中存在维度困境，深度强化学习（DRL）虽有所改进，但在应用于实际机器人时，因仿真与现实存在差距，导致训练的策略难以直接在现实中应用。因此，设计一个能快速训练DRL控制策略且适用于实际水下机器人的学习框架具有重要意义。 所存在的问题 建模困难：仿鱼机器人在游泳时，流固相互作用复杂，其动力学模型难以精确建立，甚至无法建立，使得基于模型的控制方法难以有效设计控制策略。 RL应用受限：传统RL在处理复杂控制任务时，存在维度困境，难以满足高精度要求的任务。早期将RL应用于仿鱼机器人控制的研究虽有开创性，但仍面临诸多挑战。 仿真与现实差距：将DRL训练的策略从仿真转移到现实时，由于模拟器难以准确反映现实情况，导致策略在现实中往往失效，这限制了DRL在实际移动机器人中的应用。 解决方法 构建仿真系统：提出Self-Switching-Simulator（Tri-S）系统，结合基于计算流体动力学（CFD）的环境和数据驱动的代理环境。CFD环境采用HyperFLOW软件构建，能逼真模拟水下机器人与流体环境的相互作用；代理环境依据物理实验数据构建，可快速训练控制策略，两者结合有效平衡了仿真精度和计算速度。 设计训练方法：设计基于DRL的训练方法，采用优势演员-评论家（A2C）算法，结合价值函数和策略搜索。训练过程分两个阶段，先在代理环境中训练，当平均奖励达到预设阈值后，自动切换到CFD环境继续训练，直至奖励超过完成阈值，从而获得可直接应用于物理机器人的控制策略。 选择合适任务验证：选取路径跟踪控制任务和姿态控制任务来验证框架的有效性。路径跟踪控制任务要求机器人跟踪预定义路径，姿态控制任务要求机器人在无预设路径的情况下达到目标姿态，通过这两个典型任务全面验证框架在不同类型任务中的表现。 所用到的数据集：本文未使用公开数据集。实验中使用的数据主要源于物理仿鱼机器人的实验，包括在不同控制输入下机器人的运动数据，如线性速度、位置变化、姿态变化等。通过对这些数据的采样和处理，构建代理环境中的映射关系，用于训练和验证控制策略。 所进行的实验 路径跟踪控制实验：在仿真中训练机器人跟踪直线，将训练得到的策略(\\pi_{Pa}{*})部署到物理机器人上，让其跟踪随机生成的贝塞尔曲线，并与对比DRL策略(\\pi_{Pa}{s})和视线（LOS）控制器(u_{Pa}{*})进行比较。实验结果表明，(\\pi_{Pa}{*})在平均跟踪误差、标准误差和最大跟踪误差等指标上表现更优，验证了学习框架在路径跟踪控制任务中的有效性和从仿真到现实的转移能力。 姿态控制实验：该任务要求机器人同时实现位置和方向的控制，是一个更具挑战性的多目标问题。通过引入课程学习方法处理稀疏奖励问题，在仿真中训练得到策略(\\Pi_{Po}^{}) ，并在物理机器人上进行实验。实验结果显示，(\\Pi_{Po}^{})能成功控制机器人完成姿态控制任务，在不同难度任务下的最终位置误差和最终方向误差均较小。 英文题目：Grasping Living Objects With Adversarial Behaviors Using Inverse Reinforcement Learning 中文题目：利用逆强化学习抓取具有对抗行为的活体目标 研究背景：机器人抓取是机器人学的重要研究课题，传统研究多聚焦于非活体目标，而活体目标具有对抗行为，其运动和变形难以预测与建模，这使得抓取活体目标成为具有挑战性且尚未充分探索的问题。现有针对非活体目标的抓取算法及强化学习（RL）抓取方法，在面对活体目标时会因目标的对抗行为而失效，且可能产生较大接触力，损伤目标或机器人抓手。 所存在的问题 建模与预测困难：活体目标的对抗行为使其运动和变形难以精确建模和预测，增加了抓取的难度。 稀疏奖励问题：传统的稀疏奖励机制难以区分对抗行为和正常行为，无法有效引导机器人学习高质量的抓取策略。 忽视手部结构：传统基于RL的控制器在设计抓取策略时，未充分考虑机器人手部结构，难以实现手指关节的协同作用，影响抓取效果。 解决方法 两阶段RL算法：将抓取活体目标问题分解为预抓取和抓持两个阶段，分别用RL学习相应策略。预抓取阶段关注目标的对抗行为，寻找合适抓取姿态；抓持阶段计算高质量手指动作，在稳定抓持的同时尽量减小接触力。 逆强化学习获取奖励函数：在预抓取阶段，采用逆强化学习（IRL）从活体目标的对抗行为中学习奖励函数，将其负值加入抓取智能体的奖励函数，引导机器人对抗目标的逃避行为，提高抓取成功率。 动态模型学习：利用高斯过程在线学习动力学模型，以估计背景分布，为计算对抗奖励的梯度提供支持，同时采用选择性遗忘机制，解决数据增多时模型计算复杂度增加的问题。 图卷积网络优化抓持策略：在抓持阶段，将机器人手部状态表示为图，采用图卷积网络（GCN）处理状态图，生成高质量抓持动作，使不同手指关节能协同工作，同时设计密集奖励函数，鼓励长时间稳定抓持并惩罚大接触力。 所用到的数据集：本文未使用公开数据集。实验数据主要源于模拟环境和真实环境的测试。在模拟环境中，使用PyBullet和OpenAI Gym框架，将活体目标模拟为三连杆铰接机器人；在真实环境中，使用UR10机器人和机动玩具鱼进行实验，通过RGB-D相机获取目标姿态数据，并利用点云配准算法处理数据。 所进行的实验 对抗奖励性能实验：通过对比成功和失败抓取情节中对抗奖励的值，验证其能在抓取过程中提前区分成功与失败，为智能体提供密集反馈信号。让人类控制机器人抓取并记录数据，与奖励函数输出对比，结果表明奖励值趋势与人类判断的抓取概率趋势相符，且使用对抗奖励训练的智能体成功率更高。 预抓取阶段性能实验：对比本文方法与一阶段RL、Q学习的奖励，结果显示本文方法能学习到更有效的抓取策略，奖励值更高。与工业抓取方法和QT-Opt方法比较，本文方法在处理活体目标快速姿态变化时表现更优，成功率更高。 抓持阶段性能实验：对比图RL算法和普通RL算法，图RL算法奖励更高，表现更优。对比本文学习的策略和手动设计的“关闭”策略，本文策略的接触力更小，成功率更高，且实验验证了所提图结构对提高抓取性能的有效性。 泛化能力实验：在多种不同的活体目标上测试训练的策略，结果表明对与训练目标相似的物体成功率较高，对差异较大的物体微调后成功率显著提升，验证了方法的泛化能力🔶13-1 英文题目：Heavy Material Handling Manipulator for Agricultural Robot 中文题目：用于农业机器人的重型物料搬运机械臂 研究背景：在农业领域，存在大量搬运重物的作业，如收获西瓜、南瓜等蔬菜以及搬运有机肥料袋等，这些工作劳动强度大。随着日本老龄化加剧，对相关技术的需求更为迫切。20世纪90年代虽提出许多农业机器人，但此前常用的极坐标型机械臂等并不适合搬运重型物料。 所存在的问题：传统用于农业的机器人机械臂，如极坐标型和关节型，在搬运重型物料时存在局限性。其关节扭矩会随物料远离机器人而增大，不适用于重型物料搬运，难以满足农业生产需求。 解决方法：设计并开发一种适用于农业机器人搬运重型物料的平行型机械臂。通过对不同类型机械臂的运动学指标分析，确定平行型机械臂在搬运重型物料和适配移动平台方面具有优势。该机械臂的设计综合考虑了农业作业特点，如采用履带式移动平台以适应潮湿环境，机械臂结构设计满足物料分散、水平搬运的需求。 所用到的数据集：本文未使用公开数据集。在实验过程中，自行采集了相关数据，如在西瓜收获实验中，测量了西瓜的直径、质量，以及机械臂在操作过程中的关节位移、作业时间等数据。 所进行的实验：进行了西瓜收获实验以验证机械臂的性能。实验在京都大学实验农场开展，使用搭载平行型机械臂的机器人进行西瓜采摘。实验中利用光学测量仪器获取西瓜位置，机器人按照特定流程进行采摘。多次重复实验后，机器人采摘成功率达到86.7%，且采摘过程中未对西瓜造成损伤，但部分失败案例是由于目标西瓜过小且离机器人较远，同时发现采摘时间有待通过设计新的控制系统进一步缩短。 英文题目：Hierarchical Diffusion Policy: manipulation trajectory generation via contact guidance 中文题目：分层扩散策略：通过接触引导生成操作轨迹 研究背景：基于学习的机器人操作策略在处理3D场景感知、丰富接触和人机交互等问题时面临挑战。强化学习样本效率低、泛化性弱，模仿学习难以超越专家表现且处理多模态动作分布困难。扩散模型虽在建模复杂数据分布方面有优势，但现有基于扩散模型的机器人操作策略在接触丰富的任务中表现不佳。 所存在的问题：现有机器人操作策略难以有效处理接触丰富的任务，端到端策略交互性有限，且传统方法在处理多模态动作分布、从演示中学习超越专家行为以及应对复杂任务时存在不足。 解决方法 提出分层扩散策略（HDP）：将操作规划分为高层接触规划和低层轨迹生成，通过引入接触点指导轨迹生成，提高机器人操作的精确性和可解释性。Guider网络基于扩散过程预测接触点，Actor网络根据观察和接触点预测动作序列，Critic网络通过Q学习优化Actor网络，引导动作趋向接触点。 关键技术改进：采用一次性梯度优化，提高训练速度；引入轨迹增强，通过添加噪声构造负样本，增强Critic评估Actor的能力；提出提示引导，允许手动指定接触点，增强人机交互性。 所用到的数据集 模拟环境数据集：使用Robomimic中的2个任务数据集，包含熟练人类（PH）遥操作演示数据集和混合熟练/非熟练人类（MH）演示数据集；自行构建Tilt和Push-T任务数据集，分别用于测试机器人在复杂操作和精确推物任务中的表现。 真实世界数据集：在Move-T和布料展开任务中，通过实验采集相关数据，包括机器人的操作轨迹、物体的姿态和点云信息等。 所进行的实验 模拟实验：在多个模拟任务中对比HDP与其他基线方法，如扩散策略（Diffusion Policy）、基于序列建模的方法（LSTM-GMM、BET）和基于能量模型的方法（IBC）。结果表明HDP在所有测试任务上均显著优于基线方法，平均成功率提高了17.0%，且在表达多模态动作分布、利用3D条件信息和学习最优演示方面表现出色。 真实世界实验：在Move-T任务和布料展开任务中对HDP进行测试。Move-T任务中，HDP相比Diffusion Policy平均成功率提高了84.1%，提示引导可使成功率提高145%，且在面对干扰时表现出较强的鲁棒性；布料展开任务中，HDP成功率达到76%，比Diffusion Policy高26.7% ，证明HDP能有效处理刚性和可变形物体的操作任务。 英文题目：iFEM2.0: Dense 3-D Contact Force Field Reconstruction and Assessment for Vision-Based Tactile Sensors 中文题目：iFEM2.0：基于视觉触觉传感器的密集三维接触力场重建与评估 研究背景：在机器人执行复杂任务时，触觉感知尤其是密集三维接触力感知至关重要，它有助于机器人在非结构化任务中安全地与人交互，并提供物体属性信息。然而，实现机器人对密集三维接触力场的可靠感知颇具挑战，现有触觉传感器和相关算法存在诸多问题，如传统传感器难以精确测量切向力，基于视觉的触觉传感器在重建接触力时易受噪声影响，且缺乏统一的评估设备和标准。 所存在的问题 传感器局限：传统触觉传感器在测量切向力方面存在困难，且部分新型传感器存在空间分辨率低、制造复杂、耐久性差和成本高等问题。 算法缺陷：现有基于视觉触觉传感器的接触力重建方法，如机器学习、自然亥姆霍兹 - 霍奇分解等，存在结果噪声大、只能测量合力而非完整分布、依赖特定假设或受物体纹理影响等问题。 评估困难：缺乏直接测量三维接触力分布的设备，现有间接测量方法和模拟手段存在引入局部扰动、信号集成困难、结果不准确等问题。 解决方法 提出iFEM2.0算法：采用多层逆有限元方法，通过多层网格约束和岭正则化，有效解决模型不准确和病态问题，增强对测量噪声的处理能力，实现更精确、鲁棒的三维接触力分布重建。 确定参数组合：系统分析触觉传感器的本构模型、单元参数和材料属性等，通过仿真比较和原位机械校准，确定适合iFEM2.0的参数组合，平衡算法的准确性和效率。 建立评估基准：建立涵盖准确性、保真度和抗噪性的综合评估基准，为三维接触力重建方法提供统一的评估标准，有助于比较不同方法的性能。 所用到的数据集 模拟数据集：利用Abaqus/Standard进行仿真，获取不同接触场景下的模拟数据，包括传感器的几何形状、材料属性、接触条件和加载情况等信息，用于确定参数组合和评估算法性能。 实验数据集：使用GelSlim 3.0视觉触觉传感器在实际实验中采集数据，包括与各种标准和日常物体接触时的视觉触觉图像，以及通过ATI力/扭矩传感器测量得到的高精度三维合力数据，作为实验的真实值。 所进行的实验 仿真实验：根据建立的评估基准，对iFEM2.0算法在模拟环境下进行全面评估，包括准确性、保真度、抗噪性和泛化性。实验结果表明，iFEM2.0在重建三维接触力场时，精度高、能有效捕捉力场细节、抗噪性强，且在不同接触场景下具有良好的泛化能力。 实际验证：搭建实验平台，使用GelSlim 3.0传感器与多种物体进行接触实验，通过图像处理算法和iFEM2.0算法重建三维接触力场，并与ATI传感器测量结果对比。结果显示，iFEM2.0在实际应用中能有效捕捉和可视化动态三维接触力分布，虽存在一些失效模式，但整体精度较高，实时性能良好，适用于机器人控制等实际场景。 英文题目：RGB-D Object Recognition and Grasp Detection Using Hierarchical Cascaded Forests 中文题目：基于分层级联森林的RGB-D物体识别与抓取检测 研究背景：物体识别和抓取检测是自主机器人实现视觉感知和与现实世界交互的关键能力。但在复杂环境中，由于视觉信息分割、相似物体区分、抗噪声等因素，该任务极具挑战性。现有物体识别方法存在诸多问题，如局部描述符难以处理结构简单或纹理少的物体，全局描述符在遮挡情况下准确性低，基于词袋（BOW）的方法计算成本高且信息损失大；在机器人抓取方面，现有方法在设计通用输入特征和实现快速运行时存在困难。 所存在的问题 物体识别挑战：同一类物体外观差异大（类内方差）、不同类物体形状相似（类间相似性），以及识别时间和计算复杂度随学习的物体类别数量线性增加，影响识别准确性和效率。 机器人抓取难题：机器人抓取依赖于机械臂的姿态和物体结构属性，现有基于学习的方法在设计能泛化到未知物体的输入特征和实现快速运行时方面存在不足。 解决方法 提出统一框架：设计了一个统一的框架，通过提出新的训练目标函数，在图像层次结构的不同级别（如面片和物体级别）学习离散和连续预测器，降低物体类别和抓取姿态概率分布的不确定性，实现物体识别和抓取检测。 创新数据表示：引入“结构嵌入”（STEM）的RGB-D数据表示方法，通过多个特征图（RGB颜色、LAB颜色梯度、局部表面法线、表面法线的局部方向和点云点的投影距离）捕捉点云的外观和结构信息，利用预训练的卷积神经网络（CNNs）提取高度判别性特征。 构建级联森林架构：采用分层级联森林架构，在面片和物体级别计算物体类别标签和抓取姿态概率，并将这些概率融合为累积概率输出，用于推断目标物体的类别标签和抓取姿态。 所用到的数据集 华盛顿RGB-D物体数据集（Washington RGB-D object dataset）：包含300个物体，分为51个类别，用于评估物体识别性能，测试不同方法在类别识别和实例识别任务中的准确率。 华盛顿场景数据集（Washington Scene dataset）：由办公室、厨房和会议室环境的视频序列组成，用于评估基于深度的物体识别性能，测试模型在复杂场景中对不同物体类别的识别能力。 康奈尔抓取物体数据集（Cornell Grasping object dataset）：包含885张图像和标注的抓取矩形框，用于评估物体识别和抓取检测性能，通过不同的数据分割方式测试模型对已知物体新位置和未知物体的泛化能力。 所进行的实验 物体识别实验：在华盛顿RGB-D物体数据集上，对比了不同方法的类别识别和实例识别准确率，结果表明所提框架在两种任务上均优于现有方法；在华盛顿场景数据集上，验证了基于STEM的深度特征在深度物体识别上的优势。 抓取检测实验：在康奈尔抓取物体数据集上，使用“矩形度量”评估抓取检测结果，结果显示所提框架从单层级推理到级联分层推理，性能逐步提升，且选择性抓取模型能进一步优化抓取结果。 特征重要性实验：通过与从原始RGB-D图像中提取的局部特征对比，验证了基于STEM的CNN特征在分离物体类别和实例方面的优势，即使在训练数据较少的情况下也表现出色。 参数选择实验：分析了框架中参数ω、树的数量(N_{t})和分割分辨率对性能的影响，结果表明ω = 0.6时能在分类和回归之间取得最佳平衡，(N_{t}=50)且树深度为6时模型性能较好，增加分割分辨率对整体准确率影响不大 英文题目：Safe Multiagent Motion Planning Under Uncertainty for Drones Using Filtered Reinforcement Learning 中文题目：基于滤波强化学习的无人机在不确定性环境下的安全多智能体运动规划 研究背景：在杂乱且存在随机不确定性的工作空间中进行多智能体运动规划，是设计可靠自主系统的关键挑战，在交通、物流、监测和农业等领域对这类规划器需求迫切。基于强化学习（RL）的运动规划虽能处理通用动力系统和复杂任务，但缺乏安全保证，且多智能体RL存在非平稳性和可扩展性问题。在确定性和随机环境下，已有多种多智能体运动规划方法被提出，但都存在一定的局限性。 所存在的问题 安全保障不足：多数基于RL的方法通过软约束来保证安全，存在训练误差，无法提供可靠的安全保障。 多智能体RL的缺陷：多智能体RL存在非平稳性和可扩展性问题，训练难以收敛，且随着智能体数量增加，联合状态空间和动作空间维度快速增长，训练难度加大。 随机环境下规划困难：在随机环境中，概率约束使运动规划问题更具挑战性，需平衡运动计划的保守性与风险，同时确保满足其他问题目标的解决方案存在。现有一些单智能体运动规划方法扩展到多智能体系统时计算成本过高，且部分方法未考虑多智能体设置或不能保证递归可行性。 解决方法：提出一种结合RL和基于约束控制的轨迹规划方法，用于在随机、杂乱的工作空间中进行安全的多智能体运动规划。先通过单智能体RL从数据中学习运动计划，该计划可能存在碰撞风险；再利用基于约束控制的安全滤波器，通过凸优化、机会约束和基于集合的方法，对RL生成的运动计划进行在线评估和修正，确保智能体在存在不确定性的情况下，以高概率满足安全约束，实现安全运动规划。 所用到的数据集：未使用公开数据集。在研究过程中，自行设定实验场景参数来模拟相关数据，如在实验中使用Crazyflie 2.1 quadrotors作为目标平台，设定其2-D运动的相关动力学参数、噪声协方差、风险界限等数据，用于实验中的控制设定和性能评估，未提及数据的具体来源和性质。 所进行的实验 实验验证：使用六个四旋翼无人机在3×3米的工作空间进行实验，工作空间内设有七个圆形障碍物和两个目标区域。对比了基于RL控制器和安全滤波器的组合方法与仅使用安全滤波器的基线控制器（比例控制器）。结果显示，RL控制器与安全滤波器结合的方法能使智能体更快到达目标，而基线控制器下粉色智能体常被困在障碍物之间无法到达目标。同时，实验还通过绘制智能体间的间隙图，验证了系统的集体安全性，且求解QP的时间小于控制采样周期，满足实时性要求。 RL运动规划器评估：在100×100的网格上对学习到的策略进行确定性评估，结果表明RL智能体在多数初始条件下能学会导航到目标，但策略并不完美，存在碰撞或无法到达目标的情况。然而，RL与安全滤波器的组合确保了安全运动规划，对于不到2%的初始条件，虽RL策略在800个时间步内未到达目标，但仍保证了安全性。 仿真研究：将提出的方法与纯MPC（模型预测控制）的运动规划器进行比较。在100次蒙特卡罗模拟中，提出的方法完成运动规划任务的成功率（99%）显著高于纯MPC方法（55%）。使用终端约束通常会使智能体与障碍物及智能体之间的最小间距更大，任务完成时间更短，但也会使问题难度增加。此外，研究还发现提出的方法在保证安全的同时，任务完成时间比无终端约束的纯MPC方法更长，体现了安全与性能之间的权衡。 可扩展性研究：通过在仿真中改变智能体数量（从2到24）、减小智能体半径以及降低噪声协方差，收集安全滤波器的计算时间。结果表明，随着智能体数量增加，安全滤波器的计算时间仅适度增加，得益于其凸QP结构。与确定性设置下的前期工作相比，虽计算量有所增加，但仍在可接受范围内，证明了该方法具有一定的可扩展性。 英文题目：Safe Reinforcement Learning in Uncertain Contexts 中文题目：不确定环境下的安全强化学习 研究背景：在现实世界中部署机器学习算法时，确保安全性至关重要，安全学习算法应运而生。然而，多数现有算法仅考虑机器人内部动力学及外部约束，未充分关注外部环境变化对机器人动力学的影响。实际应用中，机器人常面临离散的外部环境变化，如操作不同重量物体或在不同表面作业等，这些变化可建模为离散的上下文变量，但现有文献通常假定上下文变量已知，与实际情况不符。 所存在的问题 上下文变量难以测量：在实际机器人学习场景中，虽然机器人可获取一些与上下文相关的测量信息，如摄像头图像，但难以从这些信息中准确推断上下文变量，如物体重量等。 现有方法假设不切实际：多数安全学习算法在处理上下文变量时，假设其已知或可精确测量，这在现实中往往不成立。而考虑未知上下文的现有方法，或假设无上下文信息，或需依赖与上下文直接相关的低维任务参数，均不适用于本文研究场景。 分类算法缺乏适用的理论保证：现有的分类算法众多，但在与安全学习算法结合时，缺乏能提供输入相关的频率主义不确定性区间的理论保证，难以满足在不确定环境下安全学习的需求。 解决方法 推导多分类的频率主义界限：基于条件平均嵌入（CMEs）推导多分类的频率主义不确定性界限，为从测量中估计当前上下文概率提供理论依据，从而使分类器输出能用于安全学习算法并提供安全保证。 提出上下文识别方法：利用最大平均差异（MMD）比较系统轨迹的概率分布，结合子采样策略和定义的上下文差异阈值，提出一种具有统计保证的上下文识别方法，在分类器不确定性过高时，用于确定当前上下文。 结合分类与上下文识别的安全学习算法：将基于CMEs的多分类方法与上下文识别方法相结合，融入安全学习算法。根据分类器输出的概率和不确定性界限，判断是否需要进行上下文识别实验，以确保在不确定环境下安全地学习最优策略。 所用到的数据集 Furuta摆实验：使用Furuta摆实验数据，通过在摆杆上添加不同重量的物体改变动力学特性，用智能手机摄像头拍摄重量图像，将图像转换为灰度图并缩放至32×32像素，用于训练和测试算法。实验中，每次实验开始时随机确定当前上下文（添加不同重量或不添加重量），并基于此进行相关操作和数据收集。 MNIST数据集：用于评估分类界限的性能，该数据集包含手写数字图像，任务是预测图像中的数字，并通过算法推断分类的确定性。使用其中前10000个训练图像训练算法，在测试集中每个数字的首次出现处进行评估。 德国交通标志识别基准（GTSRB）数据集：用于进一步验证算法在实际场景中的有效性，该数据集包含不同交通标志的图像。随机选择十个交通标志，使用前1000个包含这些标志的图像训练分类器，在测试集中这些标志的首次出现处进行评估。 所进行的实验 Furuta摆实验：以Furuta摆为实验对象，使用SAFEOPT算法学习平衡控制器，在实验过程中，通过随机添加不同重量或不添加重量到摆杆，模拟不同上下文环境。实验初期，由于缺乏先验知识，不确定性界限较高，需通过上下文识别确定当前上下文；随着实验进行，积累的数据使分类器能做出更自信的决策。最终实验结果表明，在所有实验中，Furuta摆的杆从未掉落，成功保证了安全性。 与SAFEOPT算法对比实验：在模拟环境中，对比本文算法与未改进的SAFEOPT算法。结果显示，本文算法虽因上下文识别需要额外实验而增加了训练时间，但能有效避免失败；而未考虑上下文的SAFEOPT算法虽训练时间最短，但在实验中积累了较多失败案例。 敏感性分析实验：在模拟的Furuta摆实验中，改变分类接受阈值(p_{safe})，研究其对分类结果的影响。实验发现，随着(p_{safe})增加，分类器超过阈值的情况减少，误分类数量也减少；对于难以区分的上下文，分类器常报告低确定性，需进行上下文分类。 英文题目：Surface-Based Detection and 6-DoF Pose Estimation of 3-D Objects in Cluttered Scenes 中文题目：基于表面的杂乱场景中三维物体检测与六自由度位姿估计 研究背景：在基于视觉感知的自主机器人操作中，估计目标物体的六自由度（6-DoF）位姿至关重要。然而，在杂乱的三维环境中，由于遮挡、复杂背景以及不同光照和视角导致的物体外观变化，通用的物体识别和6-DoF位姿估计仍是计算机/机器人视觉领域中未解决的难题。现有方法在处理这些问题时存在诸多不足，因此需要新的方法来解决这些挑战。 所存在的问题 基于关键点的方法：现有基于关键点或关键点对的物体检测和位姿估计方法，将整幅图像作为一个整体处理，在杂乱场景和物体部分遮挡时，易对背景或非目标区域进行不必要或错误的处理，且对目标物体区域的处理可能不足。同时，该方法存在参数难以确定、依赖大量计算来匹配关键点、难以处理噪声和同一物体的多个实例等问题。 基于分割的方法：基于RGB-D数据分割的物体检测方法，部分仅关注物体检测而未涉及6-DoF位姿估计；有的虽进行位姿估计，但存在分割不准确、对物体形状和纹理要求较高、无法直接应用于杂乱场景等问题。 基于外观的三维物体建模方法：现有的基于外观的三维物体建模工作通常需要在非常受控的环境中进行，难以获取完整的物体模型，且无法处理物体表面的透明区域。 解决方法 建立物体模型：提出一种基于RGB-D图像中分割出的光滑表面建立物体模型的策略。通过在不同视角下拍摄物体的RGB-D图像，利用环境中的关键点（如两个具有丰富视觉信息的地标）来注册不同视图，构建包含自动分割和注册的3-D光滑表面、视觉特征及相对位姿信息的物体模型。该模型对物体表面信息缺失（如透明区域）具有鲁棒性，且无需精确的物体和相机位置信息，便于自主构建。 物体检测和位姿估计：基于构建的物体模型，提出一种从单张图像中进行物体检测和6-DoF位姿估计的策略。先对测试图像中的光滑几何表面进行分割并描述其视觉特征，利用训练好的多类支持向量机（SVMs）分类器对表面片段进行识别和标注，再通过匹配ASIFT关键点对预测物体模型的位姿，必要时使用ICP算法优化位姿估计。之后，通过场景重建和基于图割的算法去除冗余和错误的物体识别结果，提高检测和位姿估计的准确性。 所用到的数据集：自行创建了包含17个测试集的数据集，共92张RGB-D图像，这些图像由Microsoft Kinect从不同杂乱场景的不同视角拍摄。每个测试图像包含6 - 14个被遮挡的物体或同一物体的不同实例。此外，在训练过程中，为每个物体拍摄5×6张RGB-D图像，为背景拍摄12张RGB-D图像用于构建物体模型和训练分类器。 所进行的实验 训练实验：对不同的k值（128、256、512、1024、2048）进行测试，选择k = 512作为ASIFT直方图的维度，并确定SVMs训练参数C = 6.73和γ = 0.04。通过K折交叉验证，分析不同视觉特征（HSV、ASIFT及其组合）下训练的SVMs分类器的最优准确率。 测试实验：将所有测试表面片段分类到物体标签的结果与物体实例的联合检测和位姿估计结果进行比较。结果表明，虽表面片段分类的精度 - 召回率曲线中召回率较高时精度较低，但物体实例 英文题目：The GR2 Gripper: An Underactuated Hand for Open-Loop In-Hand Planar Manipulation 中文题目：GR2夹爪：一种用于开环手内平面操作的欠驱动手 研究背景：在机器人操作领域，使用机器人夹爪在不借助高保真接触传感器、主动/滑动表面或先验工作空间探索的情况下，对未知物体进行灵巧操作仍是一个有待解决的问题，然而这一能力在许多机器人应用中至关重要。单自由度两指机器人夹爪虽在工业和研究中广泛应用，但在执行装配前的零件对齐等任务时存在局限性。为了克服这些局限性，增强夹爪的能力，实现对抓取物体的重新定向等手内操作，成为了研究的重点方向。 所存在的问题：传统两指机器人夹爪功能较为单一，通常仅用于抓取操作，在处理需要灵巧操作的任务时存在不足。如在装配任务中，面对零件的不对准问题，传统夹爪无法独立调整物体方向，需借助通用冗余机械臂、额外手腕机构或重新抓取等策略，但这些方法会受到空间限制、关节限制等因素制约，在非结构化环境中问题更为突出。同时，传统夹爪在进行手内操作时，需要高保真接触传感器和数值算法来协调手指运动，这增加了系统的复杂性和成本，限制了其实用性。 解决方法：提出GR2夹爪概念，对传统两指机器人夹爪的拓扑结构进行改进。将传统固定几何形状的底座改为由两个通过枢轴关节连接的三元连杆组成的可变几何形状的四元连杆，其中一个三元连杆为夹爪的接地底座，另一个可绕枢轴关节旋转，且通过拉伸弹簧与底座相连。采用简单的低电平位置 - 扭矩切换控制方案，在操作过程中，一个手指通过位置控制推动物体运动，另一个手指通过扭矩控制维持接触约束，无需精确协调手指运动即可保持对物体的稳定抓取。在抓取物体前，GR2夹爪的手指类似独立的四杆连杆；抓取物体后，输入角度固定，手 - 物体系统形成封闭机制，通过中央三角形枢轴的旋转改变四元连杆的尺寸，实现对物体的重新定向操作。 所用到的数据集：未使用公开数据集。研究过程中，自行设计并制作了一系列不同直径的圆形和方形测试物体，用于评估GR2夹爪的手内操作性能。方形物体在测试时，考虑了手指垫与物体侧面和角落接触的不同情况。这些测试物体的设计和使用，为研究GR2夹爪在不同条件下的操作表现提供了数据支持。 所进行的实验：构建GR2夹爪原型，其结构组件采用3D打印，指尖可拆卸以测试不同接触几何形状，中央三角形枢轴可锁定或预加载弹性带。每个手指由MX - 28 Dynamixel伺服电机通过肌腱驱动，通过特定方式实现扭矩控制模式。对GR2夹爪进行手内操作测试，使用Ascension trakSTAR传感器测量测试物体在操作过程中的位移和方向变化。在测试中，对不同尺寸的圆形和方形物体进行操作，对比中央三角形枢轴锁定和解锁两种情况下夹爪的操作性能。实验结果表明，解锁中央三角形枢轴时，GR2夹爪对不同尺寸物体能实现大致相同的笛卡尔工作空间，且物体的重新定向范围更大；而枢轴锁定时，传统设计的两指夹爪对较小物体的重新定向范围较小，最大可实现的方向变化也减小。此外，实验还发现枢轴解锁有助于系统保持与物体的稳定接触，减少滑动，且实际操作中滚动接触更常见，中央枢轴提供的额外自由度有助于克服单个手指的运动学限制。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Paper-Robot_list","slug":"Paper-Robot-list","permalink":"http://example.com/tags/Paper-Robot-list/"},{"name":"Paper-TRO","slug":"Paper-TRO","permalink":"http://example.com/tags/Paper-TRO/"}]},{"title":"PaperReading-Robot_list-CVPR","slug":"paper_Robot_cvpr_list","date":"2025-04-21T08:20:48.021Z","updated":"2025-04-21T08:46:36.700Z","comments":true,"path":"2025/04/21/paper_Robot_cvpr_list/","link":"","permalink":"http://example.com/2025/04/21/paper_Robot_cvpr_list/","excerpt":"","text":"英文题目：DexGraspAnything: Towards Universal Robotic Dexterous Grasping with Physics Awareness 中文题目：DexGraspAnything：迈向具有物理感知的通用机器人灵巧抓取 研究背景： 灵巧抓取对机器人至关重要，是机器人实现复杂操作任务的基础能力。五手指灵巧手相比简单抓手，在灵活性、操作精度和通用性上优势显著，能更好地适应人类环境，与各种物体和工具交互，是具身智能交互的核心。早期灵巧抓取研究主要采用分析方法，依赖手动推导和物理约束优化抓取姿势，但因搜索空间大、优化复杂，成功率较低。随着深度学习发展，数据驱动方法成为主流，其中基于回归的方法生成的抓取姿势多样性有限，生成式方法尤其是扩散模型虽能生成多样姿势，但缺乏物理约束，导致生成的抓取姿势往往不是最优，存在手与物体穿透或接触不足的问题。 所存在的问题： 传统分析方法：搜索空间大且优化复杂，导致灵巧抓取的成功率低。 数据驱动的回归方法：直接从输入数据预测抓取参数，易出现模式崩溃和平均化问题，生成的抓取姿势多样性受限。 基于扩散模型的生成方法：在训练和采样过程中缺乏物理约束，生成的抓取姿势常出现手与物体穿透或接触不足的情况，成功率不理想。 现有数据集：存在数据分布狭窄、物体类别有限、可扩展性差等问题，难以满足基于扩散模型的生成方法对大规模、高质量训练数据的需求。 解决方法： 提出DexGrasp Anything方法，将精心设计的物理约束集成到扩散模型的训练和采样阶段。具体包括： 物理约束设计：引入表面拉力，确保抓取可行性，使手指内表面靠近物体表面；引入外部穿透排斥力，减少手与物体的不必要相交，保持交互的空间准确性；引入自穿透排斥力，维持手的结构几何形状，避免手指间碰撞。 物理感知训练：定义物理感知训练目标，将标准均方误差目标与多个物理约束目标线性组合，使扩散模型在训练过程中学习物理先验。通过估计的干净样本将梯度传播到含噪数据，引导模型生成更符合物理规则的抓取姿势。 物理引导采样：利用训练好的扩散生成器，在采样过程中进一步增强物理约束。采用分类器引导技术，将物理约束纳入采样过程，通过调整后验均值来引导扩散模型生成更合理的抓取姿势。同时，应用球形高斯约束来减轻估计偏差，逐步调整抓取配置，使其更符合物理可行性。 LLM增强的表示提取：利用强大的大语言模型（LLM）增强传统的物体表示。通过提示LLM获取物体的语义先验信息，与几何物体特征结合，经点Transformer编码和跨注意力机制集成到扩散模型中，提升模型生成精确抓取姿势的能力。 所用到的数据集： 构建了DexGrasp Anything（DGA）数据集，这是目前最大且最多样化的灵巧抓取数据集： 数据构建：收集多个来源的现有数据，包括模拟数据、真实捕获数据和人类手部抓取数据，如GRAB、DexGraspNet等。利用机器人遥操作系统将人类手部数据集转换为灵巧手参数，并进行严格过滤，确保数据质量。使用训练好的模型以“模型在环”的策略生成更多数据，最终形成大规模多样的数据集。 数据统计：该数据集包含两个主要部分，DGA - curated约有88万个抓取姿势，涉及5664个不同物体；DGA - generated约有252万个抓取姿势，涵盖10034个不同物体，总共包含超过340万个抓取姿势，涉及15698个物体。 数据特点：数据规模大，远超之前的数据集；物体种类多样，涵盖广泛的类别和来源，其物体特征在特征空间中分布更广；抓取姿势多样，有助于提升现有方法生成结果的多样性，同时保持或提高抓取成功率。 英文题目：Spatial-Temporal Graph Diffusion Policy with Kinematic Modeling for Bimanual Robotic Manipulation 中文题目：用于双机器人操作的结合运动学建模的时空图扩散策略 研究背景： 双机器人操作是机器人执行复杂任务的基本能力，对机器人系统至关重要。模仿学习在单臂操作中取得成功，但在双机器人操作场景中面临挑战。双机器人操作需要协调双臂运动并符合物理约束，这使得现有方法在实际应用中可靠性和可行性受限。主流方法采用预测下一个最佳末端执行器姿势（NBP），再通过逆运动学计算关节旋转角度的两阶段流水线，但这种方法存在诸多问题。 所存在的问题： 现有方法在双机器人操作任务中存在两个主要问题。一是很少考虑机器人的物理结构，可能导致双臂自碰撞或相互干扰；二是忽略运动学约束，预测的姿势可能不符合机器人关节的实际限制，使得生成的运动不可靠，在实际执行中容易失败。 解决方法： 提出Kinematics enhanced Spatial-TemporAl gRaph Diffuser（KStar Diffuser）框架，将机器人结构和运动学融入双机器人运动生成过程。构建动态时空图，根据机器人的URDF规范，节点表示关节属性，边捕获空间关系和时间依赖，通过图卷积网络（GCN）编码为扩散过程提供物理约束；引入可微运动学模块，通过可微正向运动学将预测的关节位置映射到参考末端执行器姿势，作为扩散过程的条件，确保生成的运动满足结构和运动学约束。 所用到的数据集： 使用RLBench2基准数据集进行实验，该数据集是为双机器人操作定制的扩展版本，包含许多与现实场景相似的任务，用于评估KStar Diffuser在双机器人操作任务中的能力。同时，构建了两个基于模拟基准的真实世界任务数据集，即lift plate和handover item easy，用于进一步评估模型在真实环境中的有效性。 英文题目：FMB: a Functional Manipulation Benchmark for Generalizable Robotic Learning 中文题目：FMB：用于可泛化机器人学习的功能性操作基准 研究背景： 机器人操作是机器人研究的基础问题，但让机器人实现类似人类的灵巧操作仍极具挑战，主要困难在于处理复杂的接触动力学以及环境和物体的可变性。虽然机器人学习技术有潜力解决这些问题，但目前的研究存在局限性，要么侧重于简单技能的广泛泛化，未充分考虑操作中的物理挑战；要么专注于执行狭窄任务的复杂技能，缺乏广泛的泛化能力。 所存在的问题： 现有研究难以同时在广泛泛化和处理灵巧操作的物理复杂性方面取得进展。在机器人操作任务中，缺乏一个综合且易于使用的框架，该框架应包含具有实际相关性的挑战性任务、适量的高质量数据、易于重现的实验设置、相关方法的基线结果以及对实验结果的深入分析。 解决方法： 提出功能性操作基准（FMB），通过精心设计任务、提供全面的数据集以及可重现的硬件和软件系统，来研究机器人操作学习中的关键挑战。FMB中的任务分为单物体多阶段操作任务和多物体多阶段操作任务，涵盖抓取、重新定位和装配等基本操作技能。同时，提供了一套模仿学习框架，包括训练好的策略，方便研究人员对不同阶段的操作技能进行研究。 所用到的数据集： FMB构建了包含22,550个演示轨迹的数据集，涵盖单物体和多物体操作任务。其中，单物体操作任务数据集包含从抓取到插入的完整演示2700个，以及单独插入阶段的演示4050个；多物体操作任务数据集包含3个多物体装配任务的150个端到端演示。数据集中的每个演示轨迹都包含多个传感器模态的数据，如多个摄像头的RGB和深度图像、机器人运动学信息以及末端执行器的力/扭矩测量数据。 英文题目：BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects 中文题目：BundleSDF：未知物体的神经6自由度跟踪与3D重建 研究背景： 6自由度（6-DoF）姿态跟踪和从单目RGBD视频进行未知物体的3D重建是计算机视觉中的两个关键且紧密相关的问题，在增强现实、机器人操作等领域有广泛应用前景。但此前的研究通常将这两个问题分开处理，且存在诸多局限性。例如，神经场景表示方法在创建高质量3D物体模型时，往往依赖已知的相机姿态和/或真实物体掩模，且难以对动态相机下的静态物体进行完整3D重建；而6-DoF物体姿态估计和跟踪方法，通常需要测试物体的纹理3D模型进行预训练或在线模板匹配，类别级方法在处理分布外物体实例和未见物体类别时也存在困难。 所存在的问题： 现有方法在处理未知物体的6-DoF姿态跟踪和3D重建时，无法同时满足无需物体先验知识、能处理复杂场景（如遮挡、无纹理、镜面高光等）以及实时性好等要求。传统方法在面对纹理或几何线索不足、存在遮挡等情况时，容易出现跟踪漂移、重建不准确等问题，难以实现可靠的实时跟踪和高质量的3D重建。 解决方法： 提出一种联合解决6-DoF姿态跟踪和3D重建的方法。该方法通过在线姿态图优化、并行训练的神经物体场以及动态记忆池协同工作来实现。首先利用基于Transformer的特征匹配网络和RANSAC算法进行粗姿态初始化；接着将关键帧存储在记忆池中，通过与记忆池中的帧对比更新当前帧姿态；然后进行在线姿态图优化，选择部分记忆帧参与优化以得到更准确的姿态估计；最后，神经物体场利用记忆池中的帧学习物体的3D形状和外观，同时调整记忆帧的姿态。此外，采用混合SDF表示处理不确定自由空间问题，提高模型的鲁棒性。 所用到的数据集： 使用了三个真实世界数据集进行实验评估。HO3D数据集包含人类手部与YCB物体交互的RGBD视频；YCBInEOAT数据集包含双臂机器人操作YCB物体的以自我为中心的RGBD视频；BEHAVE数据集包含人体与物体交互的RGBD视频，评估时限制在单视图设置下。这些数据集涵盖了不同形式的交互和动态场景，用于全面测试方法在各种情况下的性能。 英文题目：On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks 中文题目：精确几何数据在密集3D视觉任务中的重要性研究 研究背景： 3D视觉在自动驾驶、机器人视觉和增强现实等领域至关重要，距离测量是其关键。当前存在多种传感器模态和深度预测方法，且有丰富的公开数据集用于评估深度估计流程。不同传感器依据不同测距原理各有利弊，但在训练模型时，人们常未充分考量其特性，直接将其测量数据当作真值，忽略了数据不准确或存在误差的问题。 所存在的问题： 不同深度传感器在面对诸如无纹理区域、反射性材料和半透明物体等场景时，会产生测量误差。而基于不准确数据训练的模型会出现偏差，泛化能力也会受到影响。若在评估时将存在误差的传感器测量值视为真值，就无法发现这些问题，进而可能导致对模型性能的错误解读。 解决方法： 构建一个独特的多模态传感器数据集，涵盖D-ToF、I-ToF、被动/主动立体视觉和单目RGB + P等多种传感器数据，并使用高精度3D扫描仪和对齐渲染获取同步捕获的高精度真值数据。通过该数据集，深入分析不同深度传感器模态对模型训练的影响，量化传感器噪声的作用，为改进密集视觉估计和针对性的数据融合提供依据。 所用到的数据集： 自主构建的室内多模态数据集，包含7个室内区域、6张桌子、4把椅子以及64个来自9个类别的 household objects。数据集中共有13个场景，分为10个训练场景和3个测试场景。该数据集与以往室内多模态深度数据集不同，它能同时提供高精度的真值（深度、表面法线、6D物体姿态、实例掩码、相机姿态、密集场景网格）以及多样的传感器数据，为研究提供了更全面准确的数据支持。 英文题目：Dexterous Grasp Transformer 中文题目：灵巧抓取变换器 研究背景： 机器人灵巧抓取在机器人学和计算机视觉领域极为关键，在工业生产和日常生活场景应用广泛。深度学习与大规模数据集推动了基于学习的抓取方法发展，使其在抓取质量和泛化性上取得进展。但现有方法在生成多样且高质量的抓取姿势方面存在不足，生成式模型生成的抓取姿势多样性欠佳，而传统判别式模型每次只能预测一个抓取姿势，且为获取多样抓取需多次旋转输入点云并推理，耗时且影响质量。 所存在的问题： 现有学习方法在生成可行且多样的抓取姿势时面临挑战，条件生成模型受条件限制，推理时生成的抓取姿势相似；传统判别式模型预测能力有限。同时，将灵巧抓取生成视为集合预测任务时，存在优化难题，如模型易出现崩溃或预测的抓取姿势穿透物体的情况，影响抓取效果与模型性能。 解决方法： 提出Dexterous Grasp Transformer（DGTR）这一判别式框架，将灵巧抓取生成定义为集合预测任务，利用Transformer架构和可学习的抓取查询，一次前向传递就能预测多样的可行抓取姿势。针对优化难题，提出动态 - 静态匹配训练（DSMT）策略和对抗平衡测试时自适应（AB - TTA）策略。DSMT策略通过动态训练引导模型学习合适目标，再经静态训练优化物体穿透问题，提升优化稳定性；AB - TTA策略利用一对对抗损失在灵巧手参数空间中优化预测的抓取姿势，增强抓取质量并减少穿透。 所用到的数据集： 使用DexGraspNet数据集进行实验评估，该数据集包含133万多个ShadowHand对5355个物体的抓取数据，涵盖133多个物体类别。利用此数据集对DGTR框架的生成质量和多样性进行综合评估，通过多种评估指标验证了DGTR在生成高质量、多样抓取姿势方面的有效性。 英文题目：Forecasting of 3D Whole-body Human Poses with Grasping Objects 中文题目：基于物体抓取的3D全身人体姿势预测 研究背景： 在计算机视觉和人机交互领域，预测3D人体姿势对理解人类行为、提升智能系统预测能力意义重大。过往研究在该领域虽有进展，但多聚焦于预测人体主要关节，忽视了手部的精细动作以及与物体的交互，而手部动作在人机交互中起着关键作用，能更精准地表达人体姿势，反映行为意图。 所存在的问题： 现有方法主要关注人体主要关节的预测，忽略了手部精细动作和物体交互。同时，在处理人体内部的异质性（如不同身体部位运动模式差异）和外部交互性（如人体与物体交互的动态变化）方面存在不足，难以实现精确的全身动作预测。 解决方法： 提出C³HOST（cross-context cross-modal consolidation for 3D whole-body pose forecasting）方法。先将人体划分为身体、左手和右手三个部分，分别提取时空信息，利用最大平均差异（MMD）进行特征对齐，减少特征异质性；接着通过循环交叉注意力机制捕捉身体各部分间的内部交互；然后计算人体与物体间的距离信息，利用门控共享单元调整其影响，并通过分布归一化在时间域对齐物体和人体关节特征；最后使用图注意力网络聚合多模态信息，学习人体与物体的交互，从而预测未来的全身动作序列。 所用到的数据集： 使用了GRAB和BEHAVE两个数据集。GRAB数据集包含约160万帧数据，由10名演员执行29种动作，通过高精度动作捕捉技术标注全身SMPL-X参数，定义了25个身体关节和每个手部15个关节，还包含50个物体的点云数据。BEHAVE数据集包含386个样本，由4个Kinect RGB-D相机以30fps的帧率捕获，涉及8个对象、20个物体和17种人机交互类型，每个姿势由67个关节的骨架表示 。 英文题目：Generate Subgoal Images before Act: Unlocking the Chain-of-Thought Reasoning in Diffusion Model for Robot Manipulation with Multimodal Prompts 中文题目：行动前生成子目标图像：在多模态提示的机器人操作扩散模型中开启思维链推理 研究背景： 具身操作致力于打造能在复杂环境中感知、推理和行动的通用机器人，视觉语言模型（VLM）的发展为机器人闭环控制带来潜力，但机器人在执行长时操作任务、理解复杂多模态提示时仍困难重重。复杂操作场景仅靠文本提示难以充分准确描述，而直接利用VLM预测动作序列易因缺乏中间子目标指导产生偏差，借助大语言模型（LLMs）分解的文本子提示又过于复杂冗余，增加理解难度。 所存在的问题： 一方面，机器人在处理长时操作任务时，由于缺乏中间子目标的指导，执行过程中的小错误容易累积，导致与原始任务指令产生较大偏差。另一方面，复杂操作场景的多模态提示难以用文本精确表述，现有方法在理解和遵循这些提示时存在不足，影响任务的完成效果 。 解决方法： 提出CoTDiffusion分层框架，将扩散模型作为高层视觉规划器。设计语义对齐模块，通过三重对齐架构和掩码补丁预测，实现粗到细的训练，捕捉视觉子目标与提示间的语义关联，跟踪生成图像进度，开启扩散模型的思维链推理能力。采用双向生成和帧连接机制，增强生成子目标图像的保真度和指令跟随准确性。利用生成的子目标图像指导底层基础模型进行动作规划，降低对基础模型能力的要求，使其专注于基本单物体操作原语 英文题目：MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception 中文题目：MCD：用于机器人感知的大规模多校园多样本数据集 研究背景： 环境感知和自我运动估计在机器人应用中至关重要，公共数据集推动了相关研究，但现有标注的多模态大规模数据集存在局限性。这些数据集大多偏向自动驾驶场景，依赖昂贵的传统传感器，且经过精心处理，与现实场景存在差距。同时，新的感知技术如低成本的非重复周转（NRE）激光雷达和超宽带（UWB）技术虽带来机遇，但也面临扫描稀疏、视野有限和非视距观测等挑战，急需相应的数据集来支持研究。 所存在的问题： 现有标注的多模态大规模数据集在环境感知和自我运动估计研究中存在不足，如KITTI数据集缺乏IMU数据和环境多样性，Newer College Dataset存在地面真值误差和标注缺失等问题。此外，大多数数据集难以提供高精度的位置地面真值，且在涵盖多种感知模态、适应不同环境以及处理真实场景中的复杂因素方面存在欠缺。同时，针对新的感知技术如NRE激光雷达，相关的语义分割研究还比较匮乏。 解决方法： 提出MCD数据集，该数据集具有多方面优势。它包含多种传感模态，如经典旋转激光雷达、MEMS NRE激光雷达、传统相机、IMU和UWB传感器等；对59k的NRE激光雷达扫描进行了逐点语义标注，涵盖29个类别，为NRE激光雷达的语义分割研究提供了数据支持；数据采集来自欧亚三个大学校园，覆盖更广泛的纬度范围，增加了环境多样性；通过基于优化的激光雷达 - 惯性数据配准，生成连续时间的地面真值，提高了准确性；将现实场景中的各种挑战因素纳入数据，如运动失真、极端光照、玻璃反射和太阳干扰等，并在标注和图像中体现这些噪声类别，以训练机器人系统应对这些特殊情况。同时，提供了数据集、数据加载脚本和基准测试指令，方便研究人员对语义分割和SLAM算法进行研究和分析。 所用到的数据集： MCD数据集包含18个序列，涵盖三个大学校园（每个校园6个序列，其中3个白天采集，3个夜晚采集），包含超过200k的激光雷达扫描数据、1500k的相机帧数据，以及高频的IMU和UWB数据。在语义标注方面，对11个序列的Livox激光雷达点云进行了标注，共划分29个语义类。此外，还提供了基于调查级先验地图配准生成的连续时间地面真值数据，用于支持机器人感知相关的研究和算法评估。 英文题目：CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation 中文题目：CyberDemo：增强模拟人类演示以实现现实世界中的灵巧操作 研究背景： 模仿学习在机器人操作领域颇具前景，它能让机器人通过人类演示获取复杂技能，但该方法高度依赖高质量的演示数据，而收集这些数据往往需要耗费大量人力，在多手指灵巧手操作任务中，数据收集的复杂性和精确性要求更高。传统观念认为，直接从真实机器人收集特定任务的演示数据是解决问题的最佳方式，但这种“黄金标准”面临挑战。利用模拟环境收集人类演示数据具有诸多优势，如无需真实硬件、可远程并行执行，还能通过数据增强提升任务表现。然而，将模拟环境中训练的策略转移到现实世界存在“模拟到现实”（sim2real）的挑战。 所存在的问题： 收集大规模高质量的演示数据困难重重，特别是针对高自由度的灵巧手操作任务。虽然数据增强可提高策略的泛化能力，但以往的方法多在图像层面进行，未充分考虑物理现实。此外，“模拟到现实”的转移问题一直是机器人学习领域的关键难题，传统的领域随机化和领域适应方法在解决高自由度灵巧手操作任务的动力学差距方面效果不佳。 解决方法： 提出CyberDemo框架，首先在模拟环境中通过低成本设备进行远程操作，收集人类演示数据；接着利用模拟器的优势，对原始演示数据进行多维度的数据增强，包括随机化相机视图、光照和纹理、添加多样物体以及随机化物体姿态等，同时考虑了视觉和动力学的变化，以提升策略对不同条件的鲁棒性；然后，使用自动课程学习和动作聚合的方法训练操作策略，根据任务成功率逐步增加数据增强的复杂度，并将小运动的步骤聚合以减少噪声影响；最后，利用少量真实演示数据对训练好的策略进行微调，以适应现实世界的操作。 所用到的数据集： 在模拟环境中，基于SAPIEN模拟器构建真实世界任务环境，使用相同的远程操作系统收集人类演示数据。在现实环境中，利用低成本远程操作系统收集数据，每个任务仅收集三分钟的机器人轨迹数据。为评估模型性能，设计了三个操作任务，包括两个准静态任务（抓取放置、倾倒）和一个非准静态任务（旋转），并针对每个任务设计了不同的数据增强和课程学习级别 。 英文题目：Smart Help: Strategic Opponent Modeling for Proactive and Adaptive Robot Assistance in Households 中文题目：智能家居帮助：面向家庭环境中主动适应性机器人辅助的战略对手建模 研究背景： 随着社会的发展，对于弱势群体（如老人、儿童和残疾人）在日常任务中所需的辅助技术需求显著增加。然而，当前针对这些群体的高级AI驱动辅助解决方案的研究仍然较少，传统的人机交互往往忽略了人类的能力和情感需求，比如他们的实践机会、自我提升感和自尊心等。 所存在的问题： 以往的人机交互研究主要集中在简单的合作上，而现有的辅助技术通常只是接管一切，忽视了用户的情感福祉。提供帮助时缺乏对接受者情感接受度的敏感性和考虑，这导致了一个新的挑战：如何不仅关注任务的成功完成，还要注重接收者对帮助的情感接受度。 解决方法： 提出了一个名为“Smart Help”的新挑战，旨在为具有多样化的残疾和动态目标的人类代理在不同任务和环境中提供既主动又适应的支持。通过利用AI2-THOR建立了一个新型互动3D真实家居环境，并引入了一个创新的对手建模模块来优化辅助机器人的帮助策略。该模型不仅能推断用户的目标分布，还能对其每个目标独立完成的能力进行推理，从而选择性地解决那些阻碍任务完成的关键瓶颈。 所用到的数据集： 本研究基于AI2-THOR构建了一个多智能体互动环境，这是一个专为真实的家庭模拟设计的三维互动环境。此外，为了更好地评估Smart Help任务中的性能，还提出了一系列评估指标，并建立了包括联合目标与能力推断、瓶颈推理以及帮助策略改进在内的基准模型。实验验证了模型组件的有效性，并展示了该整体方法相对于基线的优越性。所有环境、数据集和代码都可通过https://github.com/caozh20/SmartHelp获取。 英文题目：Rapid Motor Adaptation for Robotic Manipulator Arms 中文题目：机器人机械臂的快速运动适应 研究背景： 随着计算机视觉和高级规划的发展，灵巧操作物体（即低级控制技能）仍是创建可帮助完成一般操作任务机器人的主要障碍之一。经典的机器人操作方法依赖精确模型，创建模型的复杂性较高；许多强化学习方法样本效率低且泛化能力差。为解决这些问题，Kumar等人提出快速运动适应（RMA），在四足机器人locomotion方面取得成效。 所存在的问题： 将RMA应用于一般操作任务并不简单，因为操作任务目标和行为因物体特征而异，仅靠本体感觉不足以完成任务，还需要抓取前的视觉推理。此外，现有方法在处理物体变化、样本效率和泛化能力等方面存在不足。 解决方法： 提出类别和实例字典作为几何感知操作的强代理，用于学习不可跨物体转移的策略；使用深度卷积神经网络估计环境的部分特权信息，隐式地进行物体类别和实例分类；首次将快速运动适应应用于机器人手臂的一般物体操作任务；统一快速运动适应两个学习阶段的目标形式化。通过在ManiSkill2环境中训练策略，利用近端策略优化和标准反向传播分别优化策略训练阶段和适配器训练阶段的目标函数。 所用到的数据集： YCB数据集（78个物体）、EGAD数据集（2281个物体）、PartNet-Mobility数据集（60个物体用于水龙头转动任务） 。 英文题目：DexArt: Benchmarking Generalizable Dexterous Manipulation with Articulated Objects 中文题目：DexArt：使用关节物体进行可泛化灵巧操作的基准测试 研究背景： 为实现家用机器人像人类一样操作日常关节物体，灵巧操作学习至关重要。当前机器人操作多依赖平行夹爪，限制了可操作物体范围；虽强化学习在灵巧操作有进展，但多聚焦单刚体操作。同时，现有机器人操作基准测试存在不足，如MetaWorld未考虑跨物体实例的泛化，ManiSkill受平行夹爪限制任务和操作方式。 所存在的问题： 学习灵巧操作因机器人手的高自由度关节而极具挑战，操作多样关节物体增加了关节自由度的复杂性，且在测试时对未见物体的泛化存在困难，这是强化学习的主要瓶颈。此外，现有基准测试无法很好地支持研究可泛化的灵巧操作技能以及视觉感知对决策的影响。 解决方法： 提出DexArt基准测试，定义多个复杂操作任务，使用Allegro Hand在模拟环境中操作关节物体。采用强化学习结合可泛化的视觉表征学习，以3D点云作为观察数据，利用PointNet编码器提取视觉表征辅助决策。通过实验研究不同方法和设置，如改变训练物体数量、视觉骨干网络大小、预训练方法等，探究其对策略学习性能的影响。 所用到的数据集： DexArt Manipulation Dataset (DAM)：渲染的点云观测数据，包含每个物体6k个点云（包括观测和想象点云），机器人和关节物体状态随机采样，用于分割预训练时标注为4个类别。 PartNet-Mobility Manipulation Dataset (PMM)：直接从PartNet-Mobility渲染，无任务信息，含46个物体类别，每个类别1k个点云，物体状态和相机视点随机采样，用于分类和分割任务。 英文题目：Target-referenced Reactive Grasping for Dynamic Objects 中文题目：针对动态物体的目标参考反应式抓取 研究背景： 反应式抓取在工业领域需求大，如在人机协作场景中，机器人若能实现反应式抓取可减轻工人压力。然而，与静态环境下的抓取不同，动态任务设置对算法设计提出新挑战。此前研究多聚焦于规划时间上平滑的抓取动作，却很少关注语义一致性，即确保机器人在后续帧中抓取物体的同一部分，并且在杂乱场景中，经典方法预测的抓取位置也不一定在同一物体上。 所存在的问题： 现有方法在反应式抓取时，难以同时保证抓取动作在时间上的平滑性和语义上的一致性。6D姿态跟踪用于解决该问题时存在缺陷，一方面，其依赖实例分割，推理速度慢，无法满足实时性要求；另一方面，常需要物体的先验知识（如CAD模型），在现实中可能无法获取，且泛化能力有限。 解决方法： 提出在目标参考设置下通过跟踪生成的抓取空间来解决反应式抓取问题，具体包含两个阶段。首先，利用注意力图神经网络发现抓取姿态对应关系，并选择与目标姿态相似度最高的抓取姿态；其次，基于目标和历史信息对选定的抓取姿态进行优化。利用现成的抓取检测器生成抓取候选集，通过特定的特征表示和注意力图神经网络进行抓取特征聚合与对应估计，同时使用记忆增强的优化网络结合历史信息优化抓取姿态。 所用到的数据集： GraspNet-1Billion数据集：大规模真实世界抓取检测数据集，包含89个物体、190个场景以及每个场景256个相机视图，用于训练模型。 Moving GraspNet数据集：自行收集的包含30个动态物体场景的测试集，记录了每帧的RGBD图像和物体6D姿态，并手动标注了第一帧的10个抓取目标，后续帧的标注通过6D姿态投影得到。 英文题目：ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation 中文题目：ARCTIC：用于灵巧双手操作物体的数据集 研究背景： 人类在日常生活中频繁操作复杂物体，物体的运动或变形往往源于人类施加的外力。然而，在手部姿态估计领域，对操作过程中手和物体物理一致性动力学的研究较少。现有的手 - 物体数据集大多局限于刚性物体的抓取，很少包含对关节物体丰富且灵巧的操作示例，缺乏用于研究手和关节物体物理一致且同步运动的具有真实3D注释的数据集。 所存在的问题： 现有数据集无法满足研究手与关节物体物理一致且同步运动的需求，其局限性体现在以下方面：多聚焦于刚性物体抓取，缺乏关节物体操作数据；手部姿态变化范围有限，多为静态抓取；数据采集受遮挡影响大，难以获取高质量3D注释；难以研究双手灵巧操作和动态手 - 物体接触等。此外，现有从RGB图像估计3D手和物体的方法，以及手 - 物体接触检测方法，在处理关节物体复杂交互时存在不足。 解决方法： 构建ARCTIC数据集，通过精确的Vicon动作捕捉系统和多视图RGB系统同步采集数据，获取10名受试者与11个关节物体交互的210万帧RGB图像，并为每帧图像匹配准确的3D手和物体网格以及详细的动态接触信息。针对数据集提出两个新任务：一致运动重建，利用ArcticNet模型，通过编码器 - 解码器架构估计双手和关节物体的参数，实现从单目视频中重建3D运动，使其在时空上保持一致；交互场估计，引入InterField模型，通过CNN和PointNet估计每帧图像中手部顶点与物体间的最短距离，以此研究手 - 物体的相对空间关系。 所用到的数据集： ARCTIC数据集包含10名受试者对11个关节物体进行灵巧操作的339个序列，210万张来自8个静态视图和1个第一人称视角的RGB图像，并配有3D手和物体网格。数据采集使用Vicon动作捕捉系统和多视图RGB系统，通过获取模板几何形状、估计旋转轴、捕捉交互、求解姿态和计算接触等步骤完成。该数据集可用于研究双手与关节物体的交互，为相关任务提供了丰富的数据支持。 英文题目：FLEX: Full-Body Grasping Without Full-Body Grasps 中文题目：FLEX：无需全身抓取数据的全身抓取姿态生成方法 研究背景： 生成与场景真实交互的3D虚拟人在AR/VR、视频游戏和机器人等领域具有重要意义。当前研究依赖大规模3D数据集来生成虚拟人，但数据收集存在诸多困难，如使用光学标记的动作捕捉系统收集数据繁琐，涉及物体和场景时更复杂，且难以涵盖所有与物理世界交互的可能方式，导致基于特定任务数据集训练的模型泛化能力差、生成的全身姿态多样性有限。 所存在的问题： 现有生成虚拟人全身抓取姿态的方法存在明显不足。一方面，依赖数据收集的方法难以扩展到更广泛场景，无法适应物体位置、方向变化以及场景中存在家具等复杂情况；另一方面，这些方法生成的全身姿态多样性严重受限，难以满足现实场景中多样化的需求。 解决方法： 提出FLEX框架，该框架利用现有的手部抓取模型和人体姿态先验，通过几何和解剖约束将两者结合，在不使用任何3D全身抓取数据的情况下生成全身抓取姿态。具体通过在手部抓取模型和人体姿态先验模型的潜在空间中搜索合适的潜在变量，同时引入障碍物避免损失、注视损失等，并结合姿态 - 地面先验来优化生成的人体姿态，使其满足自然、合理且不与障碍物相交的要求。此外，还设计了一个映射网络，将多个相互依赖的参数控制问题转化为对单个可控潜在向量的优化。 所用到的数据集： ReplicaGrasp数据集：为了使全身物体抓取任务更具挑战性和现实代表性而构建。该数据集包含来自GRAB的50个日常物体，分布在ReplicaCAD的48个容器中，通过Habitat模拟器模拟物体处于各种可行位置，共产生4800个实例。这些实例中，物体所在的家具被视为障碍物，用于评估生成的全身人类抓取姿态是否具有“场景感知”能力，即不与障碍物相交，同时还要自然可行。 GRAB数据集：是一个通过动作捕捉收集的人体与日常物体交互的数据集，但该数据集中的物体交互场景较为单一，缺乏多样性。FLEX在该数据集上进行实验，以对比评估其在不使用全身抓取数据的情况下，与其他基于该数据集训练的方法的性能差异。 英文题目：Robot Structure Prior Guided Temporal Attention for Camera-to-Robot Pose Estimation from Image Sequence 中文题目：基于机器人结构先验引导的时间注意力机制用于从图像序列估计相机到机器人的位姿 研究背景： 在机器人执行自主任务过程中，相机到机器人的位姿估计是关键环节，其结果直接决定机器人在环境中的精准定位与操作能力。传统方法，如采用AR标签进行手眼校准，存在诸多弊端，包括关节配置与图像捕获过程繁琐、无法实现线上实时校准等，在需要频繁调整相机位置的下游任务中，这些问题尤为突出。基于视觉的位姿估计方法为在线手眼校准带来了新的可能，但现有的大多数基于学习的单帧估计方法，因受单视图图像模糊性和机器人自遮挡现象的影响，导致位姿估计性能不佳。 所存在的问题： 经典的相机到机器人位姿估计方法存在显著局限性，难以满足实际应用需求。基于单帧图像的视觉估计方法，在机器人发生自遮挡时，由于单视图图像信息不完整、存在模糊性，无法准确获取位姿信息。此外，如何有效融合时间序列信息，以提升位姿估计的准确性和鲁棒性，是当前研究面临的重要挑战。现有方法在处理时间信息时，往往不能充分利用图像序列中的时间关联，导致位姿估计结果不稳定，难以适应动态变化的环境。 解决方法： 提出SGTAPose方法，该方法通过引入机器人结构先验和时间注意力机制，实现从图像序列中准确估计相机到机器人的位姿。具体而言： 结构先验引导特征对齐：根据前一帧估计的位姿生成重投影置信图，以此引导网络聚焦于当前帧中关键点的残差信息，实现帧间特征的精准对齐，有效利用机器人结构先验知识增强特征的一致性和准确性。 时间交叉注意力增强融合：针对不同分辨率的特征图，采用差异化的融合策略，充分挖掘时间序列中各帧之间的关联信息，通过时间交叉注意力机制实现特征的高效融合，提升对时间信息的利用效率。 位姿精炼优化：设计位姿精炼器，通过重加权PnP问题求解，降低异常关键点对相机到机器人位姿计算的干扰，对初始位姿进行优化，进一步提高位姿估计的精度和可靠性。 此外，构建大规模合成数据集，并运用域随机化技术，有效缩小模拟数据与真实数据之间的差距，为模型训练提供丰富且具有代表性的数据，提升模型的泛化能力和适应性。 所用到的数据集： Panda Syn Training：借助Blender工具生成的大规模合成训练集，包含约60k个视频，每个视频包含3个连续帧，总计180k张图像。该数据集通过多种域随机化手段，模拟真实场景中的各种变化，包含时间序列的RGB图像、2D/3D预定义关键点位置以及部分位姿信息，主要用于模型的训练过程，帮助模型学习不同场景下的位姿估计模式。 Panda Syn Testing：同样为作者生成的合成测试集，包含347个视频，每个视频有30个连续帧，共10k张图像，用于评估模型在合成数据上的性能表现，检验模型对合成场景的适应能力和位姿估计准确性。 Panda 3CAM-RS：由外部安装的Intel RealSense D415相机拍摄Franka Emika Panda机械臂得到的真实世界测试集，包含约6k张图像，用于测试模型在真实场景下的位姿估计效果，验证模型从模拟到真实场景的迁移能力。 Panda 3CAM-AK：属于真实世界测试集，由Microsoft Azure Kinect相机采集，包含约6k张图像，用于评估模型在不同相机采集数据下的性能差异，考察模型对不同传感器数据的适应性。 Panda Orb：由Intel RealSense D415相机从27个不同视图拍摄所得，包含约32k张图像，是一个多样化的真实数据集，可用于全面评估模型性能、开展消融实验，以及测试模型在自遮挡等复杂场景下的鲁棒性 。 英文题目：Learning Human-to-Robot Handovers from Point Clouds 中文题目：从点云学习人机交接 研究背景： 人机交接在人机交互中至关重要，可助力机器人在日常协作与生产制造中发挥作用。然而，实现该任务颇具挑战，机器人需依据有限的视觉输入对人类行为做出反应。当前具身人工智能倾向于在模拟环境中训练机器人，但模拟人类的难度阻碍了人机交互任务的进展。尽管已有研究引入逼真的模拟环境（如HandoverSim），但尚未充分探索在该环境中进行带人类参与的策略训练。 所存在的问题： 一方面，传统基于模型的抓取规划方法依赖物体的3D形状模型，难以处理未见物体；另一方面，基于学习的方法在处理动态交互场景时存在不足，如需要复杂的手工设计成本函数，且在机器人和人类同时运动的情况下，难以通过开环运动规划器获得有效的专家演示来指导训练，同时现有方法在模拟到现实的迁移方面也表现不佳。 解决方法： 提出一种基于视觉的人机交接学习框架，通过两阶段师生训练框架进行训练。在第一阶段，固定人类为静止状态，利用运动和抓取规划获取专家演示，辅助强化学习策略训练；第二阶段，在人类和机器人同时运动的动态场景中，以预训练的策略为教师，对下游策略进行微调。网络输入为手腕摄像头获取的点云，经感知模块处理后，由视觉控制模块预测机器人动作和抓取时机。采用PointNet++进行点云特征编码，利用TD3算法进行策略学习，并结合行为克隆损失、标准演员 - 评论家损失和辅助目标损失来优化网络。 所用到的数据集： DexYCB数据集：包含大量人类与物体交互序列，HandoverSim利用其轨迹在模拟环境中驱动虚拟人类的运动，为训练和评估人机交接任务提供了多样化的人类手部运动和物体交互数据。 HandoverSim基准测试场景：包含1000个独特的人机交接场景，分为训练集、验证集和测试集。每个场景都有独特的人类交接动作，用于评估不同方法在模拟环境中的人机交接性能，作者利用该基准测试场景对提出的方法进行训练和评估。 英文题目：Markerless Camera-to-Robot Pose Estimation via Self-supervised Sim-to-Real Transfer 中文题目：通过自监督模拟到真实迁移的无标记相机到机器人位姿估计 研究背景： 在基于视觉的机器人控制中，相机到机器人的位姿估计是关键环节，位置基视觉伺服（PBVS）依赖它将环境信息转换到机器人坐标系以实现操作任务。传统方法通过在机器人上附加标记来校准位姿，但存在无法在线校准、易受环境因素影响等问题，限制了机器人在复杂现实环境中的应用。深度学习的发展为无标记相机到机器人位姿估计带来新途径，主要分为基于关键点和基于渲染的方法，但前者受模拟到现实差距的限制，后者计算时间长，不适用于动态场景。 所存在的问题： 基于关键点的方法，其性能受限于在模拟环境中训练的关键点检测器，难以适应真实场景；基于渲染的方法，迭代渲染和比较的过程耗时耗能，不适合动态场景下的在线估计。此外，获取真实世界数据的3D标注成本高、工作量大，主流深度学习方法依赖合成数据和域随机化来缩小模拟与真实之间的差距，但效果有限。 解决方法： 提出CtRNet，一种端到端的位姿估计框架。先在合成数据上预训练网络，学习分割机器人和估计位姿的知识。接着，利用前景分割和可微渲染进行自监督训练，实现从模拟到真实的迁移。自监督训练通过最小化渲染的机器人轮廓图像与分割掩码之间的差异来优化神经网络参数，其中前景分割为位姿估计提供监督。在推理时，CtRNet利用关键点检测的快速速度和基于渲染方法的高性能，通过ResNet50作为骨干网络提取特征，经一系列操作生成机器人掩码和关键点，再结合机器人关节角度通过PnP求解器估计位姿，并利用隐函数定理实现PnP求解器的反向传播以训练关键点检测器。 所用到的数据集： DREAM-real数据集：包含约50K张由Azure Kinect、XBOX 360 Kinect和RealSense这3种不同相机拍摄的Franka Emika Panda机械臂的RGB图像，分辨率为(640×480)，每个图像帧都提供了相机到机器人位姿的真实值。使用平均距离（ADD）和曲线下面积（AUC）指标评估方法的准确性，用于评估CtRNet在该数据集上的性能，并与其他先进方法进行比较。 Baxter数据集：包含100张由Azure Kinect相机拍摄的Rethink Baxter左臂的RGB图像，分辨率为(2048×1526)，提供了相对于相机帧的2D和3D末端执行器位置的真实值。采用ADD指标评估末端执行器的位姿估计性能，使用正确关键点百分比（PCK）指标评估末端执行器的重投影误差，用于研究不同预训练样本数量对自监督训练的影响，以及评估CtRNet在该数据集上的2D和3D性能表现。 英文题目：Weakly Supervised Posture Mining for Fine-grained Classification 中文题目：用于细粒度分类的弱监督姿态挖掘方法 研究背景： 细粒度分类任务具有挑战性，因其数据集中不同子类间的视觉差异细微，难以识别。以往研究可分为三类，早期完全监督方法在训练和测试阶段都依赖边界框标注，成本高且易出错；后续方法仅在训练阶段使用标注；近期部分方法虽无需标注，但现有基于局部区域创建图的方法不易移植，且难以感知具有正确上下文信息的判别区域及区域间关系。 所存在的问题： 现有细粒度分类方法存在不足，完全监督方法标注成本高，弱监督方法在感知判别区域及其关系方面存在缺陷，难以有效挖掘姿态信息，导致分类性能受限。此外，传统交叉熵（CE）损失函数在细粒度分类中，因忽视负标签中的类间差异信息，无法充分发挥模型的分类能力。 解决方法： 提出PMRC（posture mining and reverse cross-entropy）框架，该框架可与不同骨干网络结合。利用深度导航器（Deep Navigator）从图像生成判别区域，构建图结构，并通过消息传递聚合图以获取分类结果。设计了一种新的训练范式，使深度导航器和消息传递相互通信和训练，以学习挖掘姿态信息。提出反向交叉熵（RCE）损失函数，通过反转softmax输出层的标签分数，学习类间差异信息，提升模型在细粒度分类上的性能。 所用到的数据集： CUB-200-2011：包含200种鸟类的11,788张图像，训练集5,994张，测试集5,794张。每张图像有详细标注，因其每个物种训练图像较少，被认为是竞争较为激烈的数据集。 Stanford Cars：由196类汽车的16,185张图像组成，训练集和测试集近似50-50划分，类别通常为生产年份和型号级别。 FGVC Aircraft：有102种不同飞机模型变体的10,200张图像，每个变体100张，图像中的飞机有紧密边界框和分层模型标签标注。 Stanford Dogs：包含120类狗的20,580张图像，训练集12,000张，测试集8,580张 。 英文题目：NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis 中文题目：掌间NeRF：通过新视图合成实现机器人的矫正增强 研究背景： 在基于视觉的机器人控制中，6自由度（6-DoF）物体抓取是关键问题。从专家演示中进行模仿学习是有前景的方法，但存在误差累积问题，复杂视觉任务常需在线专家监督或环境交互，成本高昂。同时，离线“反馈增强”方法未充分应用于视觉观测，而利用安装在机器人手腕上的手眼相机虽能提升性能，但仍无法解决根本问题。此外，深度传感器在处理透明或反光物体时存在局限性，基于视觉的6-DoF抓取策略发展面临挑战。 所存在的问题： 传统6-DoF抓取管道采用开环轨迹执行，无法利用感知反馈进行精确抓取。模仿学习在处理复杂视觉任务时，因协变量偏移导致策略易偏离数据分布，且纠正错误困难。现有离线反馈增强方法未涉及视觉观测，标准图像增强方法不修改动作标签，难以有效提升视觉策略性能。 解决方法： 提出SPARTN（Synthetic Perturbations for Augmenting Robot Trajectories via NeRF），这是一种基于NeRF的离线数据增强技术。利用NeRF为每个演示场景训练神经辐射场，通过向演示中的相机姿态注入噪声，并使用NeRF渲染新视图，同时计算矫正动作标签，将增强数据与原始演示数据结合训练反应式实时策略。该方法将矫正反馈增强扩展到视觉领域，能生成仅基于RGB的6-DoF抓取策略，且完全离线，无需额外专家监督或环境交互。 所用到的数据集： 模拟6-DoF抓取基准数据集：包含2500个抓取1500个ShapeNet物体的演示，用于评估SPARTN和相关方法。策略在该数据集上接收来自手腕相机的RGB、RGBD或点云观测，并控制夹爪在6-DoF末端执行器空间中进行相对姿态变化。评估时使用YCB或ShapeNet数据集中未在训练中出现的物体，每个物体在不同初始机器人配置和物体初始姿态下测试10次。 真实世界实验数据集：使用Franka Emika Panda机器人在8个不同环境中进行抓取实验，任务是抓取具有不同几何形状、反射性、透明度和径向对称等特征的目标物体。每个环境中收集少量专家抓取演示，用于训练和评估SPARTN和普通行为克隆（BC）策略，评估时随机化物体和机器人的初始配置。 英文题目：PyPose: A Library for Robot Learning with Physics-based Optimization 中文题目：PyPose：基于物理优化的机器人学习库 研究背景： 深度学习在机器人感知领域成果显著，但在泛化能力上存在不足，易受训练数据、环境变化和计算资源的限制。物理优化方法虽具有良好的泛化能力和较高精度，但依赖手动参数调整且缺乏高级语义信息。目前，学习方法和物理优化通常在机器人系统的不同模块中单独使用，这种解耦的范式难以达到最优解，限制了系统性能和泛化能力。同时，开发端到端可微的集成方法成为趋势，但现有的实现方式多为特定问题编写，缺乏统一框架，不同语言库的混合使用增加了系统复杂性，阻碍了研究进展。 所存在的问题： 深度学习和物理优化方法各自存在局限性，两者结合的现有方式多为特定问题编写，缺乏统一框架，不同语言库的混合使用导致系统复杂，开发周期长。此外，现有的相关开源库也存在不足，如LieTorch仅支持一阶可微操作，CvxpyLayer不支持李群操作和二阶优化器，Theseus采用旋转矩阵表示变换，内存效率低。 解决方法： 提出PyPose，这是一个基于PyTorch的面向机器人的开源库，旨在将深度感知模型与基于物理的优化相结合。通过定义LieTensor来表示3D变换，解决了在学习模型中3D变换的梯度计算和数值问题，支持任意阶梯度自动求导和并行计算，且内存效率高。利用PyTorch的Function和Module概念实现可微的机器人相关功能，提供多种实用模块。集成多种优化器，支持二阶优化，如Levenberg-Marquardt算法，并引入FastTriggs校正器提高稳定性，同时支持多种策略来限制优化步长。 所用到的数据集： TartanAir数据集：用于自监督学习视觉SLAM前端的实验。在实验中，利用估计的姿态和地标位置作为伪标签进行监督，对在MegaDepth上预训练的CAPS网络在TartanAir数据集上进行微调，以评估PyPose在视觉SLAM后端优化中的可行性，通过实验验证了自监督训练后CAPS模型在两帧匹配任务中的匹配精度提高。 KITTI数据集：用于IMU预积分和IMU-centric PGO（姿态图优化）的实验展示。通过在该数据集上的实验，展示了PyPose的IMUPreintegrator模块在处理IMU数据时，结合PGO能够有效减少轨迹漂移，验证了其在惯性导航中的有效性。 EuRoC数据集：用于训练和评估IMU校准网络。通过在该数据集上的实验，验证了使用PyPose的IMUPreintegrator模块训练的IMU校准网络，在姿态估计的精度上相较于传统方法有显著提升，证明了PyPose在基于学习的多模态SLAM和惯性导航系统研究中的优势。 英文题目：UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy 中文题目：UniDexGrasp：通过学习多样提案生成和目标条件策略实现通用机器人灵巧抓取 研究背景： 机器人抓取是机器人与环境交互的基础能力，在平行夹爪抓取算法取得进展的同时，灵巧抓取因能实现复杂和精细的物体操作而愈发重要。但灵巧手的高自由度使得生成有效抓取姿势和规划执行轨迹难度增大，现有工作在解决该问题时，或依赖理想输入，或难以在现实任务设置中实现通用化和多样化的灵巧抓取。 所存在的问题： 现有方法在生成高质量且多样的灵巧抓取姿势以及执行通用的灵巧抓取任务方面存在不足。在抓取姿势生成阶段，基于学习的方法难以兼顾质量和多样性，如使用条件变分自编码器（CVAE）的方法易出现模式崩溃问题。在抓取执行阶段，传统分析方法需对模型进行简化，强化学习和模仿学习方法依赖物体网格的全知知识，难以在现实任务设置中部署，且现有方法均无法在原始视觉输入下对大量物体实现通用化抓取。 解决方法： 提出UniDexGrasp，将任务分解为两个阶段。在抓取提案生成阶段，设计一种条件抓取姿势生成模型，将旋转与平移和关节角度解耦，分别使用GraspIPDF和GraspGlow进行建模，再通过ContactNet优化采样的抓取姿势，以生成多样且高质量的抓取提案。在目标条件抓取执行阶段，采用教师 - 学生学习框架，先使用近端策略优化（PPO）算法结合状态规范化、对象课程学习等技术学习一个能获取理想状态输入的教师策略，再使用DAgger算法将其蒸馏为仅依赖现实输入的学生策略。 所用到的数据集： 通过合成的方式生成了大规模的数据集，包含133个类别中5519个物体实例的112万个有效抓取。这些物体被划分为训练实例（3251个）、已见类别未见实例（754个）和未见类别实例（1514个），用于训练和评估模型在不同情况下的抓取性能。 英文题目：Affordances from Human Videos as a Versatile Representation for Robotics 中文题目：从人类视频中学习可供性作为机器人的通用表示 研究背景： 机器人领域期望构建能通过观察人类进行理解和学习交互的机器人，这推动了视觉领域对相关问题的研究。从人类视频中提取可供性知识的研究虽在静态数据集上取得一定成果，但在与机器人系统集成方面存在不足，如不清楚如何定义和表示可供性，也缺乏在真实机器人和野外环境中的实验评估。而机器人学习方法在面对新任务或新环境时，通常需要从头开始学习，即便有视觉预训练，也难以应对复杂的状态空间和确定具体的操作方式。 所存在的问题： 目前，视觉可供性学习和机器人学习之间存在明显差距。一方面，现有可供性学习方法多在人类视频数据集上测试，未与机器人系统集成，无法明确可供性的有效定义和表示，也难以评估其性能；另一方面，机器人学习在面对新环境和任务时，缺乏有效的先验知识，难以从视觉输入中直接获取如何操作物体的信息，且复杂的状态空间使得学习难度增大。 解决方法： 提出Vision-Robotics Bridge (VRB)方法，旨在弥合视觉与机器人之间的差距。采用接触点和接触后轨迹作为视觉可供性的可操作表示，这种表示以机器人为中心，便于机器人使用。利用大规模以自我为中心的人类交互视频，通过机器人优先的方法提取可供性，包括利用现成工具估计自我运动、人体姿态和手 - 物体交互来获取监督信息，并处理相机运动和视觉领域转移问题。将学习到的可供性与多种机器人学习范式（离线模仿学习、探索、目标条件学习和动作参数化强化学习）无缝集成，以加速机器人在野外的学习。 所用到的数据集： 文中未明确提及具体的数据集名称，但提到利用互联网上的人类行为视频进行模型训练。在实验阶段，使用两个不同的机器人平台（Franka Emika Panda手臂和Hello Stretch移动操作器）在多个环境中进行实验，涵盖多种任务场景，如与橱柜、刀具、蔬菜、架子、锅、门、盖子、抽屉、洗碗机、垃圾桶等物体的交互任务。针对每个任务，通过估计任务空间图像裁剪区域，并使用校准的机器人 - 相机系统（Intel RealSense D415i）将预测的接触点和轨迹投影到3D空间用于机器人控制。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Paper-Robot_list","slug":"Paper-Robot-list","permalink":"http://example.com/tags/Paper-Robot-list/"},{"name":"Paper-CVPR","slug":"Paper-CVPR","permalink":"http://example.com/tags/Paper-CVPR/"}]},{"title":"Coding-isaacsim-控制-Franka和Carter机器人","slug":"coding_RL__isaacsim_demo_of_car_and_robot","date":"2025-04-07T02:42:41.219Z","updated":"2025-04-07T02:45:49.882Z","comments":true,"path":"2025/04/07/coding_RL__isaacsim_demo_of_car_and_robot/","link":"","permalink":"http://example.com/2025/04/07/coding_RL__isaacsim_demo_of_car_and_robot/","excerpt":"","text":"摘要 code 摘要 在 Isaac Sim 仿真环境中添加 Franka 和 Carter 两个机器人，设置它们的初始位姿，然后进行 4 个循环的仿真，在不同的循环中控制机器人的移动和停止，并在特定循环中打印 Carter 机器人的关节位置。最后关闭仿真应用。 code 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102# Copyright (c) 2020-2024, NVIDIA CORPORATION. All rights reserved.## NVIDIA CORPORATION and its licensors retain all intellectual property# and proprietary rights in and to this software, related documentation# and any modifications thereto. Any use, reproduction, disclosure or# distribution of this software and related documentation without an express# license agreement from NVIDIA CORPORATION is strictly prohibited.# 导入SimulationApp类，用于启动Isaac Sim仿真应用from isaacsim import SimulationApp# 启动仿真应用，设置为非无头模式，即打开GUI界面simulation_app = SimulationApp(&#123;&quot;headless&quot;: False&#125;) import sysimport carbimport numpy as np# 导入World类，用于管理仿真世界from isaacsim.core.api import World# 导入Articulation类，用于处理可活动的关节物体from isaacsim.core.prims import Articulation# 导入一些实用函数，用于添加引用到场景、获取场景单位等from isaacsim.core.utils.stage import add_reference_to_stage, get_stage_units# 导入ArticulationAction类型，用于定义关节动作from isaacsim.core.utils.types import ArticulationAction# 导入设置相机视角的实用函数from isaacsim.core.utils.viewports import set_camera_view# 导入获取资产根路径的函数from isaacsim.storage.native import get_assets_root_path# 准备场景# 获取Isaac Sim资产的根路径assets_root_path = get_assets_root_path()# 如果无法找到资产根路径，记录错误信息，关闭仿真应用并退出程序if assets_root_path is None: carb.log_error(&quot;Could not find Isaac Sim assets folder&quot;) simulation_app.close() sys.exit()# 创建一个世界对象，设置场景单位为米my_world = World(stage_units_in_meters=1.0)# 向场景中添加默认的地面平面my_world.scene.add_default_ground_plane() # 设置相机视角，指定相机位置和目标位置set_camera_view( eye=[5.0, 0.0, 1.5], target=[0.00, 0.00, 1.00], camera_prim_path=&quot;/OmniverseKit_Persp&quot;) # 添加Franka机器人到场景# 构建Franka机器人的USD文件路径asset_path = assets_root_path + &quot;/Isaac/Robots/Franka/franka.usd&quot;# 将Franka机器人的USD文件引用添加到场景中，指定其在场景中的路径add_reference_to_stage(usd_path=asset_path, prim_path=&quot;/World/Arm&quot;) # 创建一个Franka机器人的关节物体对象arm = Articulation(prim_paths_expr=&quot;/World/Arm&quot;, name=&quot;my_arm&quot;) # 添加Carter机器人到场景# 构建Carter机器人的USD文件路径asset_path = assets_root_path + &quot;/Isaac/Robots/NVIDIA/Carter/nova_carter/nova_carter.usd&quot;# 将Carter机器人的USD文件引用添加到场景中，指定其在场景中的路径add_reference_to_stage(usd_path=asset_path, prim_path=&quot;/World/Car&quot;)# 创建一个Carter机器人的关节物体对象car = Articulation(prim_paths_expr=&quot;/World/Car&quot;, name=&quot;my_car&quot;)# 设置Franka机器人和Carter机器人的初始位姿，避免在仿真开始前发生碰撞# 设置Franka机器人的世界位置arm.set_world_poses(positions=np.array([[0.0, 1.0, 0.0]]) / get_stage_units())# 设置Carter机器人的世界位置car.set_world_poses(positions=np.array([[0.0, -1.0, 0.0]]) / get_stage_units())# 初始化世界my_world.reset()# 进行4个循环的仿真for i in range(4): # 打印当前循环次数 print(&quot;running cycle: &quot;, i) # 在第1次和第3次循环时，移动机器人 if i == 1 or i == 3: print(&quot;moving&quot;) # 移动Franka机器人，设置其关节位置 arm.set_joint_positions([[-1.5, 0.0, 0.0, -1.5, 0.0, 1.5, 0.5, 0.04, 0.04]]) # 移动Carter机器人，设置其关节速度 car.set_joint_velocities([[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]) # 在第2次循环时，停止机器人 if i == 2: print(&quot;stopping&quot;) # 重置Franka机器人的关节位置 arm.set_joint_positions([[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]) # 停止Carter机器人，设置其关节速度为0 car.set_joint_velocities([[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]) # 每个循环中进行100步的仿真 for j in range(100): # 执行一步仿真，同时进行渲染和物理模拟 my_world.step(render=True) # 在第3次循环时，打印Carter机器人的关节位置 if i == 3: # 获取Carter机器人的关节位置 car_joint_positions = car.get_joint_positions() print(&quot;car joint positions:&quot;, car_joint_positions)# 关闭仿真应用simulation_app.close()","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-isaacsim","slug":"Code-isaacsim","permalink":"http://example.com/tags/Code-isaacsim/"}]},{"title":"Coding-强化学习-策略梯度方法(PG)-控制倒立杆","slug":"coding_RL__play_bar_demo","date":"2024-12-21T07:01:20.987Z","updated":"2024-12-24T08:24:52.093Z","comments":true,"path":"2024/12/21/coding_RL__play_bar_demo/","link":"","permalink":"http://example.com/2024/12/21/coding_RL__play_bar_demo/","excerpt":"","text":"摘要 play code train code test code 摘要 使用策略梯度方法训练策略网络 1.play code 2.训练code 3.测试code play code 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import gymimport pygameimport timeimport random&quot;&quot;&quot;这个脚本是一个简单的 OpenAI Gym 库中 CartPole-v1 环境的实现。游戏通过键盘上的左右箭头键进行控制。目标是尽可能长时间地平衡一个移动小车上的杆。如果杆倒下或小车移出边界，游戏将结束。脚本使用 Pygame 处理键盘输入，并在每一步之间暂停游戏一小段时间以允许人类反应。游戏将在每一步打印小车和杆的状态，并在游戏结束时显示总时间和步数。&quot;&quot;&quot;if __name__==&#x27;__main__&#x27;: pygame.init() env = gym.make(&#x27;CartPole-v1&#x27;,render_mode=&#x27;human&#x27;) state = env.reset() cart_position = state[0]#小车的位置 cart_speed = state[1]#小车的速度 pole_angle = state[2]#杆的角度 pole_speed = state[3]#杆的尖端速度 print(f&quot;Begin state: &#123;state&#125;&quot;) print(f&quot;cart_position = &#123;cart_position:.2f&#125;&quot;) print(f&quot;cart_speed = &#123;cart_speed:.2f&#125;&quot;) print(f&quot;pole_anglee=t&#123;pole_angle:.2f&#125;&quot;) print(f&quot;pole_speed = &#123;pole_speed:.2f&#125;&quot;) time.sleep(3)#暂停环境3秒，给玩家一个缓冲时间 #实现游戏过程 start_time = time.time()#记录游戏的开始时间小 max_action = 10000 #设置游戏的最大执行次数 #最多让用户输入1000次方向键，游戏就以通关结束 #这里要说的是，1000次其实已经很多了，我大概最多就能坚持100多次 step = 0 fail = False for step in range(1, max_action + 1): #首先使用time.sleep，使游戏暂停θ.3秒，用于人的反应 time.sleep(0.3) #如果觉得自己反应够快，这里可以设置更短的时间，比如0.1秒 #觉得自己反应慢，就设置更长的时间，比如1秒。 #以非阻塞的方式，接收用户的键盘输入 keys = pygame.key.get_pressed() action = 0 if not keys[pygame.K_LEFT] and not keys[pygame.K_RIGHT]: action = random.choice([0,1]) if keys[pygame.K_LEFT]: action = 0 elif keys[pygame.K_RIGHT]: action = 1 state,_,done,_ = env.step(action) if done: fail = True break #打印一些调试信息 print(f&quot;step = &#123;step&#125; action = &#123;action&#125;&quot; f&quot;angle = &#123;state[2]:.2f&#125; position = &#123;state[0]:.2f&#125;&quot;) #计算结束时间 end_time = time.time() game_time = end_time - start_time #打印提示 if fail: print(f&quot;Game over, you play &#123;game_time:.2f&#125; seconds, &#123;step&#125; steps.&quot;) else: print(f&quot;Congratulation! You play &#123;game_time:.2f&#125; seconds, &#123;step&#125; steps.&quot;) env.close() train code 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394import gymimport pygameimport timeimport randomimport torchfrom torch import nnimport torch.nn.functional as Fimport torchimport numpy as np#实现损失函数的计算方法，函数传入步数n和这n步对应的概率Log-pdef compute_policy_loss(n, log_p): #R就是q r = list() #构造奖励r的列表 for i in range(n, 0, -1): r.append(i * 1.0) r = torch.tensor(r) r=(r-r.mean())/r.std()#进行标准化处理 loss = 0 #计算损失函数 for pi, ri in zip(log_p, r): loss += -pi * ri return lossclass CartPolePolicy(nn.Module): def __init__(self): super(CartPolePolicy, self).__init__() #定义两个线性层，大小分别是4*128与128*2 self.fc1 = nn.Linear(4, 128) self.fc2 = nn.Linear(128, 2) #定义一个dropout层，丢弃比率是60% self.drop = nn.Dropout(p=0.6) def forward(self, x): x = self.fc1(x) x = self.drop(x) x = F.relu(x) x = self.fc2(x) #使用softmax决策最终的行动，是向左还是向右 return F.softmax(x, dim = 1)if __name__==&#x27;__main__&#x27;: pygame.init() env = gym.make(&#x27;CartPole-v1&#x27;,render_mode=&#x27;human&#x27;) state = env.reset() policy = CartPolePolicy() optimizer = torch.optim.Adam(policy.parameters(),lr=0.01) max_episode=1000 max_action=10000 max_steps=5000 for episode in range(1, max_episode + 1): #对于每一轮循环，都要重新启动一次游戏环境小黑黑讲 state = env.reset() step =0 log_p = list() for step in range(1, max_action + 1): state = torch.from_numpy(state).float().unsqueeze(0) probs=policy(state) #计算神经网络给出的行动概率 #基于网络给出的概率分布，随机选择行动,m是一个概率对象 m = torch.distributions.Categorical(probs) action=m.sample() # 这里并不是直接使用概率较大的行动 #而是通过概率分布生成action，这样可以进一步探索低概率的行动 state,_,done,_ = env.step(action.item()) if done: break log_p.append(m.log_prob(action)) if step&gt;max_steps:#当step大于最大步数时 print(f&quot;Done! Last episode &#123;episode&#125; Run steps: &#123;step&#125;&quot;) break #跳出循环，结束训练 #每一回合游戏，都会做一次梯度下降算法 # Example of calculating the natural logarithm of an array using numpy optimizer.zero_grad() Loss = compute_policy_loss(step, log_p) Loss.backward() optimizer.step() if episode % 10==0: print(f&#x27;Episode &#123;episode&#125; Run steps: &#123;step&#125;&#x27;) torch.save(policy.state_dict(),&#x27;cartpole_policy.pth&#x27;) test code 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495import gymimport pygameimport timeimport randomimport torchfrom torch import nnimport torch.nn.functional as Fimport torch#实现损失函数的计算方法，函数传入步数n和这n步对应的概率Log-pdef compute_policy_loss(n, log_p): r = list() #构造奖励r的列表 for i in range(n, 0, -1): r.append(i * 1.0) r = torch.tensor(r) r=(r-r.mean())/r.std()#进行标准化处理 Loss = 0 #计算损失函数 for pi, ri in zip(log_p, r): loss += -pi * ri return lossclass CartPolePolicy(nn.Module): def __init__(self): super(CartPolePolicy, self).__init__() #定义两个线性层，大小分别是4*128与128*2 self.fc1 = nn.Linear(4, 128) self.fc2 = nn.Linear(128, 2) #定义一个dropout层，丢弃比率是60% self.drop = nn.Dropout(p=0.6) def forward(self, x): x = self.fc1(x) x = self.drop(x) x = F.relu(x) x = self.fc2(x) #使用softmax决策最终的行动，是向左还是向右 return F.softmax(x, dim = 1)if __name__==&#x27;__main__&#x27;: pygame.init() env = gym.make(&#x27;CartPole-v1&#x27;,render_mode=&#x27;human&#x27;) state = env.reset() cart_position = state[0]#小车的位置 cart_speed = state[1]#小车的速度 pole_angle = state[2]#杆的角度 pole_speed = state[3]#杆的尖端速度 print(f&quot;Begin state: &#123;state&#125;&quot;) print(f&quot;cart_position = &#123;cart_position:.2f&#125;&quot;) print(f&quot;cart_speed = &#123;cart_speed:.2f&#125;&quot;) print(f&quot;pole_anglee=t&#123;pole_angle:.2f&#125;&quot;) print(f&quot;pole_speed = &#123;pole_speed:.2f&#125;&quot;) time.sleep(3)#暂停环境3秒，给玩家一个缓冲时间 policy = CartPolePolicy() policy.load_state_dict(torch.load(&#x27;cartpole_policy.pth&#x27;)) policy.eval() #实现游戏过程 start_time = time.time()#记录游戏的开始时间小 max_action = 1000 #设置游戏的最大执行次数 #最多让用户输入1000次方向键，游戏就以通关结束 step = 0 fail = False for step in range(1, max_action + 1): #首先使用time.sleep，使游戏暂停θ.3秒，用于人的反应 time.sleep(0.1) state = torch.from_numpy(state).float().unsqueeze(0) probs = policy(state) action = torch.argmax(probs,dim=1).item() state,_,done,_ = env.step(action) if done: fail = True break #打印一些调试信息 print(f&quot;step = &#123;step&#125; action = &#123;action&#125;&quot; f&quot;angle = &#123;state[2]:.2f&#125; position = &#123;state[0]:.2f&#125;&quot;) #计算结束时间 end_time = time.time() game_time = end_time - start_time #打印提示 if fail: print(f&quot;Game over, you play &#123;game_time:.2f&#125; seconds, &#123;step&#125; steps.&quot;) else: print(f&quot;Congratulation! You play &#123;game_time:.2f&#125; seconds, &#123;step&#125; steps.&quot;) env.close()","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-强化学习","slug":"Code-强化学习","permalink":"http://example.com/tags/Code-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"},{"name":"Code-模型库","slug":"Code-模型库","permalink":"http://example.com/tags/Code-%E6%A8%A1%E5%9E%8B%E5%BA%93/"}]},{"title":"Coding-robot-手柄按键检测","slug":"coding_python_robot_jojstick_check","date":"2024-12-06T07:03:40.401Z","updated":"2024-12-06T07:05:52.158Z","comments":true,"path":"2024/12/06/coding_python_robot_jojstick_check/","link":"","permalink":"http://example.com/2024/12/06/coding_python_robot_jojstick_check/","excerpt":"","text":"检测按下的手柄按键序号 检测按下的手柄按键序号 12345678910111213141516171819202122232425262728293031323334353637383940414243import pygameimport sys# 初始化pygamepygame.init()# 设置手柄pygame.joystick.init()# 检查是否有手柄连接if pygame.joystick.get_count() == 0: print(&quot;没有检测到手柄。&quot;) sys.exit()# 获取手柄对象joystick = pygame.joystick.Joystick(0)joystick.init()print(f&quot;已连接手柄: &#123;joystick.get_name()&#125;&quot;)# 主循环try: while True: for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() sys.exit() # 检测手柄按钮按下 if event.type == pygame.JOYBUTTONDOWN: print(f&quot;按钮 &#123;event.button&#125; 被按下&quot;) # 检测手柄按钮释放 elif event.type == pygame.JOYBUTTONUP: print(f&quot;按钮 &#123;event.button&#125; 被释放&quot;) # 检测手柄轴移动 elif event.type == pygame.JOYAXISMOTION: print(f&quot;轴 &#123;event.axis&#125; 移动到 &#123;event.value&#125;&quot;)except KeyboardInterrupt: pygame.quit()","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-机器人","slug":"Code-机器人","permalink":"http://example.com/tags/Code-%E6%9C%BA%E5%99%A8%E4%BA%BA/"}]},{"title":"Coding-robot-手柄_键盘控制信号获取-串口发送","slug":"coding_python_robot_jojstick","date":"2024-12-06T06:55:35.173Z","updated":"2024-12-06T07:03:20.938Z","comments":true,"path":"2024/12/06/coding_python_robot_jojstick/","link":"","permalink":"http://example.com/2024/12/06/coding_python_robot_jojstick/","excerpt":"","text":"获取手柄控制信号，通过串口发送 获取键盘信号，通过串口发送 获取手柄控制信号，通过串口发送 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import pygameimport timeimport serial# 设置串口和波特率ser = serial.Serial(&#x27;COM5&#x27;, 9600) # 替换为你的Arduino串口time.sleep(2) # 等待Arduino初始化pygame.init()pygame.joystick.init()done=Falsewhile (done != True): for event in pygame.event.get(): # User did something if event.type == pygame.QUIT: # If user clicked close done = True # Flag that we are done so we exit this loop joystick_count = pygame.joystick.get_count() for i in range(joystick_count): joystick = pygame.joystick.Joystick(i) joystick.init() axes = joystick.get_numaxes() print(&#x27;================&#x27;) for i in range(7): axis = joystick.get_axis(i) value = int(axis * 90) + 90 if i==0: print(axis) if abs(value)&gt;100 : print(&quot;发送指令: 0&quot;) ser.write(b&#x27;0&#x27;) # 顺时针转 time.sleep(0.1) # 防止重复发送 if abs(value)&lt;=60 : print(&quot;发送指令: 1&quot;) ser.write(b&#x27;1&#x27;) # 顺时针转 time.sleep(0.1) # 防止重复发送 if i==6: print(axis) if abs(value)&gt;100 : print(&quot;发送指令: 3&quot;) ser.write(b&#x27;3&#x27;) # 顺时针转 time.sleep(0.1) # 防止重复发送 if abs(value)&lt;=60 : print(&quot;发送指令: 2&quot;) ser.write(b&#x27;2&#x27;) # 顺时针转 time.sleep(0.1) # 防止重复发送 # if i==0: # print(axis) # if abs(value)&gt;100 : # print(&quot;发送指令: 4&quot;) # ser.write(b&#x27;4&#x27;) # 顺时针转 # time.sleep(0.1) # 防止重复发送 # if abs(value)&lt;=60 : # print(&quot;发送指令: 5&quot;) # ser.write(b&#x27;5&#x27;) # 顺时针转 # time.sleep(0.1) # 防止重复发送# 按键 axes = joystick.get_numbuttons() print(&#x27;================&#x27;) for i in range(axes): axis =joystick.get_button(i) if i==8: if axis==1: print(axis) print(&quot;发送指令: 4&quot;) ser.write(b&#x27;4&#x27;) # 顺时针转 time.sleep(0.1) # 防止重复发送 if i==9: if axis==1: print(axis) print(&quot;发送指令: 5&quot;) ser.write(b&#x27;5&#x27;) # 顺时针转 time.sleep(0.1) # 防止重复发送 . 获取键盘信号，通过串口发送 1234567891011121314151617181920212223242526272829303132333435363738394041424344# import serial# import time# # 设置串口和波特率# ser = serial.Serial(&#x27;COM5&#x27;, 9600) # 替换&#x27;COM3&#x27;为你的Arduino串口# time.sleep(2) # 等待Arduino初始化# # 发送数据# ser.write(b&#x27;1&#x27;)# # 读取数据# while True:# if ser.in_waiting &gt; 0: # 检查是否有可读数据# line = ser.readline().decode(&#x27;utf-8&#x27;).rstrip() # 读取一行# print(line) # 打印接收到的数据# ser.close() # 关闭串口import serialimport timeimport keyboard # 导入keyboard库# 设置串口和波特率ser = serial.Serial(&#x27;COM5&#x27;, 9600) # 替换为你的Arduino串口time.sleep(2) # 等待Arduino初始化try: print(&quot;按 &#x27;1&#x27; 进行顺时针转动，按 &#x27;0&#x27; 进行逆时针转动，按 &#x27;q&#x27; 退出。&quot;) while True: if keyboard.is_pressed(&#x27;1&#x27;): # 检测按键 &#x27;1&#x27; print(&quot;发送指令: 1&quot;) ser.write(b&#x27;1&#x27;) # 顺时针转 time.sleep(0.1) # 防止重复发送 elif keyboard.is_pressed(&#x27;0&#x27;): # 检测按键 &#x27;0&#x27; print(&quot;发送指令: 0&quot;) ser.write(b&#x27;0&#x27;) # 逆时针转 time.sleep(0.1) # 防止重复发送 elif keyboard.is_pressed(&#x27;q&#x27;): # 检测按键 &#x27;q&#x27; print(&quot;退出程序。&quot;) break # 退出循环finally: ser.close() # 关闭串口","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-机器人","slug":"Code-机器人","permalink":"http://example.com/tags/Code-%E6%9C%BA%E5%99%A8%E4%BA%BA/"}]},{"title":"Coding-python-深度相机相关","slug":"coding_python_image_RGBD","date":"2024-12-06T06:38:25.979Z","updated":"2024-12-06T06:55:24.347Z","comments":true,"path":"2024/12/06/coding_python_image_RGBD/","link":"","permalink":"http://example.com/2024/12/06/coding_python_image_RGBD/","excerpt":"","text":"显示RBG和深度图 获取指定点的深度信息 鼠标点击图像，获取像素坐标及深度信息 计算手眼标定矩阵，鼠标点击图像，将像素坐标转换为机械臂坐标，进一步发送坐标给机械臂 显示RBG和深度图 123456789101112131415161718192021222324252627282930313233343536373839404142import pyrealsense2 as rsimport numpy as npimport cv2if __name__ == &quot;__main__&quot;: # Configure depth and color streams pipeline = rs.pipeline() config = rs.config() config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30) config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30) # Start streaming pipeline.start(config) try: while True: # Wait for a coherent pair of frames: depth and color frames = pipeline.wait_for_frames() depth_frame = frames.get_depth_frame() color_frame = frames.get_color_frame() if not depth_frame or not color_frame: continue # Convert images to numpy arrays depth_image = np.asanyarray(depth_frame.get_data()) color_image = np.asanyarray(color_frame.get_data()) # Apply colormap on depth image (image must be converted to 8-bit per pixel first) depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET) # Stack both images horizontally images = np.hstack((color_image, depth_colormap)) # Show images cv2.namedWindow(&#x27;RealSense&#x27;, cv2.WINDOW_AUTOSIZE) cv2.imshow(&#x27;RealSense&#x27;, images) key = cv2.waitKey(1) # Press esc or &#x27;q&#x27; to close the image window if key &amp; 0xFF == ord(&#x27;q&#x27;) or key == 27: cv2.destroyAllWindows() break finally: # Stop streaming pipeline.stop() 获取指定点的深度信息 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import pyrealsense2 as rsimport numpy as npimport cv2def get_depth_at_point(depth_frame, point): # 获取指定点的深度值 depth_value = depth_frame.get_distance(point[0], point[1]) return depth_valueif __name__ == &quot;__main__&quot;: # Configure depth and color streams pipeline = rs.pipeline() config = rs.config() config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30) config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30) # Start streaming pipeline.start(config) # 定义点的坐标 (x, y) point = (320, 240) # 默认设置为图像中心 try: while True: # Wait for a coherent pair of frames: depth and color frames = pipeline.wait_for_frames() depth_frame = frames.get_depth_frame() color_frame = frames.get_color_frame() if not depth_frame or not color_frame: continue # Convert images to numpy arrays depth_image = np.asanyarray(depth_frame.get_data()) color_image = np.asanyarray(color_frame.get_data()) # 获取指定点的深度信息 depth_value = get_depth_at_point(depth_frame, point) print(f&quot;Depth at point &#123;point&#125;: &#123;depth_value:.2f&#125; meters&quot;) # 在指定点绘制绿色圆圈 cv2.circle(color_image, point, 5, (0, 255, 0), -1) # Apply colormap on depth image depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET) # Stack both images horizontally images = np.hstack((color_image, depth_colormap)) # Show images cv2.namedWindow(&#x27;RealSense&#x27;, cv2.WINDOW_AUTOSIZE) cv2.imshow(&#x27;RealSense&#x27;, images) key = cv2.waitKey(1) # Press esc or &#x27;q&#x27; to close the image window if key &amp; 0xFF == ord(&#x27;q&#x27;) or key == 27: cv2.destroyAllWindows() break # 可以在这里添加代码，通过键盘输入更新点的位置 # 如果需要动态输入，可以使用 cv2.setMouseCallback() 来获取鼠标点击的位置 finally: # Stop streaming pipeline.stop() 鼠标点击图像，获取像素坐标及深度信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import pyrealsense2 as rsimport numpy as npimport cv2def get_depth_at_point(depth_frame, point): &quot;&quot;&quot;获取指定点的深度值。&quot;&quot;&quot; depth_value = depth_frame.get_distance(point[0], point[1]) return depth_valuedef main(): # 初始化摄像头 pipeline = rs.pipeline() config = rs.config() config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30) config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30) pipeline.start(config) try: while True: # 等待深度和颜色帧 frames = pipeline.wait_for_frames() depth_frame = frames.get_depth_frame() color_frame = frames.get_color_frame() if not depth_frame or not color_frame: continue # 转换为numpy数组 depth_image = np.asanyarray(depth_frame.get_data()) color_image = np.asanyarray(color_frame.get_data()) # 显示图像 cv2.imshow(&#x27;Color Image&#x27;, color_image) # 鼠标点击获取坐标 def get_point(event, x, y, flags, param): if event == cv2.EVENT_LBUTTONDOWN: depth_value = get_depth_at_point(depth_frame, (x, y)) print(f&quot;Depth at (&#123;x&#125;, &#123;y&#125;): &#123;depth_value:.2f&#125; meters&quot;) #这里 cv2.setMouseCallback(&#x27;Color Image&#x27;, get_point) key = cv2.waitKey(1) if key &amp; 0xFF == ord(&#x27;q&#x27;) or key == 27: cv2.destroyAllWindows() break finally: pipeline.stop()if __name__ == &quot;__main__&quot;: main() 计算手眼标定矩阵，鼠标点击图像，将像素坐标转换为机械臂坐标，进一步发送坐标给机械臂 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import pyrealsense2 as rsimport numpy as npimport cv2import serialimport timeclass RobotArmController: def __init__(self, port, baudrate=9600): self.ser = serial.Serial(port, baudrate) time.sleep(2) def send_command(self, command): self.ser.write(f&quot;&#123;command&#125;&quot;.encode()) print(f&quot;发送: &#123;command&#125;&quot;) def move_to(self, x, y, z): command = f&quot;G0 &#123;x&#125; &#123;y&#125; &#123;z&#125;&quot; self.send_command(command) def close(self): self.ser.close()def get_depth_at_point(depth_frame, point): return depth_frame.get_distance(point[0], point[1])def main(): # 初始化摄像头 pipeline = rs.pipeline() config = rs.config() config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30) config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30) pipeline.start(config) # 初始化机械臂 controller = RobotArmController(port=&#x27;COM5&#x27;) # 手眼标定点的存储 camera_points = [] arm_points = [] try: while True: frames = pipeline.wait_for_frames() depth_frame = frames.get_depth_frame() color_frame = frames.get_color_frame() if not depth_frame or not color_frame: continue depth_image = np.asanyarray(depth_frame.get_data()) color_image = np.asanyarray(color_frame.get_data()) cv2.imshow(&#x27;Color Image&#x27;, color_image) # 鼠标点击获取坐标 def get_point(event, x, y, flags, param): if event == cv2.EVENT_LBUTTONDOWN: depth_value = get_depth_at_point(depth_frame, (x, y)) print(f&quot;Depth at (&#123;x&#125;, &#123;y&#125;): &#123;depth_value:.2f&#125; meters&quot;) camera_points.append([x, y, depth_value]) # 移动机械臂到指定坐标（假设在Z轴上） controller.move_to(x, y, depth_value) arm_points.append([x, y, depth_value]) # 假设机械臂坐标与深度值一致 cv2.setMouseCallback(&#x27;Color Image&#x27;, get_point) key = cv2.waitKey(1) if key &amp; 0xFF == ord(&#x27;q&#x27;) or key == 27: cv2.destroyAllWindows() break # 计算手眼标定矩阵 if len(camera_points) &gt; 0 and len(arm_points) &gt; 0: camera_points = np.array(camera_points) arm_points = np.array(arm_points) # 计算最小二乘法拟合转换矩阵 A = np.hstack((camera_points, np.ones((camera_points.shape[0], 1)))) B = arm_points # 求解AX = B X, _, _, _ = np.linalg.lstsq(A, B, rcond=None) print(&quot;Hand-eye calibration matrix:&quot;) print(X) finally: controller.close() pipeline.stop()if __name__ == &quot;__main__&quot;: main()","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-图像处理","slug":"Code-图像处理","permalink":"http://example.com/tags/Code-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"},{"name":"Code-机器人","slug":"Code-机器人","permalink":"http://example.com/tags/Code-%E6%9C%BA%E5%99%A8%E4%BA%BA/"}]},{"title":"Coding-基于exl序号寻找文件夹中对应序号的txt","slug":"coding_python_find_txt_from_exl_","date":"2024-11-29T11:40:49.783Z","updated":"2024-11-29T11:51:36.888Z","comments":true,"path":"2024/11/29/coding_python_find_txt_from_exl_/","link":"","permalink":"http://example.com/2024/11/29/coding_python_find_txt_from_exl_/","excerpt":"","text":"基于exl序号寻找文件夹中对应序号的txt 基于exl序号寻找文件夹中对应序号的txt 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import pandas as pdimport numpy as npimport osimport globimport fnmatch# 读取CSV文件csv_file_path = &#x27;now_have.csv&#x27; # 替换为你的CSV文件路径,前两列为序号对,后一列为真值，都有表头# 读取 CSV 文件到 DataFrame# data = pd.read_csv(csv_file_path, delimiter=&#x27; &#x27;, names=[&#x27;c&#x27;, &#x27;l&#x27;, &#x27;t&#x27;], header=None)data_pd = pd.read_csv(csv_file_path, header=0) # 打印第一列的内容print(&quot;First column (c):&quot;)# 定义文件夹路径folder_path = &#x27;dataset\\\\toushe\\\\toushe_kongjianguangyuan_aiwantisi_hs2048&#x27; # 空间光源+好光谱save_name = &#x27;gao_1121_good_light_good_spectrum_green_data.npy&#x27;# 使用glob模块获取所有.txt文件txt_files = glob.glob(os.path.join(folder_path, &#x27;*.txt&#x27;))# 过滤掉文件名中包含 &quot;gualazhi&quot; 字符串的文件filtered_txt_files = [file for file in txt_files if not fnmatch.fnmatch(os.path.basename(file), &#x27;*gualazhi*&#x27;)]data_y = []# 打印所有.txt文件的路径for file_path_ in txt_files: file_path = os.path.basename(file_path_) # 使用分隔符分割文件名 name_parts = file_path.split(&#x27;-&#x27;) # 提取 &quot;10-1&quot; 部分 extracted_part = (int(name_parts[0]), int(name_parts[1])) # 查找匹配的行 matching_rows = data_pd[(data_pd[&#x27;c&#x27;] == extracted_part[0]) &amp; (data_pd[&#x27;l&#x27;] == extracted_part[1])] # 如果找到匹配的行，打印 &quot;OK&quot; if not matching_rows.empty: with open(file_path_, &#x27;r&#x27;) as file: lines = file.readlines() # 提取数据部分 data_lines = lines[8:] # 跳过前8行元数据和表头 data = [] for line in data_lines: if line.strip(): # 忽略空行 parts = line.strip().split(&#x27;;&#x27;) wave = float(parts[0]) sample = float(parts[1]) dark = float(parts[2]) reference = float(parts[3]) corrected = float(parts[4]) feature=corrected/(reference-dark) true_label = float(matching_rows[&#x27;t&#x27;]) data.append([feature,true_label]) data_np = np.array(data) data_y.append(data_np) print(&quot;OK&quot;) else: # print(&quot;No match found&quot;) pass# 数据处理完data_y_np = np.array(data_y)transposed_data = np.transpose(data_y_np, (0, 2, 1))np.save(save_name, transposed_data)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-kmeans聚类+T-SEN可视化","slug":"coding_network_cluster_tsen","date":"2024-11-29T11:35:39.962Z","updated":"2024-12-21T07:00:46.171Z","comments":true,"path":"2024/11/29/coding_network_cluster_tsen/","link":"","permalink":"http://example.com/2024/11/29/coding_network_cluster_tsen/","excerpt":"","text":"摘要 main.py 摘要 kmeans聚类,T-SEN可视化 12345678910111213141516171819202122232425262728293031323334import numpy as npimport matplotlib.pyplot as pltfrom sklearn.cluster import KMeansfrom sklearn.manifold import TSNE# 加载数据data = np.load(&#x27;test.npy&#x27;)# 假设 data 的特征在第一维features = data[:, 0, :] # 根据实际数据结构选择特征# 聚类num_clusters = 3 # 根据需要设置聚类的数量kmeans = KMeans(n_clusters=num_clusters, random_state=0)kmeans.fit(features)clusters = kmeans.labels_ # 获取每个样本的聚类标签# 使用 t-SNE 降维tsne = TSNE(n_components=2, random_state=10)feature_tsne = tsne.fit_transform(features)# 绘制 t-SNE 散点图plt.figure(figsize=(10, 8))scatter = plt.scatter(feature_tsne[:, 0], feature_tsne[:, 1], c=clusters, cmap=&#x27;viridis&#x27;, marker=&#x27;o&#x27;)# 创建颜色条legend1 = plt.legend(*scatter.legend_elements(), title=&quot;Clusters&quot;)plt.gca().add_artist(legend1)plt.title(&#x27;t-SNE Visualization of Clusters&#x27;)plt.xlabel(&#x27;t-SNE Component 1&#x27;)plt.ylabel(&#x27;t-SNE Component 2&#x27;)plt.grid()plt.show()","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-分类","slug":"Code-分类","permalink":"http://example.com/tags/Code-%E5%88%86%E7%B1%BB/"}]},{"title":"Coding-神经网络训练框架","slug":"coding_network_framework","date":"2024-11-29T11:28:46.014Z","updated":"2024-11-29T11:35:46.879Z","comments":true,"path":"2024/11/29/coding_network_framework/","link":"","permalink":"http://example.com/2024/11/29/coding_network_framework/","excerpt":"","text":"摘要 main.py 摘要 神经网络训练框架 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# 设置超参数# input_size = 1625 # mini_spectruminput_size = 2048 # good_spectrumhidden_size = 128num_classes = 5learning_rate = 0.001num_epochs = 300batch_size = 32# 定义损失函数和优化器criterion = nn.CrossEntropyLoss()optimizer = optim.Adam(model.parameters(), lr=learning_rate)# 训练模型for epoch in range(num_epochs): model.train() running_loss = 0.0 for inputs, labels in train_loader: optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f&#x27;Epoch [&#123;epoch+1&#125;/&#123;num_epochs&#125;], Loss: &#123;running_loss/len(train_loader):.4f&#125;&#x27;)# 测试模型model.eval()with torch.no_grad(): correct = 0 total = 0 class_correct = list(0. for i in range(num_classes)) class_total = list(0. for i in range(num_classes)) for inputs, labels in test_loader: outputs = model(inputs) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() # 统计每个类的准确率 c = (predicted == labels).squeeze() for i in range(len(labels)): _label = labels[i] class_correct[_label] += c[i].item() class_total[_label] += 1 print(f&#x27;Accuracy of the model on the test data: &#123;100 * correct / total:.2f&#125;%&#x27;) # 统计每个类的样本数量","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-分类","slug":"Code-分类","permalink":"http://example.com/tags/Code-%E5%88%86%E7%B1%BB/"}]},{"title":"Coding-强化学习-DQN","slug":"coding_RL_DQN","date":"2024-11-29T11:21:21.442Z","updated":"2024-11-29T11:58:42.007Z","comments":true,"path":"2024/11/29/coding_RL_DQN/","link":"","permalink":"http://example.com/2024/11/29/coding_RL_DQN/","excerpt":"","text":"摘要 DQN 摘要 基于TD算法的DQN DQN 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162import randomimport gymnasium as gymimport numpy as npimport collectionsfrom tqdm import tqdmimport torchimport torch.nn.functional as Fimport matplotlib.pyplot as pltimport rl_utilsclass ReplayBuffer: &#x27;&#x27;&#x27; 经验回放池 &#x27;&#x27;&#x27; def __init__(self, capacity): self.buffer = collections.deque(maxlen=capacity) # 队列,先进先出 def add(self, state, action, reward, next_state, done): # 将数据加入buffer self.buffer.append((state, action, reward, next_state, done)) #默认右边进，左边出 def sample(self, batch_size): # 从buffer中采样数据,数量为batch_size transitions = random.sample(self.buffer, batch_size) state, action, reward, next_state, done = zip(*transitions)#解包 return np.array(state), action, reward, np.array(next_state), done def size(self): # 目前buffer中数据的数量 return len(self.buffer) class Qnet(torch.nn.Module): &#x27;&#x27;&#x27; 只有一层隐藏层的Q网络 &#x27;&#x27;&#x27; def __init__(self, state_dim, hidden_dim, action_dim): super(Qnet, self).__init__() self.fc1 = torch.nn.Linear(state_dim, hidden_dim) self.fc2 = torch.nn.Linear(hidden_dim, action_dim) def forward(self, x): x = F.relu(self.fc1(x)) # 隐藏层使用ReLU激活函数 return self.fc2(x)class DQN: &#x27;&#x27;&#x27; DQN算法 &#x27;&#x27;&#x27; def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma, epsilon, target_update, device): self.action_dim = action_dim self.q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device) # Q网络 # 目标网络 self.target_q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device) # 使用Adam优化器 self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate) self.gamma = gamma # 折扣因子 self.epsilon = epsilon # epsilon-贪婪策略 self.target_update = target_update # 目标网络更新频率 self.count = 0 # 计数器,记录更新次数 self.device = device def take_action(self, state): # epsilon-贪婪策略采取动作 if np.random.random() &lt; self.epsilon:# 有0.01的概率随机选择一个动作 action = np.random.randint(self.action_dim) else: state = torch.tensor([state], dtype=torch.float).to(self.device) action = self.q_net(state).argmax().item() #选择一个最大的q值并返回索引，索引就是动作 return action def update(self, transition_dict): states = torch.tensor(transition_dict[&#x27;states&#x27;], dtype=torch.float).to(self.device) actions = torch.tensor(transition_dict[&#x27;actions&#x27;]).view(-1, 1).to( self.device) rewards = torch.tensor(transition_dict[&#x27;rewards&#x27;], dtype=torch.float).view(-1, 1).to(self.device) next_states = torch.tensor(transition_dict[&#x27;next_states&#x27;], dtype=torch.float).to(self.device) dones = torch.tensor(transition_dict[&#x27;dones&#x27;], dtype=torch.float).view(-1, 1).to(self.device) q_values = self.q_net(states).gather(1, actions) # Q值 # 下个状态的最大Q值 max_next_q_values = self.target_q_net(next_states).max(1)[0].view( -1, 1) #下一个状态最大的q值 q_targets = rewards + self.gamma * max_next_q_values * (1 - dones ) # TD误差目标 dqn_loss = torch.mean(F.mse_loss(q_values, q_targets)) # 均方误差损失函数 self.optimizer.zero_grad() # PyTorch中默认梯度会累积,这里需要显式将梯度置为0 dqn_loss.backward() # 反向传播更计算梯度 self.optimizer.step()# 反向传播更新参数 if self.count % self.target_update == 0: self.target_q_net.load_state_dict( self.q_net.state_dict()) # 更新目标网络 self.count += 1lr = 2e-3num_episodes = 500hidden_dim = 128gamma = 0.98 epsilon = 0.01target_update = 10buffer_size = 10000minimal_size = 500batch_size = 256device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device( &quot;cpu&quot;)env_name = &#x27;CartPole-v0&#x27;env = gym.make(env_name)random.seed(0)np.random.seed(0)env.reset(seed=0)torch.manual_seed(0)replay_buffer = ReplayBuffer(buffer_size) #经验存储池state_dim = env.observation_space.shape[0]action_dim = env.action_space.nagent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon, target_update, device)return_list = [] #回报列表for i in range(10): with tqdm(total=int(num_episodes / 10), desc=&#x27;Iteration %d&#x27; % i) as pbar: for i_episode in range(int(num_episodes / 10)): episode_return = 0 state = env.reset(seed=0) state = state[0] done = False while not done: action = agent.take_action(state) next_state, reward, done, truncated, _ = env.step(action) #环境变化，done，游戏是不是结束，truncated，是不是超过200时间步 done=done or truncated #只要有一个结束即结束 replay_buffer.add(state, action, reward, next_state, done) #跟环境互动得到的完整的经验 state = next_state episode_return += reward # 当buffer数据的数量超过一定值后,才进行Q网络训练 if replay_buffer.size() &gt; minimal_size: b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size) transition_dict = &#123; &#x27;states&#x27;: b_s, &#x27;actions&#x27;: b_a, &#x27;next_states&#x27;: b_ns, &#x27;rewards&#x27;: b_r, &#x27;dones&#x27;: b_d &#125; agent.update(transition_dict) return_list.append(episode_return) if (i_episode + 1) % 10 == 0: pbar.set_postfix(&#123; &#x27;episode&#x27;: &#x27;%d&#x27; % (num_episodes / 10 * i + i_episode + 1), &#x27;return&#x27;: &#x27;%.3f&#x27; % np.mean(return_list[-10:]) &#125;) pbar.update(1)episodes_list = list(range(len(return_list)))plt.plot(episodes_list, return_list)plt.xlabel(&#x27;Episodes&#x27;)plt.ylabel(&#x27;Returns&#x27;)plt.title(&#x27;DQN on &#123;&#125;&#x27;.format(env_name))plt.show()mv_return = rl_utils.moving_average(return_list, 9)plt.plot(episodes_list, mv_return)plt.xlabel(&#x27;Episodes&#x27;)plt.ylabel(&#x27;Returns&#x27;)plt.title(&#x27;DQN on &#123;&#125;&#x27;.format(env_name))","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-强化学习","slug":"Code-强化学习","permalink":"http://example.com/tags/Code-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"},{"name":"Code-模型库","slug":"Code-模型库","permalink":"http://example.com/tags/Code-%E6%A8%A1%E5%9E%8B%E5%BA%93/"}]},{"title":"Coding-python使用命令行命令","slug":"coding_python_subprocess","date":"2024-07-23T08:08:21.987Z","updated":"2024-11-29T11:39:40.272Z","comments":true,"path":"2024/07/23/coding_python_subprocess/","link":"","permalink":"http://example.com/2024/07/23/coding_python_subprocess/","excerpt":"","text":"示例,command变量是命令 示例,command变量是命令 1234567891011121314151617181920212223242526272829303132333435363738394041import subprocessimport osimport time# 指定视频数据的源目录video_source_dir = &quot;videos_source&quot;# 输出目录output_dir = &quot;api_out&quot;temp_dir = &#x27;videos_temp&#x27;api_dir = &#x27;videos_apiout&#x27;# 遍历视频数据目录中的所有文件for video_file in os.listdir(video_source_dir): timestamp = int(time.time()) # 检查是否为视频文件（这里假设视频文件扩展名为.mp4） if video_file.endswith(&quot;.mp4&quot;): video_path = os.path.join(video_source_dir, video_file) # 构建要执行的命令 command = [ &quot;python&quot;, &quot;main_yolo_dected_video.py&quot;, &quot;--source&quot;, temp_dir+&#x27;/&#x27;+str(timestamp), &quot;--out_dir&quot;, api_dir+&#x27;/&#x27;+str(timestamp), &quot;--video_path&quot;, video_path ] # 使用subprocess.run()执行命令 result = subprocess.run(command) # 检查命令执行状态 if result.returncode == 0: print(f&quot;Processed &#123;video_file&#125; successfully.&quot;) else: print(f&quot;Failed to process &#123;video_file&#125;. Error code: &#123;result.returncode&#125;&quot;)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-图片转视频","slug":"coding_python_pic2video","date":"2024-07-23T08:05:47.745Z","updated":"2024-11-29T11:39:40.272Z","comments":true,"path":"2024/07/23/coding_python_pic2video/","link":"","permalink":"http://example.com/2024/07/23/coding_python_pic2video/","excerpt":"","text":"将图片转成视频 将图片转成视频 1234567891011121314151617181920212223242526272829303132333435363738394041import cv2import osimport globfrom PIL import Imageimport redef natural_sort_key(s): return [int(text) if text.isdigit() else text.lower() for text in re.split(&#x27;([0-9]+)&#x27;, s)]def images_to_video(image_folder, output_video_path, fps=30): &quot;&quot;&quot; Converts a folder of images into a video. Parameters: image_folder (str): Path to the folder containing the images. output_video_path (str): Path to the output video file. fps (int, optional): Frames per second for the output video. Default is 30. &quot;&quot;&quot; # Get the list of all images in the folder sorted by name image_list = sorted(glob.glob(os.path.join(image_folder, &#x27;*.png&#x27;)),key=natural_sort_key) # Read the first image to get dimensions img = cv2.imread(image_list[0]) height, width, layers = img.shape # Define the codec and create VideoWriter object fourcc = cv2.VideoWriter_fourcc(*&#x27;mp4v&#x27;) # You can use &#x27;XVID&#x27; or other codecs depending on your needs video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height)) # Write each image to the video for image_path in image_list: img = cv2.imread(image_path) video.write(img) # Release everything if job is finished video.release() print(&quot;Video created successfully at: &quot;, output_video_path)# Example usageimage_folder = &#x27;grid&#x27;output_video_path = &#x27;test_save.mp4&#x27;images_to_video(image_folder, output_video_path, fps=30)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-视频转图片","slug":"coding_python_video2pic","date":"2024-07-23T08:03:44.310Z","updated":"2024-11-29T11:39:40.270Z","comments":true,"path":"2024/07/23/coding_python_video2pic/","link":"","permalink":"http://example.com/2024/07/23/coding_python_video2pic/","excerpt":"","text":"按将一个视频切片成图片 按将一个视频切片成图片 123456789101112131415161718192021222324252627282930313233343536import cv2import osdef extract_frames(video_path, output_folder): &quot;&quot;&quot; Extract frames from a video and save them to a specified folder. Parameters: video_path (str): The path to the video file. output_folder (str): The path to the folder where frames will be saved. &quot;&quot;&quot; # Create the output folder if it does not exist if not os.path.exists(output_folder): os.makedirs(output_folder) # Open the video file vidcap = cv2.VideoCapture(video_path) # Check if video file was opened successfully if not vidcap.isOpened(): print(&quot;Error opening video file&quot;) return # Read the video frame by frame success, image = vidcap.read() count = 0 while success: # Save the frame as an image file cv2.imwrite(os.path.join(output_folder, &quot;frame&#123;:d&#125;.jpg&quot;.format(count)), image) success, image = vidcap.read() count += 1 print(f&quot;Extracted &#123;count&#125; frames.&quot;)video_path = &#x27;nn.mp4&#x27;video_outdir = &#x27;video_data4&#x27;extract_frames(video_path,video_outdir)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-使用torch的maskrcnn识别&分割","slug":"coding_python_image_maskrcnn","date":"2024-07-13T10:18:37.202Z","updated":"2024-07-13T10:21:22.388Z","comments":true,"path":"2024/07/13/coding_python_image_maskrcnn/","link":"","permalink":"http://example.com/2024/07/13/coding_python_image_maskrcnn/","excerpt":"","text":"摘要 摘要 使用torch内置的maskrcnn实现图像的识别和分割 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106import torchimport torchvisionfrom torchvision.transforms import functional as Ffrom PIL import Imageimport matplotlib.pyplot as pltimport numpy as np# from torchvision.models.detection import coco_utils# 加载预训练的Mask R-CNN模型model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)device = torch.device(&#x27;cuda&#x27;) if torch.cuda.is_available() else torch.device(&#x27;cpu&#x27;)model.to(device)model.eval()# 图像预处理def preprocess_image(image_path): img = Image.open(image_path).convert(&quot;RGB&quot;) transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor() ]) img_tensor = transform(img) return img_tensor.unsqueeze(0).to(device)# 后处理函数，用于从模型输出中获取预测框、类别和分割掩码def process_output(output): boxes = output[&#x27;boxes&#x27;].detach().cpu().numpy() labels = output[&#x27;labels&#x27;].detach().cpu().numpy() masks = output[&#x27;masks&#x27;].detach().cpu().numpy() scores = output[&#x27;scores&#x27;].detach().cpu().numpy() # 过滤低置信度的预测 idxs = np.where(scores &gt; 0.8)[0] boxes = boxes[idxs] labels = labels[idxs] masks = masks[idxs] return boxes, labels, masks# 加载图像并进行预处理image_path = &#x27;pic.jpg&#x27; # 替换为你的图像路径img_tensor = preprocess_image(image_path)# 模型预测with torch.no_grad(): predictions = model(img_tensor)# 处理预测结果boxes, labels, masks = process_output(predictions[0])# COCO类别列表COCO_CATEGORIES = [ &#x27;__background__&#x27;, &#x27;person&#x27;, &#x27;bicycle&#x27;, &#x27;car&#x27;, &#x27;motorcycle&#x27;, &#x27;airplane&#x27;, &#x27;bus&#x27;, &#x27;train&#x27;, &#x27;truck&#x27;, &#x27;boat&#x27;, &#x27;traffic light&#x27;, &#x27;fire hydrant&#x27;, &#x27;N/A&#x27;, &#x27;stop sign&#x27;, &#x27;parking meter&#x27;, &#x27;bench&#x27;, &#x27;bird&#x27;, &#x27;cat&#x27;, &#x27;dog&#x27;, &#x27;horse&#x27;, &#x27;sheep&#x27;, &#x27;cow&#x27;, &#x27;elephant&#x27;, &#x27;bear&#x27;, &#x27;zebra&#x27;, &#x27;giraffe&#x27;, &#x27;N/A&#x27;, &#x27;backpack&#x27;, &#x27;umbrella&#x27;, &#x27;N/A&#x27;, &#x27;N/A&#x27;, &#x27;handbag&#x27;, &#x27;tie&#x27;, &#x27;suitcase&#x27;, &#x27;frisbee&#x27;, &#x27;skis&#x27;, &#x27;snowboard&#x27;, &#x27;sports ball&#x27;, &#x27;kite&#x27;, &#x27;baseball bat&#x27;, &#x27;baseball glove&#x27;, &#x27;skateboard&#x27;, &#x27;surfboard&#x27;, &#x27;tennis racket&#x27;, &#x27;bottle&#x27;, &#x27;N/A&#x27;, &#x27;wine glass&#x27;, &#x27;cup&#x27;, &#x27;fork&#x27;, &#x27;knife&#x27;, &#x27;spoon&#x27;, &#x27;bowl&#x27;, &#x27;banana&#x27;, &#x27;apple&#x27;, &#x27;sandwich&#x27;, &#x27;orange&#x27;, &#x27;broccoli&#x27;, &#x27;carrot&#x27;, &#x27;hot dog&#x27;, &#x27;pizza&#x27;, &#x27;donut&#x27;, &#x27;cake&#x27;, &#x27;chair&#x27;, &#x27;couch&#x27;, &#x27;potted plant&#x27;, &#x27;bed&#x27;, &#x27;N/A&#x27;, &#x27;dining table&#x27;, &#x27;N/A&#x27;, &#x27;N/A&#x27;, &#x27;toilet&#x27;, &#x27;N/A&#x27;, &#x27;tv&#x27;, &#x27;laptop&#x27;, &#x27;mouse&#x27;, &#x27;remote&#x27;, &#x27;keyboard&#x27;, &#x27;cell phone&#x27;, &#x27;microwave&#x27;, &#x27;oven&#x27;, &#x27;toaster&#x27;, &#x27;sink&#x27;, &#x27;refrigerator&#x27;, &#x27;N/A&#x27;, &#x27;book&#x27;, &#x27;clock&#x27;, &#x27;vase&#x27;, &#x27;scissors&#x27;, &#x27;teddy bear&#x27;, &#x27;hair drier&#x27;, &#x27;toothbrush&#x27;]# 打印标签对应的类别名称for label in labels: class_name = COCO_CATEGORIES[label] print(f&quot;Label &#123;label&#125; represents: &#123;class_name&#125;&quot;)# 可视化结果plt.figure(figsize=(10, 10))# 创建一个新的图像，用于叠加掩码overlay = np.zeros_like(F.to_pil_image(img_tensor[0]).convert(&#x27;RGB&#x27;)).astype(np.uint8)threshold = 0.99 # 设定阈值，根据你的需要调整这个值# 在新图上逐个绘制掩码for i in range(len(masks)): color = np.random.rand(3) * 255 # 生成随机颜色，并转换为0-255范围内的整数 mask = masks[i][0] * 255 # 将掩码转换为0-255范围内的灰度图# 对数据应用阈值处理，将所有大于阈值的像素置为1，其余置为0 binary_mask = (mask &gt; threshold).astype(np.uint8) overlay[binary_mask.astype(bool)] = color # 将掩码应用到叠加图像上plt.imshow(overlay)plt.show()# 使用叠加图像和原始图像创建透明效果original_image = F.to_pil_image(img_tensor[0])combined = Image.blend(original_image, Image.fromarray(overlay.astype(np.uint8)), alpha=0.4)plt.imshow(combined) # 显示带有掩码透明叠加的图像# 再次绘制边界框，以确保它们位于最上层for box in boxes: plt.gca().add_patch(plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], fill=False, edgecolor=&#x27;red&#x27;, linewidth=2)) plt.axis(&#x27;off&#x27;) # 关闭坐标轴以更好地展示图像plt.show()","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-图像处理","slug":"Code-图像处理","permalink":"http://example.com/tags/Code-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"Coding-python-标签-核验txt中类别数并显示","slug":"coding_python_label_valid_class","date":"2024-07-11T07:55:57.560Z","updated":"2024-11-29T11:39:42.000Z","comments":true,"path":"2024/07/11/coding_python_label_valid_class/","link":"","permalink":"http://example.com/2024/07/11/coding_python_label_valid_class/","excerpt":"","text":"代码 12345678910111213141516171819202122232425262728293031323334353637383940414243import osdef validate_labels(label_dir, num_classes): &quot;&quot;&quot; Validates labels in a directory against a given number of classes. It checks if any label exceeds the number of classes defined and collects all unique labels. :param label_dir: Directory path where the label files (.txt) are stored. :param num_classes: Number of classes defined in the dataset. &quot;&quot;&quot; found_labels = set() # Use a set to store unique labels for filename in os.listdir(label_dir): if filename.endswith(&#x27;.txt&#x27;): filepath = os.path.join(label_dir, filename) with open(filepath, &#x27;r&#x27;) as f: for line in f: parts = line.split() if len(parts) &gt; 0: label = int(parts[0]) if label==0: print(filename) found_labels.add(label) # Add label to the set if label &gt;= num_classes: print(f&quot;ERROR: Label &#123;label&#125; in file &#123;filename&#125; exceeds the number of classes (&#123;num_classes&#125;).&quot;) return False print(&quot;Unique labels found:&quot;, sorted(list(found_labels))) # Print sorted list of unique labels if max(found_labels) + 1 == num_classes: print(&quot;All labels are within the defined class range.&quot;) return True else: print(f&quot;Warning: Found labels up to &#123;max(found_labels)&#125;, but &#123;num_classes&#125; classes were expected.&quot;) return max(found_labels) + 1 == num_classes# Example usage:label_directory = &#x27;train/labels&#x27;number_of_classes = 17 # Replace with your actual number of classes# Validate labelsis_valid = validate_labels(label_directory, number_of_classes)if is_valid: print(&quot;Validation passed.&quot;)else: print(&quot;Validation failed.&quot;)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-图像处理-在图片上显示矩形框","slug":"coding_python_image_show_rectangle","date":"2024-07-11T07:51:47.826Z","updated":"2024-07-11T07:54:48.625Z","comments":true,"path":"2024/07/11/coding_python_image_show_rectangle/","link":"","permalink":"http://example.com/2024/07/11/coding_python_image_show_rectangle/","excerpt":"","text":"显示矩形框 显示矩形框 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import cv2def draw_boxes_on_image(image_path, label_path, class_dict, output_path=None): &quot;&quot;&quot; Reads an image and its corresponding YOLO format label file, then draws the boxes on the image. :param image_path: Path to the image file. :param label_path: Path to the YOLO format label file. :param class_dict: Dictionary mapping class names to their ids. :param output_path: Optional path to save the annotated image. :return: Annotated image as numpy array. &quot;&quot;&quot; # Load the image image = cv2.imread(image_path) height, width, _ = image.shape # Read the label file with open(label_path, &#x27;r&#x27;) as f: lines = f.readlines() # Draw boxes for each object in the label file for line in lines: parts = line.strip().split() class_id = int(parts[0]) x_center, y_center, box_width, box_height = map(float, parts[1:]) # Convert normalized coordinates back to pixel values x_min = int((x_center - box_width / 2) * width) y_min = int((y_center - box_height / 2) * height) x_max = int((x_center + box_width / 2) * width) y_max = int((y_center + box_height / 2) * height) # Get the class name class_name = next(key for key, value in class_dict.items() if value == class_id) # Draw the bounding box cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2) # Add the class name on top of the box cv2.putText(image, class_name, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2) # Optionally save the image if output_path is not None: cv2.imwrite(output_path, image) return image# Example usage:class_dict = &#123;&#x27;dog&#x27;: 0, &#x27;cat&#x27;: 1&#125;image_path = &#x27;jpg\\\\1.jpg&#x27;label_path = &#x27;txt\\\\1.txt&#x27;output_path = &#x27;annotated_image.jpg&#x27;annotated_image = draw_boxes_on_image(image_path, label_path, class_dict, output_path)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-图像处理","slug":"Code-图像处理","permalink":"http://example.com/tags/Code-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"Coding-python-标签-labelme_json2txt","slug":"coding_python_label_deepfasion2_json2txt copy","date":"2024-07-11T07:42:05.621Z","updated":"2024-11-29T11:39:40.284Z","comments":true,"path":"2024/07/11/coding_python_label_deepfasion2_json2txt copy/","link":"","permalink":"http://example.com/2024/07/11/coding_python_label_deepfasion2_json2txt%20copy/","excerpt":"","text":"代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import osimport jsondef convert_labelme_to_yolo(json_file, output_folder, class_dict): # Read the JSON file with open(json_file, &#x27;r&#x27;) as f: data = json.load(f) # Get image dimensions img_width = data[&#x27;imageWidth&#x27;] img_height = data[&#x27;imageHeight&#x27;] # Get the image name without extension img_name = os.path.splitext(os.path.basename(json_file))[0] # Create YOLO format label file txt_filename = os.path.join(output_folder, img_name + &#x27;.txt&#x27;) with open(txt_filename, &#x27;w&#x27;) as f: for shape in data[&#x27;shapes&#x27;]: if shape[&#x27;shape_type&#x27;] != &#x27;rectangle&#x27;: continue # Get category label = shape[&#x27;label&#x27;] class_id = class_dict[label] # Get bounding box coordinates points = shape[&#x27;points&#x27;] xmin, ymin = points[0] xmax, ymax = points[1] # Convert to YOLO format required coordinates x_center = (xmin + xmax) / 2.0 / img_width y_center = (ymin + ymax) / 2.0 / img_height width = (xmax - xmin) / img_width height = (ymax - ymin) / img_height # Write to label file line = f&quot;&#123;class_id&#125; &#123;x_center:.6f&#125; &#123;y_center:.6f&#125; &#123;width:.6f&#125; &#123;height:.6f&#125;\\n&quot; f.write(line)def main(): # Define your class dictionary, where keys are class names and values are class IDs # class_dict = &#123;&#x27;Brassiere&#x27;: 14, &#x27;panties&#x27;: 15&#125; class_dict = &#123; &#x27;Short sleeve top&#x27;: 1, &#x27;Long sleeve top&#x27;: 2, &#x27;Short sleeve outwear&#x27;: 3, &#x27;Long sleeve outwear&#x27;: 4, &#x27;Vest&#x27;: 5, &#x27;Sling&#x27;: 6, &#x27;Shorts&#x27;: 7, &#x27;Trousers&#x27;: 8, &#x27;Skirt&#x27;: 9, &#x27;Short sleeve dress&#x27;: 10, &#x27;Long sleeve dress&#x27;: 11, &#x27;Vest dress&#x27;: 12, &#x27;Sling dress&#x27;: 13,&#125; # Specify the folder containing JSON files and the output folder for YOLO labels input_folder = &#x27;train\\\\annos&#x27; output_folder = &#x27;train\\\\lables&#x27; # Iterate over all JSON files in the input folder for filename in os.listdir(input_folder): if filename.endswith(&#x27;.json&#x27;): json_file = os.path.join(input_folder, filename) convert_labelme_to_yolo(json_file, output_folder, class_dict)if __name__ == &quot;__main__&quot;: main()","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-python-删除两个文件夹中文件名不匹配的文件","slug":"coding_python_delete_file_not_exist","date":"2024-07-11T07:39:52.301Z","updated":"2024-11-29T11:39:40.282Z","comments":true,"path":"2024/07/11/coding_python_delete_file_not_exist/","link":"","permalink":"http://example.com/2024/07/11/coding_python_delete_file_not_exist/","excerpt":"","text":"代码 123456789101112131415161718192021222324252627282930import os# 文件夹路径folder_a = &#x27;train\\\\images&#x27;folder_b = &#x27;train\\\\labels&#x27;# 创建两个集合，分别存储去掉扩展名后的文件名names_in_a = set()names_in_b = set()# 读取文件夹a中的文件名for filename in os.listdir(folder_a): base_name = os.path.splitext(filename)[0] names_in_a.add(base_name)# 读取文件夹b中的文件名for filename in os.listdir(folder_b): base_name = os.path.splitext(filename)[0] names_in_b.add(base_name)# 找出只存在于文件夹a中，但在文件夹b中不存在的文件名files_to_delete = names_in_a - names_in_b# 删除文件夹a中没有匹配的文件for base_name in files_to_delete: for filename in os.listdir(folder_a): if os.path.splitext(filename)[0] == base_name: file_path = os.path.join(folder_a, filename) print(f&quot;Deleting: &#123;file_path&#125;&quot;) os.remove(file_path)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-python-标签-deepfasion数据集json2txt","slug":"coding_python_label_labelme_json2txt","date":"2024-07-11T07:35:30.965Z","updated":"2024-11-29T11:39:40.286Z","comments":true,"path":"2024/07/11/coding_python_label_labelme_json2txt/","link":"","permalink":"http://example.com/2024/07/11/coding_python_label_labelme_json2txt/","excerpt":"","text":"代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import osimport jsonfrom PIL import Image, ImageDrawimport globdef create_yolo_txt(json_data, image_width, image_height, txt_file_path): &quot;&quot;&quot;Convert JSON data to YOLO format and save to TXT file.&quot;&quot;&quot; with open(txt_file_path, &#x27;w&#x27;) as f: for item_key in [&#x27;item1&#x27;, &#x27;item2&#x27;]: if item_key in json_data: bbox = json_data[item_key][&#x27;bounding_box&#x27;] cat_id = json_data[item_key][&#x27;category_id&#x27;] # Convert bounding box coordinates to YOLO format x_center = (bbox[0] + bbox[2]) / (2 * image_width) y_center = (bbox[1] + bbox[3]) / (2 * image_height) width = (bbox[2] - bbox[0]) / image_width height = (bbox[3] - bbox[1]) / image_height line = f&quot;&#123;cat_id&#125; &#123;x_center:.6f&#125; &#123;y_center:.6f&#125; &#123;width:.6f&#125; &#123;height:.6f&#125;\\n&quot; f.write(line)def draw_bounding_boxes(json_data, image, output_image_path): &quot;&quot;&quot;Draw bounding boxes on the image and save.&quot;&quot;&quot; draw = ImageDraw.Draw(image) for item_key in [&#x27;item1&#x27;, &#x27;item2&#x27;]: if item_key in json_data: bbox = json_data[item_key][&#x27;bounding_box&#x27;] draw.rectangle(bbox, outline=&#x27;red&#x27;) image.save(output_image_path)def process_images_and_jsons(image_folder, json_folder, txt_output_folder, result_image_folder): &quot;&quot;&quot;Process all images and JSON files.&quot;&quot;&quot; if not os.path.exists(txt_output_folder): os.makedirs(txt_output_folder) if not os.path.exists(result_image_folder): os.makedirs(result_image_folder) for json_file in glob.glob(os.path.join(json_folder, &#x27;*.json&#x27;)): base_name = os.path.splitext(os.path.basename(json_file))[0] image_file = os.path.join(image_folder, f&#x27;&#123;base_name&#125;.jpg&#x27;) if os.path.isfile(image_file): with open(json_file, &#x27;r&#x27;) as f: data = json.load(f) image = Image.open(image_file) image_width, image_height = image.size txt_file_path = os.path.join(txt_output_folder, f&#x27;&#123;base_name&#125;.txt&#x27;) create_yolo_txt(data, image_width, image_height, txt_file_path) output_image_path = os.path.join(result_image_folder, f&#x27;&#123;base_name&#125;_result.jpg&#x27;) draw_bounding_boxes(data, image, output_image_path)# Set your folders hereimage_folder = &#x27;train\\\\train\\\\image&#x27;json_folder = &#x27;train\\\\train\\\\annos&#x27;txt_output_folder = &#x27;train\\\\train\\\\lables&#x27; #结果result_image_folder = &#x27;train\\\\train\\\\result&#x27; #框框后的结果process_images_and_jsons(image_folder, json_folder, txt_output_folder, result_image_folder)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-python-标签-更新标签文件TXT中每行的类别编号","slug":"coding_python_label_fit","date":"2024-07-11T07:32:12.253Z","updated":"2024-11-29T11:39:40.284Z","comments":true,"path":"2024/07/11/coding_python_label_fit/","link":"","permalink":"http://example.com/2024/07/11/coding_python_label_fit/","excerpt":"","text":"代码 123456789101112131415161718192021222324252627282930313233343536373839import osdef update_class_numbers(txt_file_path, class_map): &quot;&quot;&quot; 更新TXT文件中每行的类别编号，根据给定的类别映射字典。 :param txt_file_path: TXT文件的路径。 :param class_map: 字典，键是原始类别编号，值是更新后的类别编号。 &quot;&quot;&quot; updated_lines = [] # 读取TXT文件 with open(txt_file_path, &#x27;r&#x27;) as file: lines = file.readlines() # 更新每行的类别编号 for line in lines: parts = line.split() class_id = int(parts[0]) if class_id in class_map: new_class_id = class_map[class_id] parts[0] = str(new_class_id) updated_lines.append(&#x27; &#x27;.join(parts) + &#x27;\\n&#x27;) # 写回更新后的内容到原文件 with open(txt_file_path, &#x27;w&#x27;) as file: file.writelines(updated_lines)txt_dir = &#x27;datasets/clothes/train/labels&#x27;class_map = &#123;15:16 , 1: 2, 2: 3&#125; # 假设你想把类别0改为1，类别1改为2，类别2改为3# 示例使用path_list = os.listdir(txt_dir)path_list = [os.path.join(txt_dir, path) for path in os.listdir(txt_dir)]# path_list = [os.path.join(txt_dir, path) for path in os.listdir(txt_dir) if path.startswith(&#x27;self_&#x27;)]for path in path_list: update_class_numbers(path, class_map) print(path+&#x27;ok&#x27;)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-python-删除两个文件夹中文件名不互相匹配的文件","slug":"coding_python_delete_file_unpair","date":"2024-07-11T07:29:29.893Z","updated":"2024-11-29T11:39:40.282Z","comments":true,"path":"2024/07/11/coding_python_delete_file_unpair/","link":"","permalink":"http://example.com/2024/07/11/coding_python_delete_file_unpair/","excerpt":"","text":"示例：匹配只有前六位是数字的文件 示例：匹配只有前六位是数字的文件 代码 1234567891011121314151617181920import osimport re# 文件夹路径folder_path = &#x27;datasets/clothes/valid/images&#x27;# 定义文件名的匹配模式pattern = re.compile(r&#x27;^\\d&#123;6&#125;\\.(txt|jpg)$&#x27;)# 遍历文件夹中的所有文件for filename in os.listdir(folder_path): file_path = os.path.join(folder_path, filename) # 检查是否为文件 if os.path.isfile(file_path): # 检查文件名是否符合模式 if not pattern.match(filename): # 如果文件名不符合规则且是.txt或.jpg文件，则删除 if filename.endswith((&#x27;.txt&#x27;, &#x27;.jpg&#x27;)): print(f&quot;Deleting: &#123;file_path&#125;&quot;) os.remove(file_path)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-python-复制当前目录下的所有文件到指定的目标目录","slug":"coding_python_copy_file","date":"2024-07-11T07:27:05.755Z","updated":"2024-11-29T11:39:40.282Z","comments":true,"path":"2024/07/11/coding_python_copy_file/","link":"","permalink":"http://example.com/2024/07/11/coding_python_copy_file/","excerpt":"","text":"代码 1234567891011121314151617181920212223242526272829303132333435363738import osimport shutildef copy_files_to_directory(source_dir=None, target_dir=None): &quot;&quot;&quot; 复制当前目录下的所有文件到指定的目标目录。 :param source_dir: 源目录，默认为当前工作目录。 :param target_dir: 目标目录，如果目录不存在则自动创建。 &quot;&quot;&quot; if source_dir is None: source_dir = os.getcwd() # 当前工作目录 if target_dir is None: raise ValueError(&quot;Target directory must be specified.&quot;) # 确保目标目录存在 if not os.path.exists(target_dir): os.makedirs(target_dir) # 遍历源目录中的所有文件 for filename in os.listdir(source_dir): full_source_path = os.path.join(source_dir, filename) full_target_path = os.path.join(target_dir, filename) # 只复制文件，忽略子目录 if os.path.isfile(full_source_path): shutil.copy2(full_source_path, full_target_path) print(f&quot;Copied &#123;filename&#125; to &#123;target_dir&#125;&quot;)# 使用函数source_directory = &quot;datasets/result/valid/images&quot; target_directory = &quot;datasets/clothes/valid/images&quot; # 目标目录# # 使用函数# source_directory = &quot;train/labels&quot; # target_directory = &quot;datasets/clothes/train/labels&quot; # 目标目录copy_files_to_directory(source_directory, target_directory)print(&quot;final&quot;)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-opencv-开运算-闭运算-膨胀-腐蚀-高斯滤波-中值滤波","slug":"coding_python_image_cv","date":"2024-07-02T06:09:20.048Z","updated":"2024-07-02T06:19:31.219Z","comments":true,"path":"2024/07/02/coding_python_image_cv/","link":"","permalink":"http://example.com/2024/07/02/coding_python_image_cv/","excerpt":"","text":"开运算-闭运算-膨胀-腐蚀-高斯滤波-中值滤波 开运算-闭运算-膨胀-腐蚀-高斯滤波-中值滤波 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import cv2import numpy as npfrom matplotlib import pyplot as pltdef display_images(images, titles): &quot;&quot;&quot;显示多个图像&quot;&quot;&quot; # 调整布局为3行3列，因为共有7张图，最后一行只会有一个图像 rows = 3 cols = 3 for i, (image, title) in enumerate(zip(images, titles)): # 使用np.mod来计算正确的子图位置，确保不超过总格数 plt.subplot(rows, cols, i + 1), plt.imshow(image, cmap=&#x27;gray&#x27;) plt.title(title), plt.xticks([]), plt.yticks([]) plt.tight_layout() # 自动调整子图间距 plt.show()# 图像路径image_path = &#x27;data\\\\archive\\\\jpeg_images\\\\IMAGES\\\\img_0001.jpeg&#x27;# 读取图像并转换为灰度图image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)# 结构元素定义# kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))kernel = np.ones((3, 3), np.uint8) # 腐蚀操作erosion = cv2.erode(image, kernel, iterations=1)# 膨胀操作dilation = cv2.dilate(image, kernel, iterations=1)# 开运算（先腐蚀后膨胀）opening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)# 闭运算（先膨胀后腐蚀）closing = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)# 高斯滤波gaussian_blur = cv2.GaussianBlur(image, (5, 5), 0)# 中值滤波median_blur = cv2.medianBlur(image, 5)# 准备显示的图像和标题images = [image, erosion, dilation, opening, closing, gaussian_blur, median_blur]titles = [&#x27;Original Image&#x27;, &#x27;Erosion&#x27;, &#x27;Dilation&#x27;, &#x27;Opening&#x27;, &#x27;Closing&#x27;, &#x27;Gaussian Blur&#x27;, &#x27;Median Blur&#x27;]# 显示图像display_images(images, titles)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-图像处理","slug":"Code-图像处理","permalink":"http://example.com/tags/Code-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"Coding-python-删除指定文件夹下文件","slug":"coding_python_delete_file","date":"2024-07-02T02:11:46.427Z","updated":"2024-11-29T11:39:40.279Z","comments":true,"path":"2024/07/02/coding_python_delete_file/","link":"","permalink":"http://example.com/2024/07/02/coding_python_delete_file/","excerpt":"","text":"代码 123456789101112131415161718192021222324252627282930import osimport shutildef remove_files_in_folder(folder_path): &quot;&quot;&quot; 删除指定文件夹下的所有文件。 :param folder_path: 文件夹路径 &quot;&quot;&quot; for filename in os.listdir(folder_path): file_path = os.path.join(folder_path, filename) try: if os.path.isfile(file_path): os.unlink(file_path) elif os.path.isdir(file_path): shutil.rmtree(file_path) except Exception as e: print(f&quot;Failed to delete &#123;file_path&#125;. Reason: &#123;e&#125;&quot;)def main(): folders_to_clean = [ &#x27;1&#x27;, &#x27;2&#x27;] for folder in folders_to_clean: print(f&quot;Cleaning folder: &#123;folder&#125;&quot;) remove_files_in_folder(folder) print(f&quot;&#123;folder&#125; has been cleaned.&quot;)if __name__ == &quot;__main__&quot;: main()","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-python-非阻塞输入实现","slug":"coding_python_inputnotstock","date":"2024-06-28T12:56:42.673Z","updated":"2024-11-29T11:39:40.286Z","comments":true,"path":"2024/06/28/coding_python_inputnotstock/","link":"","permalink":"http://example.com/2024/06/28/coding_python_inputnotstock/","excerpt":"","text":"代码 12345678910111213141516171819202122232425262728293031323334353637383940#!/usr/bin/python# 导入必要的库import sysimport ttyimport termiosimport atexittry: # 主循环开始 while True: # 提示用户输入，并允许即时响应按键（无需按回车） print(&quot;请输入 &#x27;1&#x27; 打印1, &#x27;2&#x27; 打印2, 或输入 &#x27;q&#x27; 退出程序：&quot;) # 获取标准输入设备的文件描述符 fd = sys.stdin.fileno() # 保存当前的终端设置，以便稍后恢复 old_settings = termios.tcgetattr(fd) try: # 改变输入模式，使得可以立即读取按键 tty.setraw(sys.stdin.fileno()) # 读取用户输入的第一个字符 ch = sys.stdin.read(1) finally: # 恢复终端的原始设置 termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) # 根据用户输入执行相应操作 if ch == &#x27;1&#x27;: print(&quot;1&quot;) elif ch == &#x27;2&#x27;: print(&quot;2&quot;) elif ch.lower() == &#x27;q&#x27;: # 兼容大小写输入的退出指令 print(&quot;程序即将退出...&quot;) break # 退出循环 else: print(&quot;无效的输入，请输入 &#x27;1&#x27;, &#x27;2&#x27;, 或 &#x27;q&#x27;。&quot;)except KeyboardInterrupt: # 捕获Ctrl+C中断 print(&quot;\\n程序因键盘中断而退出。&quot;)finally: # 确保在任何情况下电机都能安全关闭 print(&quot;退出&quot;)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-python-atexit包-应用于程序结束时","slug":"coding_python_atexit","date":"2024-06-28T12:54:26.815Z","updated":"2024-11-29T11:39:40.279Z","comments":true,"path":"2024/06/28/coding_python_atexit/","link":"","permalink":"http://example.com/2024/06/28/coding_python_atexit/","excerpt":"","text":"代码 1234567891011121314151617181920import atexitimport time# 定义一个将在程序退出时被调用的函数def cleanup(): print(&quot;程序已退出，正在进行清理工作...&quot;) # 这里可以添加更多的清理代码，例如关闭文件、数据库连接等# 使用atexit模块注册清理函数atexit.register(cleanup)# 程序的主要逻辑print(&quot;程序开始运行...&quot;)# 模拟一些操作for i in range(5): print(f&quot;正在处理任务 &#123;i+1&#125;...&quot;) time.sleep(1) # 模拟耗时操作，这里引入time模块来模拟延时print(&quot;所有任务处理完毕。&quot;)# 注意：这里没有手动调用cleanup()，它会在脚本自然结束时由atexit模块自动调用","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-图像下采样","slug":"coding_python_image_downsample","date":"2024-06-28T12:49:52.833Z","updated":"2024-06-28T12:53:17.532Z","comments":true,"path":"2024/06/28/coding_python_image_downsample/","link":"","permalink":"http://example.com/2024/06/28/coding_python_image_downsample/","excerpt":"","text":"根据缩放因子下采样图像,处理文件夹 根据缩放因子下采样图像,处理文件夹及子文件夹 根据缩放因子下采样图像,处理文件夹 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748from PIL import Imageimport osdef resize_images(input_folder, output_folder, scale_factor=0.6): &quot;&quot;&quot; 将指定文件夹内的所有图片分辨率降低一倍并保存到另一个文件夹。 参数: input_folder (str): 包含图片的输入文件夹路径。 output_folder (str): 用于保存缩小后图片的输出文件夹路径。 scale_factor (float): 图片尺寸缩小的比例，默认为0.5（即分辨率降低一半）。 &quot;&quot;&quot; # 确保输出文件夹存在 os.makedirs(output_folder, exist_ok=True) # 遍历输入文件夹中的文件 for filename in os.listdir(input_folder): input_path = os.path.join(input_folder, filename) # 检查是否为图片文件 if os.path.isfile(input_path) and (filename.lower().endswith(&#x27;.jpg&#x27;) or filename.lower().endswith(&#x27;.jpeg&#x27;) or filename.lower().endswith(&#x27;.png&#x27;)): try: # 打开图片 with Image.open(input_path) as img: # 计算新的宽度和高度 width, height = img.size new_width = int(width * scale_factor) new_height = int(height * scale_factor) # 调整图片大小 resized_img = img.resize((new_width, new_height), Image.ANTIALIAS) # 构建输出路径并保存图片 output_path = os.path.join(output_folder, filename) resized_img.save(output_path) print(f&quot;Resized and saved &#123;filename&#125; to &#123;output_folder&#125;&quot;) except IOError as e: print(f&quot;Error processing &#123;filename&#125;: &#123;e&#125;&quot;)# 指定输入和输出文件夹路径input_folder = &#x27;data&#x27;output_folder = &#x27;data2&#x27;# 调用函数resize_images(input_folder, output_folder) 根据缩放因子下采样图像,处理文件夹及子文件夹 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from PIL import Imageimport osdef downsample_images(source_folder, target_folder, scale_factor=0.6): &quot;&quot;&quot; 遍历source_folder中的所有子文件夹和mask图片，将图片下采样后保存到target_folder中， 保持原有的目录结构。 参数: source_folder (str): 包含子文件夹和mask图片的源文件夹路径。 target_folder (str): 用于保存下采样后图片的目标文件夹路径。 scale_factor (float): 图片尺寸缩小的比例，默认为0.5。 &quot;&quot;&quot; # 确保目标文件夹存在 os.makedirs(target_folder, exist_ok=True) # 遍历源文件夹及其子目录 for root, dirs, files in os.walk(source_folder): # 计算当前目录相对于源目录的相对路径 relative_path = os.path.relpath(root, source_folder) target_subdir = os.path.join(target_folder, relative_path) # 确保目标子目录存在 os.makedirs(target_subdir, exist_ok=True) for filename in files: # 只处理图片文件 if filename.lower().endswith((&#x27;.jpg&#x27;, &#x27;.jpeg&#x27;, &#x27;.png&#x27;)): src_path = os.path.join(root, filename) dst_path = os.path.join(target_subdir, filename) try: # 打开图片并进行下采样 with Image.open(src_path) as img: width, height = img.size new_width = int(width * scale_factor) new_height = int(height * scale_factor) resized_img = img.resize((new_width, new_height), Image.ANTIALIAS) # 保存下采样后的图片 resized_img.save(dst_path) except IOError as e: print(f&quot;Error processing &#123;src_path&#125;: &#123;e&#125;&quot;)# 指定源文件夹和目标文件夹路径source_folder = &#x27;r1&#x27;target_folder = &#x27;r2&#x27;# 调用函数downsample_images(source_folder, target_folder)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-图像处理","slug":"Code-图像处理","permalink":"http://example.com/tags/Code-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"Coding-重命名文件夹下图像（数字方式and指定开头方式）","slug":"coding_python_image_rename","date":"2024-06-28T12:46:54.104Z","updated":"2024-11-29T11:39:40.258Z","comments":true,"path":"2024/06/28/coding_python_image_rename/","link":"","permalink":"http://example.com/2024/06/28/coding_python_image_rename/","excerpt":"","text":"按顺序重命名pic文件夹下的图像 以指定开头命名 按顺序重命名pic文件夹下的图像 1234567891011121314151617181920212223242526import osimport shutildef rename_files_in_folder(folder_path, extension=&quot;.jpg&quot;):#可改成别的后缀 &quot;&quot;&quot; 重命名指定文件夹内的所有指定扩展名的文件，按照序列1, 2, 3...进行命名。 &quot;&quot;&quot; # 获取文件夹内所有指定扩展名的文件列表 files = [f for f in os.listdir(folder_path) if f.endswith(extension)] files.sort() # 确保按文件名排序，如果需要的话 # 在同一文件夹内重命名文件 for index, filename in enumerate(files, start=1): old_path = os.path.join(folder_path, filename) new_filename = f&quot;&#123;index&#125;&#123;extension&#125;&quot; new_path = os.path.join(folder_path, new_filename) # 重命名文件 shutil.move(old_path, new_path) print(f&quot;Renamed &#x27;&#123;filename&#125;&#x27; to &#x27;&#123;new_filename&#125;&#x27;&quot;)# 指定文件夹路径pic_folder = &quot;pic&quot;# 调用函数rename_files_in_folder(pic_folder) 以指定开头命名 123456789101112131415161718192021222324252627282930313233import osdef rename_files_in_directory(directory, prefix=&quot;self_&quot;): &quot;&quot;&quot; Renames all files in the given directory to have the specified prefix. :param directory: The directory path containing the files to rename. :param prefix: The prefix to add before the original filename. &quot;&quot;&quot; try: # Walk through all the files in the directory for filename in os.listdir(directory): # Construct the full file path file_path = os.path.join(directory, filename) # Check if it&#x27;s a file (not a directory or link) if os.path.isfile(file_path): # Construct new filename with prefix new_filename = prefix + filename # Construct the new file path new_file_path = os.path.join(directory, new_filename) # Rename the file os.rename(file_path, new_file_path) print(f&quot;Renamed &#x27;&#123;filename&#125;&#x27; to &#x27;&#123;new_filename&#125;&#x27;&quot;) except Exception as e: print(f&quot;An error occurred: &#123;e&#125;&quot;)# Example usage:directory_to_rename = &quot;labels&quot;rename_files_in_directory(directory_to_rename)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-opencv-人脸识别","slug":"coding_face_recongnition","date":"2024-06-12T15:38:35.041Z","updated":"2024-06-12T15:53:31.618Z","comments":true,"path":"2024/06/12/coding_face_recongnition/","link":"","permalink":"http://example.com/2024/06/12/coding_face_recongnition/","excerpt":"","text":"摘要 程序一：人脸识别与比较 程序二：扩大矩形框范围函数封装 摘要 基于OpenCV与face_recognition的人脸识别。（怎么会有这么方便的人脸识别库） 程序一：人脸识别与比较 步骤： 加载与转换图像： 使用face_recognition.load_image_file加载图片img/11.jpg和img/22.jpg，并通过cv2.cvtColor将其从BGR格式转换为RGB。 人脸定位与特征编码： 利用face_recognition.face_locations定位图片中的人脸位置，并使用face_recognition.face_encodings提取这些区域的特征编码。 绘制人脸矩形框： 在原图上为检测到的人脸绘制矩形框，使用cv2.rectangle函数实现。 相似度比较与显示： 通过face_recognition.compare_faces比较两个人脸编码的相似度，并利用face_recognition.face_distance计算欧氏距离，展示比较结果及距离值于图像上。 1234567891011121314151617181920212223242526272829303132333435363738394041424344import cv2import numpy as npimport face_recognitionimgElon = face_recognition.load_image_file(&#x27;img/11.jpg&#x27;) # 加载图片imgElon = cv2.cvtColor(imgElon, cv2.COLOR_BGR2RGB) # 将BGR彩色图像转化为RGB彩色图像faceLoc = face_recognition.face_locations(imgElon)[0] # 定位人脸位置encodeElon = face_recognition.face_encodings(imgElon)[0] # 提取人脸的面部特征cv2.rectangle(imgElon, (faceLoc[3], faceLoc[0]), (faceLoc[1], faceLoc[2]), (255, 0, 255), 2) # 框出人脸imgTest = face_recognition.load_image_file(&#x27;img/22.jpg&#x27;)imgTest = cv2.cvtColor(imgTest, cv2.COLOR_BGR2RGB)# 原始的人脸位置faceLocTest = face_recognition.face_locations(imgTest)[0]# 计算矩形框的宽度和高度width = faceLocTest[1] - faceLocTest[3]height = faceLocTest[2] - faceLocTest[0]# 确保扩展的大小不会让矩形超出图像边界expand_width = min(width, imgTest.shape[1] - faceLocTest[1])expand_height = min(height, imgTest.shape[0] - faceLocTest[2])# 调整矩形框的坐标以扩大边界，同时确保不超出图像边界top_left = (max(0, faceLocTest[3] - expand_width), max(0, faceLocTest[0] - expand_height))bottom_right = (min(imgTest.shape[1], faceLocTest[1] + expand_width), min(imgTest.shape[0], faceLocTest[2] + expand_height))# 绘制调整后的矩形框encodeTest = face_recognition.face_encodings(imgTest)[0]cv2.rectangle(imgTest, top_left, bottom_right, (255, 0, 255), 2)result = face_recognition.compare_faces([encodeElon], encodeTest) # 比较人脸编码的相似度faceDis = face_recognition.face_distance([encodeElon], encodeTest) # 计算两个人脸的欧氏距离（欧氏距离用于计算样本之间的相似度或距离）print(result, faceDis)cv2.putText(imgTest, f&#x27;&#123;result&#125;&#123;round(faceDis[0], 2)&#125;&#x27;, (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 2) # 显示比对结果 cv2.imshow(&#x27;Elon Musk&#x27;, imgElon)cv2.imshow(&#x27;Elon Test&#x27;, imgTest)key = cv2.waitKey(0)if key == 27: # 按ESC键退出 cv2.destroyAllWindows() 程序二：扩大矩形框范围函数封装 expand_and_draw_face_box函数： 扩展矩形框： 对给定的测试图像，该函数不仅定位人脸，还智能地扩大检测框，保证扩展后不会超出图像边界。 自定义逻辑： 实现了更精细的控制，通过计算安全的扩展尺寸，避免矩形框越界问题。 结果输出： 函数直接在图像上绘制了调整后的矩形框，并可通过打印返回的矩形顶点坐标来验证效果。 1234567891011121314151617181920212223242526272829303132333435363738394041import cv2import numpy as npimport face_recognitiondef expand_and_draw_face_box(imgTest): try: # 检测人脸位置 faceLocTest = face_recognition.face_locations(imgTest)[0] # 计算矩形框的宽度和高度 width = faceLocTest[1] - faceLocTest[3] height = faceLocTest[2] - faceLocTest[0] # 确保扩展的大小不会超出图像边界 expand_width = min(width, imgTest.shape[1] - faceLocTest[1]) expand_height = min(height, imgTest.shape[0] - faceLocTest[2]) # 绘制调整后的矩形框 top_left = (max(0, faceLocTest[3] - expand_width), max(0, faceLocTest[0] - expand_height)) bottom_right = (min(imgTest.shape[1], faceLocTest[1] + expand_width), min(imgTest.shape[0], faceLocTest[2] + expand_height)) # 返回修改后的图像 cv2.rectangle(imgTest, top_left, bottom_right, (255, 0, 255), 2) return (top_left, bottom_right) except IndexError: print(&quot;No face was detected in the image.&quot;) return None # 加载图像并转换为RGB格式imgTest = face_recognition.load_image_file(&#x27;img/11.jpg&#x27;)imgTest = cv2.cvtColor(imgTest, cv2.COLOR_BGR2RGB)a,b = expand_and_draw_face_box(imgTest)print(a,b)cv2.imshow(&#x27;Image with Expanded Face Box&#x27;, imgTest)cv2.waitKey(0)cv2.destroyAllWindows()","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-图像处理","slug":"Code-图像处理","permalink":"http://example.com/tags/Code-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"MyNote","slug":"MyNote","date":"2024-06-09T08:27:36.687Z","updated":"2025-07-11T03:25:47.540Z","comments":true,"path":"2024/06/09/MyNote/","link":"","permalink":"http://example.com/2024/06/09/MyNote/","excerpt":"","text":"MyNote v2.0 一个轻量级的笔记软件📔 Github项目地址: https://github.com/chandlerye/MyNote/tree/main 应用简介 MyNote v2.0 是一款个人笔记管理软件，没有复杂的功能，旨在提供便捷的笔记记录、管理以及云同步功能。基于Qt 6.6.3 开发，本软件支持本地模式（SQLite数据库）和云模式（如MySQL），既可以本地使用，也可以联网同步使用。(联网需提供自己的云数据库) 下载 下载链接: https://github.com/chandlerye/MyNote/releases 使用说明 数据库配置 本地模式：默认配置，数据存储在本地SQLite数据库，无需特殊设置。 云模式配置：支持MySQL等远程数据库，需输入服务器IP、端口、数据库名、用户名和密码进行配置。 笔记管理 新增笔记：左下方提供快捷按钮。 编辑笔记：双击列表项标题进入编辑状态，右侧文本框实时编辑内容。 笔记排序：支持置顶操作，调整显示顺序。 导出功能：笔记可导出为txt格式。 字体调整：在配置菜单中自定义文本编辑器的字体大小。 注意事项 请确保正确配置云数据库信息，避免连接错误。 在进行模式切换前，建议备份重要数据。 使用过程中遇到任何问题，可通过提供的联系方式寻求帮助。 捐赠 支持一下吧❤️","categories":[],"tags":[]},{"title":"Coding-QT-编程知识点","slug":"coding_qt_learn","date":"2024-06-09T06:27:47.520Z","updated":"2024-06-13T02:37:22.801Z","comments":true,"path":"2024/06/09/coding_qt_learn/","link":"","permalink":"http://example.com/2024/06/09/coding_qt_learn/","excerpt":"","text":"数据库相关 QLite数据库 MySQL数据库 打开与检查数据库连接 SQL语句执行与查询 心跳检查 事件相关 概念 重写eventFilter方法 数据库相关 QLite数据库 123456789db = QSqlDatabase::addDatabase(&quot;QSQLITE&quot;); //添加数据库驱动// 获取当前程序的工作目录QString currentDir = QDir::currentPath();// 拼接数据库文件名，例如 &quot;mydatabase.db&quot;QString dbName = &quot;mydatabase.db&quot;;// 构建完整的数据库文件路径QString dbFilePath = currentDir + &quot;/&quot; + dbName;db.setDatabaseName(dbFilePath); MySQL数据库 123456db = QSqlDatabase::addDatabase(&quot;QMYSQL&quot;);db.setDatabaseName(&quot;mydatabase&quot;);db.setHostName(&quot;localhost&quot;);db.setPort(3306);db.setUserName(&quot;username&quot;);db.setPassword(&quot;password&quot;); 打开与检查数据库连接 12345if (!db.open()) &#123; qDebug() &lt;&lt; &quot;数据库连接失败&quot;;&#125; else &#123; qDebug() &lt;&lt; &quot;数据库连接成功&quot;;&#125; SQL语句执行与查询 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152 // 查询特定id的笔记内容QSqlQuery query;query.prepare(&quot;SELECT note FROM notes WHERE id = :id&quot;);query.bindValue(&quot;:id&quot;, selectedId); // 绑定查询中的id占位符// 创建notes表，如果不存在则创建query.prepare(&quot;CREATE TABLE IF NOT EXISTS notes(&quot; &quot;date TIMESTAMP DEFAULT CURRENT_TIMESTAMP, &quot; &quot;title VARCHAR(255) NOT NULL, &quot; &quot;note TEXT, &quot; &quot;id INTEGER PRIMARY KEY AUTOINCREMENT, &quot; // 增加主键自动增长 &quot;sort_order INTEGER)&quot;);if (query.next()) &#123; // 确保查询有结果再取值 int idOrder = query.value(0).toInt(); // 获取id QString title = query.value(1).toString(); // 获取title QString note = query.value(2).toString(); // 获取note int sortOrder = query.value(3).toInt(); // 获取sort_order序号&#125;// 查询所有笔记并按sort_order降序排列QSqlQuery query2;query2.prepare(&quot;SELECT id, title, note, sort_order FROM notes ORDER BY sort_order DESC&quot;);// 更新笔记内容QSqlQuery updateQuery;updateQuery.prepare(&quot;UPDATE notes SET note = :newContent WHERE id = :id&quot;);updateQuery.bindValue(&quot;:newContent&quot;, newText); // 绑定新的文本内容updateQuery.bindValue(&quot;:id&quot;, currentId); // 基于当前选中标题更新对应的笔记内容// 查询当前最大id（修正为查询最大sort_order）QSqlQuery maxSortOrderQuery;maxSortOrderQuery.prepare(&quot;SELECT MAX(sort_order) FROM notes&quot;);// 插入新笔记记录QSqlQuery insertQuery;insertQuery.prepare(&quot;INSERT INTO notes (title, note, sort_order,id) VALUES (:title, :note, :sortOrder,:id)&quot;);insertQuery.bindValue(&quot;:title&quot;, title);insertQuery.bindValue(&quot;:note&quot;, &quot;&quot;); // 初始化笔记内容为空字符串insertQuery.bindValue(&quot;:sortOrder&quot;, newSortOrder); // 使用新排序序号insertQuery.bindValue(&quot;:id&quot;, newIdOrder); // 如果id是自增的，这行可以省略或注释掉// 删除指定id的笔记记录QSqlQuery deleteQuery;deleteQuery.prepare(&quot;DELETE FROM notes WHERE id = :id&quot;);deleteQuery.bindValue(&quot;:id&quot;, idToDelete);// 统计记录数量QSqlQuery checkEmptyQuery;checkEmptyQuery.exec(&quot;SELECT COUNT(*) FROM notes&quot;);checkEmptyQuery.next();int rowCount = checkEmptyQuery.value(0).toInt(); 心跳检查 12345678910111213141516171819void MainWindow::ask_check()&#123; // 使用成员变量或局部静态变量来存储定时器，确保其生命周期足够长 QTimer* heartbeatTimer = new QTimer(this); // 注意使用this指针作为父对象，以便于管理定时器的生命周期 connect(heartbeatTimer, &amp;QTimer::timeout, this, [this]() &#123; QSqlQuery query(db); if (query.exec(&quot;SELECT 1&quot;)) &#123; if (query.next()) &#123; ui-&gt;info_label-&gt;setStyleSheet(&quot;color: blue;&quot;); qDebug() &lt;&lt; &quot;心跳检测 - 数据库连接正常&quot;; &#125; &#125; else &#123; ui-&gt;info_label-&gt;setStyleSheet(&quot;color: red;&quot;); qDebug() &lt;&lt; &quot;心跳检测失败：&quot; &lt;&lt; query.lastError().text(); &#125; &#125;); heartbeatTimer-&gt;start(60000); // 每60秒执行一次心跳检测&#125; 事件相关 概念 通过重写eventFilter方法，自定义对特定对象和事件类型的响应逻辑。例子：处理右键菜单和文本编辑器焦点变化，在合适的位置注册事件过滤器。通常在构造函数或初始化方法中完成此操作。 12ui-&gt;listWidget-&gt;installEventFilter(this);ui-&gt;textEdit-&gt;installEventFilter(this); 重写eventFilter方法 1234567891011121314151617181920212223242526272829303132333435363738bool MainWindow::eventFilter(QObject *watched, QEvent *event)&#123; // 右键菜单处理 if (watched == ui-&gt;listWidget &amp;&amp; event-&gt;type() == QEvent::ContextMenu) &#123; QContextMenuEvent *contextEvent = static_cast&lt;QContextMenuEvent*&gt;(event); // 获取鼠标点击位置并转换为列表项索引 QModelIndex index = ui-&gt;listWidget-&gt;indexAt(contextEvent-&gt;pos()); // 如果索引有效，则弹出菜单 if (index.isValid()) &#123; rightClickMenu-&gt;exec(contextEvent-&gt;globalPos()); return true; // 消耗事件 &#125; &#125; // 文本编辑器焦点变化处理 if (watched == ui-&gt;textEdit) &#123; if (event-&gt;type() == QEvent::FocusIn) &#123; isTextEditFocused = true; // 当textEdit获得焦点时，建立textChanged的信号槽连接 connect(ui-&gt;textEdit, &amp;QTextEdit::textChanged, this, &amp;MainWindow::onTextEditContentChanged); &#125; else if (event-&gt;type() == QEvent::FocusOut) &#123; isTextEditFocused = false; // 当textEdit失去焦点时，断开textChanged的信号槽连接 disconnect(ui-&gt;textEdit, &amp;QTextEdit::textChanged, this, &amp;MainWindow::onTextEditContentChanged); &#125; else if (event-&gt;type() &amp; Qt::ControlModifier) &#123; &#125; &#125; return QMainWindow::eventFilter(watched, event);&#125;","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Qt","slug":"Qt","permalink":"http://example.com/tags/Qt/"}]},{"title":"Coding-可视化-tensorboard的使用","slug":"coding_use_tensorboard","date":"2024-05-25T04:12:26.731Z","updated":"2024-11-29T11:39:40.286Z","comments":true,"path":"2024/05/25/coding_use_tensorboard/","link":"","permalink":"http://example.com/2024/05/25/coding_use_tensorboard/","excerpt":"","text":"摘要 摘要 代码段配置了在TensorBoard中展示的生成图像数量，并通过循环选取指定数量的图像，利用make_grid函数将这些图像整理成网格形式，做好归一化处理以优化显示效果。随后，使用tb_writer.add_image功能将整理好的图像网格添加至TensorBoard的日志中，标以’Generated Images’，并借助global_step参数记录了训练进程中的具体步数，以便于观察模型生成图像随训练进展的变化情况。 代码可以写在for循环最后。 1234567891011121314151617181920import torch.utils.tensorboard as tbfrom torchvision.utils import save_image,make_grid# 设置要在TensorBoard中展示的图像数量num_images_to_log = 4 # 你希望在TensorBoard可视化界面中展示的生成图像的数量# 遍历生成的图像，注意限制范围以不超过用户定义的数量或当前批次生成的图像总数for i in range(min(num_images_to_log, gen_imgs.size(0))): # 确保索引不会超出生成图像列表的边界 # 使用make_grid函数将一批图像排列成网格以便展示，这里取连续的5张图像（i:i+5） # nrow参数指定了网格的列数，这里设置为5列，因此会形成一个正方形的网格布局 # normalize参数设为True意味着将图像像素值归一化到[0,1]区间，这对于展示更有利，避免像素值溢出 # scale_each为True则会对每张图像单独进行缩放，确保对比度一致，即使图像间亮度有差异 img_grid = make_grid(gen_imgs[i:i+5], nrow=5, normalize=True, scale_each=True) # 假设gen_imgs中的图像数据已经被适当处理（如归一化），则normalize和scale_each的设定可根据实际情况调整或省略 # 将生成的图像网格添加到TensorBoard中，&#x27;Generated Images&#x27;是该图像系列在TensorBoard中的日志标签 # global_step参数用于标记这是训练过程中的哪一步（比如迭代次数），有助于跟踪随时间的变化 # 这行代码在每个epoch训练结束时执行（由外部循环控制batches_done计数），将最新的生成图像更新到TensorBoard tb_writer.add_image(&#x27;Generated Images&#x27;, img_grid, global_step=batches_done)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-重新设置图像尺寸-裁剪","slug":"coding_crop_image","date":"2024-05-24T06:39:59.576Z","updated":"2024-05-24T06:41:13.871Z","comments":true,"path":"2024/05/24/coding_crop_image/","link":"","permalink":"http://example.com/2024/05/24/coding_crop_image/","excerpt":"","text":"摘要 摘要 将图片裁剪成预定尺寸，如果图片小于预定尺寸则会用黑底填充 123456789101112131415161718192021222324252627282930313233343536373839404142from PIL import Imageimport ossource_folder = &#x27;data&#x27;output_folder = &#x27;output1024&#x27;target_resolution = (1024, 1024)if not os.path.exists(output_folder): os.makedirs(output_folder)for filename in os.listdir(source_folder): if filename.endswith(&#x27;.jpg&#x27;) or filename.endswith(&#x27;.jpeg&#x27;): img_path = os.path.join(source_folder, filename) try: with Image.open(img_path) as img: # 获取当前图片的尺寸 current_size = img.size # 计算裁剪区域以保持纵横比并确保裁剪出的目标尺寸 width_ratio = current_size[0] / target_resolution[0] height_ratio = current_size[1] / target_resolution[1] crop_ratio = max(width_ratio, height_ratio) cropped_size = (int(current_size[0] / crop_ratio), int(current_size[1] / crop_ratio)) # 调整图片大小以便裁剪（这步可选，取决于是否需要先按调整大小后的图片中心进行裁剪） # img = img.resize(cropped_size, resample=Image.LANCZOS) # 计算裁剪的起始坐标以中心对齐 left = (current_size[0] - cropped_size[0]) // 2 top = (current_size[1] - cropped_size[1]) // 2 # 直接进行中心裁剪到目标尺寸 img = img.crop((left, top, left + target_resolution[0], top + target_resolution[1])) # 保存处理后的图片到目标文件夹 output_path = os.path.join(output_folder, filename) img.save(output_path, quality=95) except IOError as e: print(f&quot;Error processing &#123;img_path&#125;: &#123;e&#125;&quot;)print(&quot;Images have been processed and saved to&quot;, output_folder)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-图像处理","slug":"Code-图像处理","permalink":"http://example.com/tags/Code-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"Coding-重新设置图像尺寸-非裁剪","slug":"coding_python_image_resize","date":"2024-05-22T12:10:04.536Z","updated":"2024-05-24T06:39:57.457Z","comments":true,"path":"2024/05/22/coding_python_image_resize/","link":"","permalink":"http://example.com/2024/05/22/coding_python_image_resize/","excerpt":"","text":"摘要 摘要 将图片设置成预定尺寸，如果图片尺寸与预期不符则会裁剪，如果图片小于预定尺寸则会用黑底填充 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152from PIL import Imageimport os# 指定源文件夹和目标文件夹source_folder = &#x27;data&#x27;output_folder = &#x27;output28&#x27;# target_resolution = (1440, 1920)target_resolution = (28, 28)# 确保目标文件夹存在if not os.path.exists(output_folder): os.makedirs(output_folder)# 遍历源文件夹中的图片for filename in os.listdir(source_folder): if filename.endswith(&#x27;.jpg&#x27;) or filename.endswith(&#x27;.jpeg&#x27;): img_path = os.path.join(source_folder, filename) try: with Image.open(img_path) as img: # 获取当前图片的尺寸 current_size = img.size # 如果图片尺寸大于或等于目标尺寸，进行中心裁剪 if current_size[0] &gt;= target_resolution[0] and current_size[1] &gt;= target_resolution[1]: width_ratio = target_resolution[0] / current_size[0] height_ratio = target_resolution[1] / current_size[1] crop_ratio = min(width_ratio, height_ratio) new_size = (int(current_size[0] * crop_ratio), int(current_size[1] * crop_ratio)) img = img.resize(new_size, resample=Image.LANCZOS) left = (new_size[0] - target_resolution[0]) // 2 top = (new_size[1] - target_resolution[1]) // 2 right = (new_size[0] + target_resolution[0]) // 2 bottom = (new_size[1] + target_resolution[1]) // 2 img = img.crop((left, top, right, bottom)) # 如果图片尺寸小于目标尺寸，进行黑色填充 else: # 创建一个目标尺寸的黑色背景图像(想要白色就是255,255,255) new_img = Image.new(&#x27;RGB&#x27;, target_resolution, (0, 0, 0)) # 明确指定填充颜色为黑色 # 计算粘贴位置以居中 paste_position = ((target_resolution[0] - current_size[0]) // 2, (target_resolution[1] - current_size[1]) // 2) new_img.paste(img, paste_position) img = new_img # 保存处理后的图片到目标文件夹 output_path = os.path.join(output_folder, filename) img.save(output_path, quality=95) except IOError as e: print(f&quot;Error processing &#123;img_path&#125;: &#123;e&#125;&quot;)print(&quot;Images have been processed and saved to&quot;, output_folder)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-图像处理","slug":"Code-图像处理","permalink":"http://example.com/tags/Code-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"Coding-SRGAN-超分辨生成对抗网络-基于RGB图片数据","slug":"coding_hight_resolutions_GAN","date":"2024-05-21T12:01:12.975Z","updated":"2024-05-21T12:10:00.541Z","comments":true,"path":"2024/05/21/coding_hight_resolutions_GAN/","link":"","permalink":"http://example.com/2024/05/21/coding_hight_resolutions_GAN/","excerpt":"","text":"摘要 srgan.py inference.py 摘要 srgan.py用于训练网络 inference.py用于使用训练好的模型文件推理 图片数据放在/home/myself/work/work-generate/data下,所有图片在一个文件夹里 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164import argparseimport osimport numpy as npimport mathimport itertoolsimport sysimport torchvision.transforms as transformsfrom torchvision.utils import save_image, make_gridfrom torch.utils.data import DataLoaderfrom torch.autograd import Variablefrom models import *from datasets import *import torch.nn as nnimport torch.nn.functional as Fimport torchos.makedirs(&quot;/home/myself/work/work-generate/srgan/images&quot;, exist_ok=True)os.makedirs(&quot;/home/myself/work/work-generate/srgan/saved_models&quot;, exist_ok=True)parser = argparse.ArgumentParser()parser.add_argument(&quot;--epoch&quot;, type=int, default=0, help=&quot;epoch to start training from&quot;)parser.add_argument(&quot;--n_epochs&quot;, type=int, default=200, help=&quot;number of epochs of training&quot;)parser.add_argument(&quot;--dataset_name&quot;, type=str, default=&quot;data&quot;, help=&quot;name of the dataset&quot;)parser.add_argument(&quot;--batch_size&quot;, type=int, default=4, help=&quot;size of the batches&quot;)parser.add_argument(&quot;--lr&quot;, type=float, default=0.0002, help=&quot;adam: learning rate&quot;)parser.add_argument(&quot;--b1&quot;, type=float, default=0.5, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--b2&quot;, type=float, default=0.999, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--decay_epoch&quot;, type=int, default=100, help=&quot;epoch from which to start lr decay&quot;)parser.add_argument(&quot;--n_cpu&quot;, type=int, default=8, help=&quot;number of cpu threads to use during batch generation&quot;)parser.add_argument(&quot;--hr_height&quot;, type=int, default=512, help=&quot;high res. image height&quot;)parser.add_argument(&quot;--hr_width&quot;, type=int, default=512, help=&quot;high res. image width&quot;)parser.add_argument(&quot;--channels&quot;, type=int, default=3, help=&quot;number of image channels&quot;)parser.add_argument(&quot;--sample_interval&quot;, type=int, default=100, help=&quot;interval between saving image samples&quot;)parser.add_argument(&quot;--checkpoint_interval&quot;, type=int, default=20, help=&quot;interval between model checkpoints&quot;)opt = parser.parse_args()print(opt)cuda = torch.cuda.is_available()hr_shape = (opt.hr_height, opt.hr_width)# Initialize generator and discriminatorgenerator = GeneratorResNet()discriminator = Discriminator(input_shape=(opt.channels, *hr_shape))feature_extractor = FeatureExtractor()# Set feature extractor to inference modefeature_extractor.eval()# Lossescriterion_GAN = torch.nn.MSELoss()criterion_content = torch.nn.L1Loss()if cuda: generator = generator.cuda() discriminator = discriminator.cuda() feature_extractor = feature_extractor.cuda() criterion_GAN = criterion_GAN.cuda() criterion_content = criterion_content.cuda()if opt.epoch != 0: # Load pretrained models generator.load_state_dict(torch.load(&quot;/home/myself/work/work-generate/srgan/saved_models/generator_%d.pth&quot;)) discriminator.load_state_dict(torch.load(&quot;/home/myself/work/work-generate/srgan/saved_models/discriminator_%d.pth&quot;))# Optimizersoptimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))Tensor = torch.cuda.FloatTensor if cuda else torch.Tensordataloader = DataLoader( ImageDataset(&quot;/home/myself/work/work-generate/%s&quot; % opt.dataset_name, hr_shape=hr_shape), batch_size=opt.batch_size, shuffle=True, num_workers=opt.n_cpu,)# ----------# Training# ----------def main(): for epoch in range(opt.epoch, opt.n_epochs): for i, imgs in enumerate(dataloader): # Configure model input imgs_lr = Variable(imgs[&quot;lr&quot;].type(Tensor)) imgs_hr = Variable(imgs[&quot;hr&quot;].type(Tensor)) # Adversarial ground truths valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False) fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False) # ------------------ # Train Generators # ------------------ optimizer_G.zero_grad() # Generate a high resolution image from low resolution input gen_hr = generator(imgs_lr) # Adversarial loss loss_GAN = criterion_GAN(discriminator(gen_hr), valid) # Content loss gen_features = feature_extractor(gen_hr) real_features = feature_extractor(imgs_hr) loss_content = criterion_content(gen_features, real_features.detach()) # Total loss loss_G = loss_content + 1e-3 * loss_GAN loss_G.backward() optimizer_G.step() # --------------------- # Train Discriminator # --------------------- optimizer_D.zero_grad() # Loss of real and fake images loss_real = criterion_GAN(discriminator(imgs_hr), valid) loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake) # Total loss loss_D = (loss_real + loss_fake) / 2 loss_D.backward() optimizer_D.step() # -------------- # Log Progress # -------------- sys.stdout.write( &quot;[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]&quot; % (epoch, opt.n_epochs, i, len(dataloader), loss_D.item(), loss_G.item()) ) batches_done = epoch * len(dataloader) + i if batches_done % opt.sample_interval == 0: # Save image grid with upsampled inputs and SRGAN outputs imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4) gen_hr = make_grid(gen_hr, nrow=1, normalize=True) imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True) img_grid = torch.cat((imgs_lr, gen_hr), -1) save_image(img_grid, &quot;/home/myself/work/work-generate/srgan/images/%d.png&quot; % batches_done, normalize=False) if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0: # Save model checkpoints torch.save(generator.state_dict(), &quot;/home/myself/work/work-generate/srgan/saved_models/generator_%d.pth&quot; % epoch) torch.save(discriminator.state_dict(), &quot;/home/myself/work/work-generate/srgan/saved_models/discriminator_%d.pth&quot; % epoch)if __name__ == &#x27;__main__&#x27;: main() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103# 导入所需库和模块import torch.nn as nn # PyTorch神经网络模块import torch.nn.functional as F # 附加功能，如激活函数等import torch # 主PyTorch库from torchvision.models import vgg19 # VGG19模型，虽然未直接使用，但可能是参考或预留import math # 数学运算库from PIL import Image # 处理图像的库import torchvision.transforms as transforms # 图像转换工具from torchvision.utils import save_image # 保存图像工具import numpy as np # 数值处理库# 定义残差块，用于构建生成器网络中的残差网络结构class ResidualBlock(nn.Module): def __init__(self, in_features): super(ResidualBlock, self).__init__() # 定义一系列卷积、批归一化、激活函数操作 self.conv_block = nn.Sequential( nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(in_features, momentum=0.8), # 动量参数设置为0.8 nn.PReLU(), # 参数为默认值的PReLU激活函数 nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(in_features, momentum=0.8), ) def forward(self, x): # 实现残差连接，输入x加上经过卷积块处理后的x return x + self.conv_block(x)# 定义生成器网络，基于残差网络结构class GeneratorResNet(nn.Module): def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16): super(GeneratorResNet, self).__init__() # 第一层卷积和PReLU激活 self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU()) # 重复定义n_residual_blocks次数的残差块 res_blocks = [ResidualBlock(64) for _ in range(n_residual_blocks)] self.res_blocks = nn.Sequential(*res_blocks) # 将所有残差块放入一个Sequential容器中 # 残差块之后的第二个卷积层和批归一化 self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64, momentum=0.8)) # 上采样层，这里使用PixelShuffle进行上采样 upsampling = [ nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.PixelShuffle(upscale_factor=2), # 使用PixelShuffle上采样，因子为2 nn.PReLU(), # 激活函数 ] * 2 # 重复两次上述操作，进行两轮上采样 self.upsampling = nn.Sequential(*upsampling) # 输出层，最终的卷积层和Tanh激活 self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh()) def forward(self, x): # 前向传播过程，包括残差结构、上采样等 out1 = self.conv1(x) out = self.res_blocks(out1) out2 = self.conv2(out) out = torch.add(out1, out2) # 这里直接相加可能有误，应是残差结构的正确应用 out = self.upsampling(out) out = self.conv3(out) return out# 反标准化函数，将模型输出转换为可显示的格式def unnormalize(tensor, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]): tensor = tensor.clone() # 防止原地修改，先复制张量 for i in range(tensor.size(1)): # 遍历每个通道 tensor[:, i, :, :] = tensor[:, i, :, :] * std[i] + mean[i] # 反标准化操作 return tensor# 主程序入口if __name__ == &#x27;__main__&#x27;: # 设置均值和标准差，用于图像标准化 mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) # 图像转换管道，包含转Tensor和标准化 transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean, std), ]) # 初始化并加载预训练的生成器模型 g = GeneratorResNet() g.load_state_dict(torch.load(&#x27;generator_100.pth&#x27;)) # 加载并处理图像 input= Image.open(&quot;C:\\\\Users\\\\hahag\\\\OneDrive\\\\WORK\\\\work-generate\\\\6.jpg&quot;).convert(&quot;RGB&quot;) input = transform(input) # 转换图像 # 添加batch维度 input = input.unsqueeze(0) # 通过生成器生成输出 output= g(input) # 反标准化 output = unnormalize(output) # 保存生成的图像 save_image(output, &#x27;output.png&#x27;, normalize=False)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-生成","slug":"Code-生成","permalink":"http://example.com/tags/Code-%E7%94%9F%E6%88%90/"}]},{"title":"Coding-GAN-生成对抗网络-基于RGB图片数据","slug":"coding_image_GAN","date":"2024-05-21T11:55:09.914Z","updated":"2024-05-21T12:01:10.777Z","comments":true,"path":"2024/05/21/coding_image_GAN/","link":"","permalink":"http://example.com/2024/05/21/coding_image_GAN/","excerpt":"","text":"摘要 gan.py 摘要 gan.py用于训练网络 图片数据放在/home/myself/work/work-generate/data/data下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216import argparseimport osimport numpy as npimport mathimport torchvision.transforms as transformsfrom torchvision.utils import save_imagefrom torch.utils.data import Dataset,DataLoaderfrom torchvision import datasetsfrom torch.autograd import Variableimport torch.nn as nnimport torch.nn.functional as Fimport torchfrom torchvision.datasets import ImageFolderfrom PIL import Imageimport torchvision.models as models# 加载预训练的VGG模型vgg = models.vgg16(pretrained=True)os.makedirs(&quot;/home/myself/work/work-generate/images&quot;, exist_ok=True)parser = argparse.ArgumentParser()parser.add_argument(&quot;--n_epochs&quot;, type=int, default=2000, help=&quot;number of epochs of training&quot;)parser.add_argument(&quot;--batch_size&quot;, type=int, default=32, help=&quot;size of the batches&quot;)parser.add_argument(&quot;--lr&quot;, type=float, default=0.0002, help=&quot;adam: learning rate&quot;)parser.add_argument(&quot;--b1&quot;, type=float, default=0.5, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--b2&quot;, type=float, default=0.999, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--n_cpu&quot;, type=int, default=8, help=&quot;number of cpu threads to use during batch generation&quot;)parser.add_argument(&quot;--latent_dim&quot;, type=int, default=1000, help=&quot;dimensionality of the latent space&quot;)parser.add_argument(&quot;--img_size&quot;, type=int, default=128, help=&quot;size of each image dimension&quot;)parser.add_argument(&quot;--channels&quot;, type=int, default=3, help=&quot;number of image channels&quot;)parser.add_argument(&quot;--sample_interval&quot;, type=int, default=400, help=&quot;interval betwen image samples&quot;)opt = parser.parse_args()print(opt)# 自定义数据集类class CustomImageDataset(Dataset): def __init__(self, root_dir, transform=None): self.root_dir = root_dir self.transform = transform self.images = [f for f in os.listdir(root_dir) if f.endswith(&#x27;.jpg&#x27;)] def __len__(self): return len(self.images) def __getitem__(self, idx): img_path = os.path.join(self.root_dir, self.images[idx]) image = Image.open(img_path).convert(&quot;RGB&quot;) # 确保图片是RGB模式 if self.transform: image = self.transform(image) return imageclass Generator(nn.Module): def __init__(self): super(Generator, self).__init__() def block(in_feat, out_feat, normalize=True): layers = [nn.Linear(in_feat, out_feat)] if normalize: layers.append(nn.BatchNorm1d(out_feat, 0.8)) layers.append(nn.LeakyReLU(0.2, inplace=True)) return layers self.model = nn.Sequential( *block(opt.latent_dim, 128, normalize=False), *block(128, 256), *block(256, 512), *block(512, 1024), nn.Linear(1024, int(np.prod(img_shape))), nn.Tanh() ) def forward(self, z): img = self.model(z) img = img.view(img.size(0), *img_shape) return imgclass Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() self.model = nn.Sequential( nn.Linear(int(np.prod(img_shape)), 512), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 256), nn.LeakyReLU(0.2, inplace=True), nn.Linear(256, 1), nn.Sigmoid(), ) def forward(self, img): img_flat = img.view(img.size(0), -1) validity = self.model(img_flat) return validityimg_shape = (opt.channels, opt.img_size, opt.img_size)cuda = True if torch.cuda.is_available() else False# Loss functionadversarial_loss = torch.nn.BCELoss()# Initialize generator and discriminatorgenerator = Generator()discriminator = Discriminator()if cuda: generator.cuda() discriminator.cuda() adversarial_loss.cuda()# Configure data loader# 图像预处理transform = transforms.Compose([ transforms.Resize((opt.img_size, opt.img_size)), # 确保这与模型输入尺寸匹配 transforms.ToTensor(), # 将PIL Image转换为Tensor transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # 归一化])# 创建数据集实例dataset = CustomImageDataset(root_dir=&#x27;/home/myself/work/work-generate/data&#x27;, transform=transform)# 创建DataLoader实例dataloader = DataLoader( dataset, batch_size=opt.batch_size, # 使用您之前定义的批处理大小 shuffle=True, # 在每个epoch开始时打乱数据 num_workers=opt.n_cpu, # 使用指定数量的CPU线程加载数据)# Optimizersoptimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensordef main(): # ---------- # Training # ---------- for epoch in range(opt.n_epochs): for i, imgs in enumerate(dataloader): # Adversarial ground truths valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False) fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False) # Configure input real_imgs = Variable(imgs.type(Tensor)) # ----------------- # Train Generator # ----------------- optimizer_G.zero_grad() # Sample noise as generator input z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim)))) # Generate a batch of images gen_imgs = generator(z) # Loss measures generator&#x27;s ability to fool the discriminator g_loss = adversarial_loss(discriminator(gen_imgs), valid) g_loss.backward() optimizer_G.step() # --------------------- # Train Discriminator # --------------------- optimizer_D.zero_grad() # Measure discriminator&#x27;s ability to classify real from generated samples real_loss = adversarial_loss(discriminator(real_imgs), valid) fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake) d_loss = (real_loss + fake_loss) / 2 d_loss.backward() optimizer_D.step() print( &quot;[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]&quot; % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item()) ) batches_done = epoch * len(dataloader) + i if batches_done % opt.sample_interval == 0: save_image(gen_imgs.data[:25], &quot;/home/myself/work/work-generate/images/%d.png&quot; % batches_done, nrow=5, normalize=True)if __name__ == &#x27;__main__&#x27;: main()","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-生成","slug":"Code-生成","permalink":"http://example.com/tags/Code-%E7%94%9F%E6%88%90/"}]},{"title":"Coding-QT-连接云端Mysql配置","slug":"coding_qt_link_mysql","date":"2024-05-16T11:33:35.467Z","updated":"2024-05-21T12:22:40.975Z","comments":true,"path":"2024/05/16/coding_qt_link_mysql/","link":"","permalink":"http://example.com/2024/05/16/coding_qt_link_mysql/","excerpt":"","text":"进入数据库 1mysql -u root -p 创建新用户 1CREATE USER &#x27;yyy&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;password&#x27;; 查看用户 1SELECT yyy,host FROM mysql.user; 使用数据库, 为特定用户（yyy）修改host 123use mysql;update user set host=&#x27;%&#x27; where user=&#x27;yyy&#x27;; 接着进行用户赋权 12GRANT ALL PRIVILEGES ON test_db.* TO &#x27;yyy&#x27;@&#x27;%&#x27; WITH GRANT OPTION;# test_db是数据库名 查看MySQL是否对外开放 1netstat -an | grep 3306 如果没有要修改配置文件 1234cd /etc/mysql/mysql.conf.d vim mysqld.cnf# 注释掉 blind-address 和 myswlx-blind-adress 然后重启 1service mysql restart 开放3306端口 1sudo ufw allow 3306 QT连接代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &quot;mainwindow.h&quot;#include &lt;QApplication&gt;#include &lt;QLocale&gt;#include &lt;QTranslator&gt;#include &lt;QCoreApplication&gt;#include &lt;QDebug&gt;#include &lt;QPluginLoader&gt;#include &lt;QSql&gt;#include &lt;QSqlDatabase&gt;#include&lt;QSqlQuery&gt;int main(int argc, char *argv[])&#123; QApplication a(argc, argv);//本地 QSqlDatabase db=QSqlDatabase::addDatabase(&quot;QMYSQL&quot;); db.setHostName(&quot;localhost&quot;); // 本地数据库 远程DB是：ipaddress db.setPort(3306); // 设置端口号 db.setDatabaseName(&quot;test_db&quot;); // 使用的数据库 sql = use &#x27;数据库名&#x27; db.setUserName(&quot;yyy&quot;); db.setPassword(&quot;passwd&quot;); if(db.open())&#123; qDebug()&lt;&lt;&quot;成功&quot;; &#125;else&#123; qDebug()&lt;&lt;&quot;失败&quot;; &#125; QTranslator translator; const QStringList uiLanguages = QLocale::system().uiLanguages(); for (const QString &amp;locale : uiLanguages) &#123; const QString baseName = &quot;main_&quot; + QLocale(locale).name(); if (translator.load(&quot;:/i18n/&quot; + baseName)) &#123; a.installTranslator(&amp;translator); break; &#125; &#125; MainWindow w; w.show(); return a.exec();&#125;","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Qt","slug":"Qt","permalink":"http://example.com/tags/Qt/"}]},{"title":"Note-悬雍垂","slug":"note-uvula","date":"2024-02-04T01:39:41.593Z","updated":"2024-02-04T01:45:36.209Z","comments":true,"path":"2024/02/04/note-uvula/","link":"","permalink":"http://example.com/2024/02/04/note-uvula/","excerpt":"","text":"悬雍垂（uvule[ˈjuːvjʊlə]）又名腭垂，俗称“小舌”、“吊钟”，是人体口腔器官，悬挂于软腭正中间的末端。悬雍垂的功能是在饮食时上升堵住食物通过鼻腔进入气管的通道，从而使食物进入食道。在语言上，部分语言需要利用小舌的振动发出小舌音。 悬雍垂与其连接的软腭，与睡眠呼吸中止症有相当程度的关联，经常好发于中年男性，可以透过手术治疗：例如悬雍垂腭咽整型手术（UPPP手术）、微创软腭支架手术，可以有效解决呼吸中止，以及打鼾音量的问题。","categories":[{"name":"笔记","slug":"笔记","permalink":"http://example.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"医学","slug":"医学","permalink":"http://example.com/tags/%E5%8C%BB%E5%AD%A6/"}]},{"title":"Coding-CGAN","slug":"coding_cGAN","date":"2024-01-26T05:51:59.270Z","updated":"2024-11-29T11:58:18.489Z","comments":true,"path":"2024/01/26/coding_cGAN/","link":"","permalink":"http://example.com/2024/01/26/coding_cGAN/","excerpt":"","text":"摘要 cgan.py 摘要 生成器的输入为噪声 zzz 和标签 lll ，要求生成器能生成真图片，要求判别器能判别生成的真图片和真正的真图片 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204import argparseimport osimport numpy as npimport mathimport torchvision.transforms as transformsfrom torchvision.utils import save_imagefrom torch.utils.data import DataLoaderfrom torchvision import datasetsfrom torch.autograd import Variableimport torch.nn as nnimport torch.nn.functional as Fimport torchos.makedirs(&quot;images&quot;, exist_ok=True)parser = argparse.ArgumentParser()parser.add_argument(&quot;--n_epochs&quot;, type=int, default=200, help=&quot;number of epochs of training&quot;)parser.add_argument(&quot;--batch_size&quot;, type=int, default=64, help=&quot;size of the batches&quot;)parser.add_argument(&quot;--lr&quot;, type=float, default=0.0002, help=&quot;adam: learning rate&quot;)parser.add_argument(&quot;--b1&quot;, type=float, default=0.5, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--b2&quot;, type=float, default=0.999, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--n_cpu&quot;, type=int, default=8, help=&quot;number of cpu threads to use during batch generation&quot;)parser.add_argument(&quot;--latent_dim&quot;, type=int, default=100, help=&quot;dimensionality of the latent space&quot;)parser.add_argument(&quot;--n_classes&quot;, type=int, default=10, help=&quot;number of classes for dataset&quot;)parser.add_argument(&quot;--img_size&quot;, type=int, default=32, help=&quot;size of each image dimension&quot;)parser.add_argument(&quot;--channels&quot;, type=int, default=1, help=&quot;number of image channels&quot;)parser.add_argument(&quot;--sample_interval&quot;, type=int, default=400, help=&quot;interval between image sampling&quot;)opt = parser.parse_args()print(opt)img_shape = (opt.channels, opt.img_size, opt.img_size)cuda = True if torch.cuda.is_available() else Falseclass Generator(nn.Module): def __init__(self): super(Generator, self).__init__() self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes) def block(in_feat, out_feat, normalize=True): layers = [nn.Linear(in_feat, out_feat)] if normalize: layers.append(nn.BatchNorm1d(out_feat, 0.8)) layers.append(nn.LeakyReLU(0.2, inplace=True)) return layers self.model = nn.Sequential( *block(opt.latent_dim + opt.n_classes, 128, normalize=False), *block(128, 256), *block(256, 512), *block(512, 1024), nn.Linear(1024, int(np.prod(img_shape))), nn.Tanh() ) def forward(self, noise, labels): # Concatenate label embedding and image to produce input gen_input = torch.cat((self.label_emb(labels), noise), -1) img = self.model(gen_input) img = img.view(img.size(0), *img_shape) return imgclass Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes) self.model = nn.Sequential( nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 512), nn.Dropout(0.4), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 512), nn.Dropout(0.4), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 1), ) def forward(self, img, labels): # Concatenate label embedding and image to produce input d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1) validity = self.model(d_in) return validity# Loss functionsadversarial_loss = torch.nn.MSELoss()# Initialize generator and discriminatorgenerator = Generator()discriminator = Discriminator()if cuda: generator.cuda() discriminator.cuda() adversarial_loss.cuda()# Configure data loaderos.makedirs(&quot;../../data/mnist&quot;, exist_ok=True)dataloader = torch.utils.data.DataLoader( datasets.MNIST( &quot;../../data/mnist&quot;, train=True, download=True, transform=transforms.Compose( [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])] ), ), batch_size=opt.batch_size, shuffle=True,)# Optimizersoptimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensorLongTensor = torch.cuda.LongTensor if cuda else torch.LongTensordef sample_image(n_row, batches_done): &quot;&quot;&quot;Saves a grid of generated digits ranging from 0 to n_classes&quot;&quot;&quot; # Sample noise z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim)))) # Get labels ranging from 0 to n_classes for n rows labels = np.array([num for _ in range(n_row) for num in range(n_row)]) labels = Variable(LongTensor(labels)) gen_imgs = generator(z, labels) save_image(gen_imgs.data, &quot;images/%d.png&quot; % batches_done, nrow=n_row, normalize=True)# ----------# Training# ----------for epoch in range(opt.n_epochs): for i, (imgs, labels) in enumerate(dataloader): batch_size = imgs.shape[0] # Adversarial ground truths valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False) fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False) # Configure input real_imgs = Variable(imgs.type(FloatTensor)) labels = Variable(labels.type(LongTensor)) # ----------------- # Train Generator # ----------------- optimizer_G.zero_grad() # Sample noise and labels as generator input z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim)))) gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size))) # Generate a batch of images gen_imgs = generator(z, gen_labels) # Loss measures generator&#x27;s ability to fool the discriminator 要求生成器能生成真图片 validity = discriminator(gen_imgs, gen_labels) g_loss = adversarial_loss(validity, valid) g_loss.backward() optimizer_G.step() # --------------------- # Train Discriminator # --------------------- optimizer_D.zero_grad() # Loss for real images validity_real = discriminator(real_imgs, labels) d_real_loss = adversarial_loss(validity_real, valid) # Loss for fake images validity_fake = discriminator(gen_imgs.detach(), gen_labels) d_fake_loss = adversarial_loss(validity_fake, fake) # Total discriminator loss 要求判别器能判别生成的真图片和真正的真图片 d_loss = (d_real_loss + d_fake_loss) / 2 d_loss.backward() optimizer_D.step() print( &quot;[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]&quot; % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item()) ) batches_done = epoch * len(dataloader) + i if batches_done % opt.sample_interval == 0: sample_image(n_row=10, batches_done=batches_done)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-模型库","slug":"Code-模型库","permalink":"http://example.com/tags/Code-%E6%A8%A1%E5%9E%8B%E5%BA%93/"}]},{"title":"Coding-GAN","slug":"coding_GAN","date":"2024-01-26T05:51:59.265Z","updated":"2024-11-29T11:58:18.484Z","comments":true,"path":"2024/01/26/coding_GAN/","link":"","permalink":"http://example.com/2024/01/26/coding_GAN/","excerpt":"","text":"摘要 cgan.py 摘要 生成器的输入为噪声 zzz ，要求生成器能生成真图片，要求判别器能判别生成的真图片和真正的真图片 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170import argparseimport osimport numpy as npimport mathimport torchvision.transforms as transformsfrom torchvision.utils import save_imagefrom torch.utils.data import DataLoaderfrom torchvision import datasetsfrom torch.autograd import Variableimport torch.nn as nnimport torch.nn.functional as Fimport torchos.makedirs(&quot;images&quot;, exist_ok=True)parser = argparse.ArgumentParser()parser.add_argument(&quot;--n_epochs&quot;, type=int, default=200, help=&quot;number of epochs of training&quot;)parser.add_argument(&quot;--batch_size&quot;, type=int, default=64, help=&quot;size of the batches&quot;)parser.add_argument(&quot;--lr&quot;, type=float, default=0.0002, help=&quot;adam: learning rate&quot;)parser.add_argument(&quot;--b1&quot;, type=float, default=0.5, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--b2&quot;, type=float, default=0.999, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--n_cpu&quot;, type=int, default=8, help=&quot;number of cpu threads to use during batch generation&quot;)parser.add_argument(&quot;--latent_dim&quot;, type=int, default=100, help=&quot;dimensionality of the latent space&quot;)parser.add_argument(&quot;--img_size&quot;, type=int, default=28, help=&quot;size of each image dimension&quot;)parser.add_argument(&quot;--channels&quot;, type=int, default=1, help=&quot;number of image channels&quot;)parser.add_argument(&quot;--sample_interval&quot;, type=int, default=400, help=&quot;interval betwen image samples&quot;)opt = parser.parse_args()print(opt)img_shape = (opt.channels, opt.img_size, opt.img_size)cuda = True if torch.cuda.is_available() else Falseclass Generator(nn.Module): def __init__(self): super(Generator, self).__init__() def block(in_feat, out_feat, normalize=True): layers = [nn.Linear(in_feat, out_feat)] if normalize: layers.append(nn.BatchNorm1d(out_feat, 0.8)) layers.append(nn.LeakyReLU(0.2, inplace=True)) return layers self.model = nn.Sequential( *block(opt.latent_dim, 128, normalize=False), *block(128, 256), *block(256, 512), *block(512, 1024), nn.Linear(1024, int(np.prod(img_shape))), nn.Tanh() ) def forward(self, z): img = self.model(z) img = img.view(img.size(0), *img_shape) return imgclass Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() self.model = nn.Sequential( nn.Linear(int(np.prod(img_shape)), 512), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 256), nn.LeakyReLU(0.2, inplace=True), nn.Linear(256, 1), nn.Sigmoid(), ) def forward(self, img): img_flat = img.view(img.size(0), -1) validity = self.model(img_flat) return validity# Loss functionadversarial_loss = torch.nn.BCELoss()# Initialize generator and discriminatorgenerator = Generator()discriminator = Discriminator()if cuda: generator.cuda() discriminator.cuda() adversarial_loss.cuda()# Configure data loaderos.makedirs(&quot;../../data/mnist&quot;, exist_ok=True)dataloader = torch.utils.data.DataLoader( datasets.MNIST( &quot;../../data/mnist&quot;, train=True, download=True, transform=transforms.Compose( [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])] ), ), batch_size=opt.batch_size, shuffle=True,)# Optimizersoptimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor# ----------# Training# ----------for epoch in range(opt.n_epochs): for i, (imgs, _) in enumerate(dataloader): # Adversarial ground truths valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False) fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False) # Configure input real_imgs = Variable(imgs.type(Tensor)) # ----------------- # Train Generator # ----------------- optimizer_G.zero_grad() # Sample noise as generator input z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim)))) # Generate a batch of images gen_imgs = generator(z) # Loss measures generator&#x27;s ability to fool the discriminator g_loss = adversarial_loss(discriminator(gen_imgs), valid) g_loss.backward() optimizer_G.step() # --------------------- # Train Discriminator # --------------------- optimizer_D.zero_grad() # Measure discriminator&#x27;s ability to classify real from generated samples real_loss = adversarial_loss(discriminator(real_imgs), valid) fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake) d_loss = (real_loss + fake_loss) / 2 d_loss.backward() optimizer_D.step() print( &quot;[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]&quot; % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item()) ) batches_done = epoch * len(dataloader) + i if batches_done % opt.sample_interval == 0: save_image(gen_imgs.data[:25], &quot;images/%d.png&quot; % batches_done, nrow=5, normalize=True)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-模型库","slug":"Code-模型库","permalink":"http://example.com/tags/Code-%E6%A8%A1%E5%9E%8B%E5%BA%93/"}]},{"title":"Hexo运营维护","slug":"coding_hexo","date":"2024-01-26T05:51:59.260Z","updated":"2024-05-22T02:26:20.530Z","comments":true,"path":"2024/01/26/coding_hexo/","link":"","permalink":"http://example.com/2024/01/26/coding_hexo/","excerpt":"","text":"生成推送 为文章添加目录 公式使用 生成推送 1hexo g -d 为文章添加目录 安装插件 1npm install hexo-toc --save 配置博客根目录下的_config.yml文件 12toc: maxdepth: 3 在需要展示目录的地方添加： 1&lt;!-- toc --&gt; 注意：显示的目录只会包含代码段 &lt; !-- toc --&gt;之后的内容 公式使用 安装 12npm un hexo-renderer-marked --savenpm i hexo-renderer-markdown-it-plus --save 在 _config.yml中配置 12345678910111213141516markdown_it_plus: highlight: true html: true xhtmlOut: true breaks: true langPrefix: linkify: true typographer: quotes: “”‘’ plugins: - plugin: name: markdown-it-katex enable: true - plugin: name: markdown-it-mark enable: false 使 mathjax生效 12title: Hello Worldmathjax: true","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"编程基础知识","slug":"编程基础知识","permalink":"http://example.com/tags/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}]},{"title":"Note-临床实验","slug":"note-clinical_trial","date":"2024-01-22T02:20:16.691Z","updated":"2024-02-04T01:42:23.635Z","comments":true,"path":"2024/01/22/note-clinical_trial/","link":"","permalink":"http://example.com/2024/01/22/note-clinical_trial/","excerpt":"","text":"0期临床试验 使用微剂量健康受试者或病人进行给药研究。 所谓微剂量，是指低于通过临床前毒理学研究获得的动物安全性数据而推导出的拟用于人体可能产生临床药理学作用剂量的1/100，同时，最大剂量不超过100ug的剂量。 Ⅰ期临床试验 从初始安全剂量开始，逐渐加大，观察人体对该种新药的耐受程度，以确定人体可接受而又不会导致毒副反应发生的剂量大小。 之后将进行多次给药试验，以确定适合于Ⅱ期临床试验所需的剂量和程序。同时，还必须进行人体的单剂量和多剂量的药动学研究，以为Ⅱ期临床试验提供合适的治疗方案。Ⅰ期临床试验通常由健康的志愿者参与。一般而言，Ⅰ期临床试验总共需要试验10~80个病人。 Ⅱ期临床试验 用较小总体的选定适应症的患者，对药物的疗效和安全性进行临床研究，其间将重点观察新药的治疗效果和不良反应。 同时，还要对新药的药动学和生物利用度方面进行研究，以确定患者与健康人的药动学差异。Ⅱ期临床试验的主要目的是为Ⅲ期临床试验做准备，以确定初步的临床适应症和治疗方案。Ⅱ期临床试验总共需要试验100-200个病人。 Ⅲ期临床试验 对已通过Ⅱ期临床试验确定了其疗效的新药，与现有已知活性的药物或无药理活性的安慰剂进行对照试验。 该期试验对于患者的选择非常严格，其还必须具有明确的疗效标准和安全性评价标准。新药在经过对照试验后，将对其疗效和长期安全性进行全面的评价，以判断其是否具有治疗学和安全性特征，这决定着其是否能够批准上市销售。Ⅲ期临床试验总共需要试验300－500个病人，最少要测试100次，否则统计学上会有误差，对照组的数量则无具体规定。 Ⅳ期临床试验 在新药推出后，通过大量调查药物对病人的临床效果及情况，监视新药有无效，如何最好地使用以及副作用的发生机会和程度。 若疗效不理想或出现严重的副作用而且发生率较高，管制部门则会将那新药召回和退市。第4期临床试验会一直进行，只要仍有很多人用这种药物。","categories":[{"name":"笔记","slug":"笔记","permalink":"http://example.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"医学","slug":"医学","permalink":"http://example.com/tags/%E5%8C%BB%E5%AD%A6/"}]},{"title":"Coding-torch-数据集-Datasets","slug":"coding_mydatasets","date":"2024-01-08T15:15:05.583Z","updated":"2024-11-29T11:58:18.483Z","comments":true,"path":"2024/01/08/coding_mydatasets/","link":"","permalink":"http://example.com/2024/01/08/coding_mydatasets/","excerpt":"","text":"torch内置datasets的使用方法 适用于图片分类的datasets(数据放在不同的文件夹下表示不同类别) 适用于图片分割，目标检测的datasets(数据和标签都是图像) 子数据集划分 读入一个文件夹下的图片制作数据集（不分类） torch内置datasets的使用方法 1234567891011121314151617import torchimport torchvisionimport torchvision.transforms as transforms# 数据预处理transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # 图像归一化])# 加载训练集和测试集trainset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=True, download=True, transform=transform)testset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=False, download=True, transform=transform)# 创建数据加载器trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2) 适用于图片分类的datasets(数据放在不同的文件夹下表示不同类别) 12345678910111213141516171819202122232425262728293031import glob # 导入用于文件路径匹配的模块from torchvision import transforms # 导入图像转换模块from torch.utils import data # 导入PyTorch数据工具模块from PIL import Image # 导入PIL图像处理库# 标准化数据transforms = transforms.Compose([ transforms.ToTensor(), # 将图像转换为张量 transforms.Resize((256, 256)), # 调整图像大小为256x256 transforms.Normalize(mean=0.5, std=0.5) # 标准化图像数据])class my_dataset(data.Dataset): def __init__(self, imgs_path, annos_path): self.imgs_path = imgs_path # 图像文件路径 self.annos_path = annos_path # 标签文件路径 def __getitem__(self, index): img_path = self.imgs_path[index] # 获取图像路径 pil_img = Image.open(img_path) # 使用PIL打开图像 pil_img = transforms(pil_img) # 对图像进行预处理 anno_path = self.annos_path[index] # 获取标签路径 anno_img = Image.open(anno_path) # 使用PIL打开标签图像 pil_anno = transforms(anno_img) # 对标签图像进行预处理 return pil_img, pil_anno# 创建训练和验证数据集train_dataset = CustomDataset(train_data_path, transform=transform)val_dataset = CustomDataset(val_data_path, transform=transform) 适用于图片分割，目标检测的datasets(数据和标签都是图像) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import glob # 导入用于文件路径匹配的模块from torchvision import transforms # 导入图像转换模块from torch.utils import data # 导入PyTorch数据工具模块from PIL import Image # 导入PIL图像处理库# 标准化数据transforms = transforms.Compose([ transforms.ToTensor(), # 将图像转换为张量 transforms.Resize((256, 256)), # 调整图像大小为256x256 transforms.Normalize(mean=0.5, std=0.5) # 标准化图像数据])class my_dataset(data.Dataset): def __init__(self, imgs_path, annos_path): self.imgs_path = imgs_path # 图像文件路径 self.annos_path = annos_path # 标签文件路径 def __getitem__(self, index): img_path = self.imgs_path[index] # 获取图像路径 pil_img = Image.open(img_path) # 使用PIL打开图像 pil_img = transforms(pil_img) # 对图像进行预处理 anno_path = self.annos_path[index] # 获取标签路径 anno_img = Image.open(anno_path) # 使用PIL打开标签图像 pil_anno = transforms(anno_img) # 对标签图像进行预处理 return pil_img, pil_anno def __len__(self): return len(self.imgs_path) # 返回数据集的长度# 训练数据集导入imgs_path = glob.glob(&#x27;facade/train_picture/*.png&#x27;) # 匹配训练图像文件路径label_path = glob.glob(&#x27;facade/train_label/*.jpg&#x27;) # 匹配训练标签文件路径# 测试数据集导入test_imgs_path = glob.glob(&#x27;facade/test_picture/*.png&#x27;) # 匹配测试图像文件路径test_label_path = glob.glob(&#x27;facade/test_label/*.jpg&#x27;) # 匹配测试标签文件路径# 对数据和标签排序，确保一一对应imgs_path = sorted(imgs_path)label_path = sorted(label_path)test_imgs_path = sorted(test_imgs_path)test_label_path = sorted(test_label_path)train_dataset = my_dataset(imgs_path, label_path) test_dataset = my_dataset(test_imgs_path, test_label_path) # 创建测试数据集对象train_loader = data.DataLoader(train_dataset, batch_size=4, shuffle=True) # 创建训练数据加载器test_loader = data.DataLoader(test_dataset, batch_size=4, shuffle=False) # 创建测试数据加载器 子数据集划分 1234567891011121314151617181920from torch.utils.data import Subset# 加载数据集，并分成两个子集train_data = dsets.ImageFolder(root=&#x27;data_self/train&#x27;, transform=transform)test_data = dsets.ImageFolder(root=&#x27;data_self/test&#x27;, transform=transform)# 创建训练数据集的索引列表train_indices1 = list(range(0, len(train_data), 2))train_indices2 = list(range(1, len(train_data), 2))# 创建训练子集1和训练子集2train_data1 = Subset(train_data, train_indices1)train_data2 = Subset(train_data, train_indices2)# 创建测试数据集的索引列表test_indices1 = list(range(0, len(test_data), 2))test_indices2 = list(range(1, len(test_data), 2))# 创建测试子集1和测试子集2test_data1 = Subset(test_data, test_indices1)test_data2 = Subset(test_data, test_indices2) 读入一个文件夹下的图片制作数据集（不分类） 12345678910111213141516171819202122232425262728293031323334353637# 自定义数据集类class CustomImageDataset(Dataset): def __init__(self, root_dir, transform=None): self.root_dir = root_dir self.transform = transform self.images = [f for f in os.listdir(root_dir) if f.endswith(&#x27;.jpg&#x27;)] def __len__(self): return len(self.images) def __getitem__(self, idx): img_path = os.path.join(self.root_dir, self.images[idx]) image = Image.open(img_path).convert(&quot;RGB&quot;) # 确保图片是RGB模式 if self.transform: image = self.transform(image) return image# 图像预处理transform = transforms.Compose([ transforms.Resize((opt.img_size, opt.img_size)), # 确保这与模型输入尺寸匹配 transforms.ToTensor(), # 将PIL Image转换为Tensor transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # 归一化])# 创建数据集实例dataset = CustomImageDataset(root_dir=&#x27;/home/yeshixin/work/work-generate/data&#x27;, transform=transform)# 创建DataLoader实例dataloader = DataLoader( dataset, batch_size=opt.batch_size, # 使用您之前定义的批处理大小 shuffle=True, # 在每个epoch开始时打乱数据 num_workers=opt.n_cpu, # 使用指定数量的CPU线程加载数据)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-模型库","slug":"Code-模型库","permalink":"http://example.com/tags/Code-%E6%A8%A1%E5%9E%8B%E5%BA%93/"}]},{"title":"Coding-猫狗数据集分类-基于CNN","slug":"coding_network_CNN","date":"2024-01-08T14:22:20.995Z","updated":"2024-01-23T03:54:29.595Z","comments":true,"path":"2024/01/08/coding_network_CNN/","link":"","permalink":"http://example.com/2024/01/08/coding_network_CNN/","excerpt":"","text":"摘要 main.py 摘要 训练一个图片分类神经网络（CNN） 文件目录 . ├── train │ ├── cats │ └── dogs └── validation ├── cats └── dogs 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140import torchimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import DataLoader, Datasetfrom torchvision import transformsfrom torchvision.datasets import ImageFolder# 定义自定义数据集类class CustomDataset(Dataset): def __init__(self, root_dir, transform=None): self.dataset = ImageFolder(root_dir, transform=transform) def __len__(self): return len(self.dataset) def __getitem__(self, idx): image, label = self.dataset[idx] return image, label# 定义卷积神经网络模型class CNNModel(nn.Module): def __init__(self, num_classes): super(CNNModel, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), ) self.classifier = nn.Sequential( nn.Linear(32 * 3 * 3, 256), nn.ReLU(inplace=True), nn.Linear(256, num_classes) ) def forward(self, x): x = self.features(x) x = x.view(x.size(0), -1) x = self.classifier(x) return x# 设置训练和验证数据集的路径train_data_path = &#x27;data1/train&#x27;val_data_path = &#x27;data1/validation&#x27;# 定义图像转换transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])# 创建训练和验证数据集train_dataset = CustomDataset(train_data_path, transform=transform)val_dataset = CustomDataset(val_data_path, transform=transform)# 创建数据加载器batch_size = 32train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)val_loader = DataLoader(val_dataset, batch_size=batch_size)# 创建模型实例num_classes = len(train_dataset.dataset.classes)model = CNNModel(num_classes)# 计算参数数量num_params = sum(p.numel() for p in model.parameters())print(&quot;模型参数数量：&quot;, num_params)# 定义损失函数和优化器criterion = nn.CrossEntropyLoss()optimizer = optim.Adam(model.parameters(), lr=0.001)# 设置训练参数num_epochs = 2000check_point = 10 #m每10个epoch验证一回device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)model.to(device)# 开始训练for epoch in range(num_epochs): model.train() running_loss = 0.0 correct_predictions = 0 for images, labels in train_loader: images = images.to(device) labels = labels.to(device) optimizer.zero_grad() outputs = model(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() _, predicted = torch.max(outputs.data, 1) correct_predictions += (predicted == labels).sum().item() running_loss += loss.item() epoch_accuracy = correct_predictions / len(train_dataset) epoch_loss = running_loss / len(train_loader) print(f&quot;Epoch [&#123;epoch+1&#125;/&#123;num_epochs&#125;], Loss: &#123;epoch_loss:.4f&#125;, Accuracy: &#123;epoch_accuracy:.4f&#125;&quot;) if epoch%check_point == 0 : # 在验证集上进行评估 model.eval() total_correct = 0 total_samples = 0 with torch.no_grad(): for images, labels in val_loader: images = images.to(device) labels = labels.to(device) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total_samples += labels.size(0) total_correct += (predicted == labels).sum().item() val_accuracy = total_correct / total_samples print(f&quot;Validation Accuracy: &#123;val_accuracy:.4f&#125;&quot;)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-分类","slug":"Code-分类","permalink":"http://example.com/tags/Code-%E5%88%86%E7%B1%BB/"}]},{"title":"Coding-条件Unet-基于注意力机制","slug":"coding_c_Unet","date":"2024-01-02T07:35:07.623Z","updated":"2024-11-29T11:58:21.351Z","comments":true,"path":"2024/01/02/coding_c_Unet/","link":"","permalink":"http://example.com/2024/01/02/coding_c_Unet/","excerpt":"","text":"摘要 unet.py test.py 摘要 可将时间条件和类别条件引入模型，共两个文件：unet.py,test.py,模型为 b=Unet(t,c,a)b=Unet(t,c,a) b=Unet(t,c,a) 其中bbb是输出，ttt是时间条件，ccc是类别条件，aaa是输入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386import mathimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.nn.modules.normalization import GroupNormdef get_norm(norm, num_channels, num_groups): if norm == &quot;in&quot;: return nn.InstanceNorm2d(num_channels, affine=True) elif norm == &quot;bn&quot;: return nn.BatchNorm2d(num_channels) elif norm == &quot;gn&quot;: return nn.GroupNorm(num_groups, num_channels) elif norm is None: return nn.Identity() else: raise ValueError(&quot;unknown normalization type&quot;)class PositionalEmbedding(nn.Module): __doc__ = r&quot;&quot;&quot;Computes a positional embedding of timesteps. Input: x: tensor of shape (N) Output: tensor of shape (N, dim) Args: dim (int): embedding dimension scale (float): linear scale to be applied to timesteps. Default: 1.0 &quot;&quot;&quot; def __init__(self, dim, scale=1.0): super().__init__() assert dim % 2 == 0 self.dim = dim self.scale = scale def forward(self, x): device = x.device half_dim = self.dim // 2 emb = math.log(10000) / half_dim emb = torch.exp(torch.arange(half_dim, device=device) * -emb) emb = torch.outer(x * self.scale, emb) emb = torch.cat((emb.sin(), emb.cos()), dim=-1) return embclass Downsample(nn.Module): __doc__ = r&quot;&quot;&quot;Downsamples a given tensor by a factor of 2. Uses strided convolution. Assumes even height and width. Input: x: tensor of shape (N, in_channels, H, W) time_emb: ignored y: ignored Output: tensor of shape (N, in_channels, H // 2, W // 2) Args: in_channels (int): number of input channels &quot;&quot;&quot; def __init__(self, in_channels): super().__init__() self.downsample = nn.Conv2d(in_channels, in_channels, 3, stride=2, padding=1) def forward(self, x, time_emb, y): if x.shape[2] % 2 == 1: raise ValueError(&quot;downsampling tensor height should be even&quot;) if x.shape[3] % 2 == 1: raise ValueError(&quot;downsampling tensor width should be even&quot;) return self.downsample(x)class Upsample(nn.Module): __doc__ = r&quot;&quot;&quot;Upsamples a given tensor by a factor of 2. Uses resize convolution to avoid checkerboard artifacts. Input: x: tensor of shape (N, in_channels, H, W) time_emb: ignored y: ignored Output: tensor of shape (N, in_channels, H * 2, W * 2) Args: in_channels (int): number of input channels &quot;&quot;&quot; def __init__(self, in_channels): super().__init__() self.upsample = nn.Sequential( nn.Upsample(scale_factor=2, mode=&quot;nearest&quot;), nn.Conv2d(in_channels, in_channels, 3, padding=1), ) def forward(self, x, time_emb, y): return self.upsample(x)class AttentionBlock(nn.Module): __doc__ = r&quot;&quot;&quot;Applies QKV self-attention with a residual connection. Input: x: tensor of shape (N, in_channels, H, W) norm (string or None): which normalization to use (instance, group, batch, or none). Default: &quot;gn&quot; num_groups (int): number of groups used in group normalization. Default: 32 Output: tensor of shape (N, in_channels, H, W) Args: in_channels (int): number of input channels &quot;&quot;&quot; def __init__(self, in_channels, norm=&quot;gn&quot;, num_groups=32): super().__init__() self.in_channels = in_channels self.norm = get_norm(norm, in_channels, num_groups) self.to_qkv = nn.Conv2d(in_channels, in_channels * 3, 1) self.to_out = nn.Conv2d(in_channels, in_channels, 1) def forward(self, x): b, c, h, w = x.shape q, k, v = torch.split(self.to_qkv(self.norm(x)), self.in_channels, dim=1) q = q.permute(0, 2, 3, 1).view(b, h * w, c) k = k.view(b, c, h * w) v = v.permute(0, 2, 3, 1).view(b, h * w, c) dot_products = torch.bmm(q, k) * (c ** (-0.5)) assert dot_products.shape == (b, h * w, h * w) attention = torch.softmax(dot_products, dim=-1) out = torch.bmm(attention, v) assert out.shape == (b, h * w, c) out = out.view(b, h, w, c).permute(0, 3, 1, 2) return self.to_out(out) + xclass ResidualBlock(nn.Module): __doc__ = r&quot;&quot;&quot;Applies two conv blocks with resudual connection. Adds time and class conditioning by adding bias after first convolution. Input: x: tensor of shape (N, in_channels, H, W) time_emb: time embedding tensor of shape (N, time_emb_dim) or None if the block doesn&#x27;t use time conditioning y: classes tensor of shape (N) or None if the block doesn&#x27;t use class conditioning Output: tensor of shape (N, out_channels, H, W) Args: in_channels (int): number of input channels out_channels (int): number of output channels time_emb_dim (int or None): time embedding dimension or None if the block doesn&#x27;t use time conditioning. Default: None num_classes (int or None): number of classes or None if the block doesn&#x27;t use class conditioning. Default: None activation (function): activation function. Default: torch.nn.functional.relu norm (string or None): which normalization to use (instance, group, batch, or none). Default: &quot;gn&quot; num_groups (int): number of groups used in group normalization. Default: 32 use_attention (bool): if True applies AttentionBlock to the output. Default: False &quot;&quot;&quot; def __init__( self, in_channels, out_channels, dropout, time_emb_dim=None, num_classes=None, activation=F.relu, norm=&quot;gn&quot;, num_groups=32, use_attention=False, ): super().__init__() self.activation = activation self.norm_1 = get_norm(norm, in_channels, num_groups) self.conv_1 = nn.Conv2d(in_channels, out_channels, 3, padding=1) self.norm_2 = get_norm(norm, out_channels, num_groups) self.conv_2 = nn.Sequential( nn.Dropout(p=dropout), nn.Conv2d(out_channels, out_channels, 3, padding=1), ) self.time_bias = nn.Linear(time_emb_dim, out_channels) if time_emb_dim is not None else None self.class_bias = nn.Embedding(num_classes, out_channels) if num_classes is not None else None self.residual_connection = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity() self.attention = nn.Identity() if not use_attention else AttentionBlock(out_channels, norm, num_groups) def forward(self, x, time_emb=None, y=None): out = self.activation(self.norm_1(x)) out = self.conv_1(out) if self.time_bias is not None: if time_emb is None: raise ValueError(&quot;time conditioning was specified but time_emb is not passed&quot;) out += self.time_bias(self.activation(time_emb))[:, :, None, None] if self.class_bias is not None: if y is None: raise ValueError(&quot;class conditioning was specified but y is not passed&quot;) out += self.class_bias(y)[:, :, None, None] out = self.activation(self.norm_2(out)) out = self.conv_2(out) + self.residual_connection(x) out = self.attention(out) return outclass UNet(nn.Module): __doc__ = &quot;&quot;&quot;UNet model used to estimate noise. Input: x: tensor of shape (N, in_channels, H, W) time_emb: time embedding tensor of shape (N, time_emb_dim) or None if the block doesn&#x27;t use time conditioning y: classes tensor of shape (N) or None if the block doesn&#x27;t use class conditioning Output: tensor of shape (N, out_channels, H, W) Args: img_channels (int): number of image channels base_channels (int): number of base channels (after first convolution) channel_mults (tuple): tuple of channel multiplers. Default: (1, 2, 4, 8) time_emb_dim (int or None): time embedding dimension or None if the block doesn&#x27;t use time conditioning. Default: None time_emb_scale (float): linear scale to be applied to timesteps. Default: 1.0 num_classes (int or None): number of classes or None if the block doesn&#x27;t use class conditioning. Default: None activation (function): activation function. Default: torch.nn.functional.relu dropout (float): dropout rate at the end of each residual block attention_resolutions (tuple): list of relative resolutions at which to apply attention. Default: () norm (string or None): which normalization to use (instance, group, batch, or none). Default: &quot;gn&quot; num_groups (int): number of groups used in group normalization. Default: 32 initial_pad (int): initial padding applied to image. Should be used if height or width is not a power of 2. Default: 0 &quot;&quot;&quot; def __init__( self, img_channels, base_channels, channel_mults=(1, 2, 4, 8), num_res_blocks=2, time_emb_dim=None, time_emb_scale=1.0, num_classes=None, activation=F.relu, dropout=0.1, attention_resolutions=(), norm=&quot;gn&quot;, num_groups=32, initial_pad=0, ): super().__init__() self.activation = activation self.initial_pad = initial_pad self.num_classes = num_classes self.time_mlp = nn.Sequential( PositionalEmbedding(base_channels, time_emb_scale), nn.Linear(base_channels, time_emb_dim), nn.SiLU(), nn.Linear(time_emb_dim, time_emb_dim), ) if time_emb_dim is not None else None self.init_conv = nn.Conv2d(img_channels, base_channels, 3, padding=1) self.downs = nn.ModuleList() self.ups = nn.ModuleList() channels = [base_channels] now_channels = base_channels for i, mult in enumerate(channel_mults): out_channels = base_channels * mult for _ in range(num_res_blocks): self.downs.append(ResidualBlock( now_channels, out_channels, dropout, time_emb_dim=time_emb_dim, num_classes=num_classes, activation=activation, norm=norm, num_groups=num_groups, use_attention=i in attention_resolutions, )) now_channels = out_channels channels.append(now_channels) if i != len(channel_mults) - 1: self.downs.append(Downsample(now_channels)) channels.append(now_channels) self.mid = nn.ModuleList([ ResidualBlock( now_channels, now_channels, dropout, time_emb_dim=time_emb_dim, num_classes=num_classes, activation=activation, norm=norm, num_groups=num_groups, use_attention=True, ), ResidualBlock( now_channels, now_channels, dropout, time_emb_dim=time_emb_dim, num_classes=num_classes, activation=activation, norm=norm, num_groups=num_groups, use_attention=False, ), ]) for i, mult in reversed(list(enumerate(channel_mults))): out_channels = base_channels * mult for _ in range(num_res_blocks + 1): self.ups.append(ResidualBlock( channels.pop() + now_channels, out_channels, dropout, time_emb_dim=time_emb_dim, num_classes=num_classes, activation=activation, norm=norm, num_groups=num_groups, use_attention=i in attention_resolutions, )) now_channels = out_channels if i != 0: self.ups.append(Upsample(now_channels)) assert len(channels) == 0 self.out_norm = get_norm(norm, base_channels, num_groups) self.out_conv = nn.Conv2d(base_channels, img_channels, 3, padding=1) def forward(self, x, time=None, y=None): ip = self.initial_pad if ip != 0: x = F.pad(x, (ip,) * 4) if self.time_mlp is not None: if time is None: raise ValueError(&quot;time conditioning was specified but tim is not passed&quot;) time_emb = self.time_mlp(time) else: time_emb = None if self.num_classes is not None and y is None: raise ValueError(&quot;class conditioning was specified but y is not passed&quot;) x = self.init_conv(x) skips = [x] for layer in self.downs: x = layer(x, time_emb, y) skips.append(x) for layer in self.mid: x = layer(x, time_emb, y) for layer in self.ups: if isinstance(layer, ResidualBlock): x = torch.cat([x, skips.pop()], dim=1) x = layer(x, time_emb, y) x = self.activation(self.out_norm(x)) x = self.out_conv(x) if self.initial_pad != 0: return x[:, :, ip:-ip, ip:-ip] else: return x 123456789101112131415161718192021222324252627282930313233from unet import *import torch.nn.functional as Fif __name__==&quot;__main__&quot;: activations = &#123; &quot;relu&quot;: F.relu, &quot;mish&quot;: F.mish, &quot;silu&quot;: F.silu, &#125; model = UNet( img_channels=3, base_channels=64, channel_mults=(1, 2, 2, 2), time_emb_dim=512, norm=&#x27;gn&#x27;, dropout=0.1, activation=activations[&#x27;silu&#x27;], attention_resolutions=(1,), num_classes=10, initial_pad=0, ) a = torch.FloatTensor(1,3,32,32) t = torch.randn(1).long() y = torch.randn(1).long() b = model(a,t,y) pass","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-模型库","slug":"Code-模型库","permalink":"http://example.com/tags/Code-%E6%A8%A1%E5%9E%8B%E5%BA%93/"}]},{"title":"Coding-3D医学图像分类-基于3D-Resnet","slug":"coding_medical_network_example","date":"2023-12-26T08:18:48.892Z","updated":"2024-01-23T03:54:18.379Z","comments":true,"path":"2023/12/26/coding_medical_network_example/","link":"","permalink":"http://example.com/2023/12/26/coding_medical_network_example/","excerpt":"","text":"摘要 main.py 摘要 训练一个3D医学图像分类神经网络（3D-Resnet），包括： 1.自定义dataload制作 2.网络定义(3D-Resnet) 3.训练过程 4.测试过程 5.模型评估（准确率） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225import torchimport torch.nn as nnimport numpy as npfrom torch.utils.data import Datasetimport osclass mydatasets(Dataset): def __init__(self, data,label): self.data = data # 加上通道数 self.label = label def __getitem__(self, index): data = self.data[index] # 获取高阶FCN label = self.label[index] return data,label def __len__(self): return self.data.shape[0] # 返回数据集的长度class BasicBlock(nn.Module): expansion = 1 def __init__(self, in_channels, out_channels, stride=1): super(BasicBlock, self).__init__() # 第一个卷积层 self.conv1 = nn.Conv3d( in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False ) self.bn1 = nn.BatchNorm3d(out_channels) self.relu = nn.ReLU(inplace=True) # 第二个卷积层 self.conv2 = nn.Conv3d( out_channels, out_channels * self.expansion, kernel_size=3, stride=1, padding=1, bias=False, ) self.bn2 = nn.BatchNorm3d(out_channels * self.expansion) # 残差连接（shortcut connection） self.shortcut = nn.Sequential() if stride != 1 or in_channels != out_channels * self.expansion: self.shortcut = nn.Sequential( nn.Conv3d( in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False, ), nn.BatchNorm3d(out_channels * self.expansion), ) def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out += self.shortcut(residual) out = self.relu(out) return outclass ResNet(nn.Module): def __init__(self, block, num_blocks, num_classes=10): super(ResNet, self).__init__() self.in_channels = 64 # 第一个卷积层 self.conv1 = nn.Conv3d( 1, 64, kernel_size=3, stride=1, padding=1, bias=False ) self.bn1 = nn.BatchNorm3d(64) self.relu = nn.ReLU(inplace=True) # ResNet的四个阶段 self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1) self.layer2 = self.make_layer(block, 128, num_blocks[1], stride=2) self.layer3 = self.make_layer(block, 256, num_blocks[2], stride=2) self.layer4 = self.make_layer(block, 512, num_blocks[3], stride=2) # 全局平均池化层和全连接层 self.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1)) self.fc = nn.Linear(512 * block.expansion, num_classes) def make_layer(self, block, out_channels, num_blocks, stride): layers = [] layers.append(block(self.in_channels, out_channels, stride)) self.in_channels = out_channels * block.expansion for _ in range(1, num_blocks): layers.append(block(self.in_channels, out_channels)) return nn.Sequential(*layers) def forward(self, x): out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.layer1(out) out = self.layer2(out) out = self.layer3(out) out = self.layer4(out) out = self.avg_pool(out) out = torch.flatten(out, 1) out = self.fc(out) return outdef ResNet18(num_classes=10): return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)def data_load(data_path,batch_size): data_label = np.load(data_path,batch_size) torch.manual_seed(9) random_index = np.random.permutation(data_label[&#x27;data&#x27;].shape[0]) data = torch.from_numpy(data_label[&#x27;data&#x27;][random_index]).float() label= torch.from_numpy(data_label[&#x27;label&#x27;][random_index]).float() train_data = data[:160] train_label = label[:160] test_data = data[160:] test_label = label[160:] train_dataset = mydatasets(train_data,train_label) test_dataset = mydatasets(test_data,test_label) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True) # 创建训练数据加载器 test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False) # 创建测试数据加载器 return train_loader,test_loaderdef train(model, train_loader, criterion, optimizer, device): model.train() # 设置模型为训练模式 train_loss = 0 for data, label in train_loader: data = data.to(device) data = torch.unsqueeze(data,1) optimizer.zero_grad() # 清除梯度 output = model(data) # 前向传播 loss = criterion(output, label.to(device).long()) # 计算损失 loss.backward() # 反向传播，计算梯度 optimizer.step() # 更新模型参数 train_loss += loss.item() * data.size(0) train_loss /= len(train_loader.dataset) # 计算平均训练损失 return train_lossdef validate(model, val_loader, criterion, device): model.eval() # 设置模型为评估模式 val_loss = 0 correct = 0 #正确个数 total = 0 #总数 with torch.no_grad(): for data, label in val_loader: data = data.to(device) data = torch.unsqueeze(data,1) output = model(data) # 前向传播 _, predicted = torch.max(output.data, 1) total += label.size(0) correct += (predicted == label.to(device)).sum().item() loss = criterion(output, label.to(device).long()) # 计算损失 val_loss += loss.item() * data.size(0) accuracy = 100 * correct / total # print(&#x27;Accuracy on the test set: %d %%&#x27; % accuracy) val_loss /= len(val_loader.dataset) # 计算平均验证损失 return accuracy,val_loss if __name__ == &quot;__main__&quot;: epoch_times = 100 batch_size = 4 device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;) train_loader,test_loader = data_load(&#x27;/home/yeshixin/work/newwork/DDPM-main/data/mri_ad90_cn113_data_label_normal.npz&#x27;,batch_size) model = ResNet18(2) model.to(device) # criterion = nn.MSELoss() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(),lr=0.001) train_losses = [] val_losses = [] best_val_loss = np.inf best_val_acc = 0 if not os.path.exists(&#x27;ckpt&#x27;): os.mkdir(&#x27;./ckpt&#x27;) # 训练模型 for epoch in range(epoch_times): train_loss = train(model, train_loader, criterion, optimizer, device) # 训练模型 val_acc,val_loss = validate(model, test_loader, criterion, device) # 验证模型 train_losses.append(train_loss) # 保存训练损失 val_losses.append(val_loss) # 保存验证损失 # 存储最小损失模型 if val_loss &lt; best_val_loss: best_val_loss = val_loss best_model = model.state_dict() torch.save(best_model, &#x27;ckpt/BestLoss_&#x27;+str(best_val_loss)+&#x27;_model.ckpt&#x27;) # 保存最佳模型参数 print(&quot;best_val_loss: &quot; + str(best_val_loss)) with open(&quot;ckpt/model_loss.txt&quot;, &quot;w&quot;) as f: f.write(str(val_loss)) # 存储最大准确率模型 if val_acc &gt; best_val_acc: best_val_acc = val_acc best_model = model.state_dict() torch.save(best_model, &#x27;ckpt/BestAcc_&#x27;+str(best_val_acc)+&#x27;_model.ckpt&#x27;) # 保存最佳模型参数 print(&quot;best_val_acc: &quot; + str(best_val_acc)) with open(&quot;ckpt/model_acc.txt&quot;, &quot;w&quot;) as f: f.write(str(best_val_acc)) print(&#x27;Epoch [&#123;&#125;/&#123;&#125;], Train Loss: &#123;:.4f&#125;, Val Loss: &#123;:.4f&#125;, Val Acc: &#123;:.4f&#125; %&#x27;.format(epoch+1, epoch_times, train_loss, val_loss,val_acc))","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-分类","slug":"Code-分类","permalink":"http://example.com/tags/Code-%E5%88%86%E7%B1%BB/"}]},{"title":"Coding-CGAN-条件生成对抗网络-基于高阶FCN数据","slug":"coding_GAN_High_Fcn","date":"2023-12-21T16:09:25.314Z","updated":"2024-05-21T12:15:06.554Z","comments":true,"path":"2023/12/22/coding_GAN_High_Fcn/","link":"","permalink":"http://example.com/2023/12/22/coding_GAN_High_Fcn/","excerpt":"","text":"摘要 main.py generate.py 摘要 main.py包括高阶FCN的处理，生成对抗网络的训练 generate.py使用生成器生成样本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299import argparseimport osimport numpy as npimport mathimport torchvision.transforms as transformsfrom torchvision.utils import save_imagefrom torch.utils.data import DataLoaderfrom torchvision import datasetsfrom torch.autograd import Variableimport torch.nn as nnimport torch.nn.functional as Fimport torchfrom scipy.io import loadmatfrom torch.utils import data # 导入PyTorch数据工具模块import randomfrom scipy.stats import pearsonros.makedirs(&quot;train_images&quot;, exist_ok=True)os.makedirs(&quot;test_images&quot;, exist_ok=True)os.makedirs(&quot;save_model&quot;, exist_ok=True)parser = argparse.ArgumentParser()parser.add_argument(&quot;--n_epochs&quot;, type=int, default=200, help=&quot;number of epochs of training&quot;)parser.add_argument(&quot;--batch_size&quot;, type=int, default=4, help=&quot;size of the batches&quot;)parser.add_argument(&quot;--lr&quot;, type=float, default=0.0002, help=&quot;adam: learning rate&quot;)parser.add_argument(&quot;--b1&quot;, type=float, default=0.5, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--b2&quot;, type=float, default=0.999, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--n_cpu&quot;, type=int, default=8, help=&quot;number of cpu threads to use during batch generation&quot;)parser.add_argument(&quot;--latent_dim&quot;, type=int, default=100, help=&quot;噪声的维度&quot;)parser.add_argument(&quot;--n_classes&quot;, type=int, default=2, help=&quot;类别数量&quot;)parser.add_argument(&quot;--img_size&quot;, type=int, default=116, help=&quot;功能连接矩阵的维度&quot;)parser.add_argument(&quot;--channels&quot;, type=int, default=1, help=&quot;矩阵通道&quot;)parser.add_argument(&quot;--sample_interval&quot;, type=int, default=400, help=&quot;interval between image sampling&quot;)opt = parser.parse_args()print(opt)img_shape = (opt.channels, opt.img_size, opt.img_size)cuda = True if torch.cuda.is_available() else Falseclass my_dataset(data.Dataset): def __init__(self, Hig_X,label_): self.Hig_X = np.expand_dims(Hig_X,1) # 加上通道数 self.label = label_ def __getitem__(self, index): X = self.Hig_X[index] # 获取高阶FCN label = self.label[index] return X,label def __len__(self): return self.Hig_X.shape[0] # 返回数据集的长度def calculate_similarity(matrix1, matrix2): correlation, _ = pearsonr(matrix1.flatten(), matrix2.flatten()) return correlation# 精度判断，即计算两个矩阵的相关系数def calculate_accuracy(generator, dataloader, device, val_subjects=None): generator.eval() correct_predictions = 0 for batch_idx, (batch_matrix) in enumerate(dataloader): matrix = batch_matrix.to(device) size = matrix.size(0) random_noise = torch.randn(size, 100).to(device=device) with torch.no_grad(): gen_matrix = generator(random_noise) for i in range(size): similarity = calculate_similarity(gen_matrix[i].cpu().numpy(), matrix[i].cpu().numpy()) # 阈值是一个经验值，根据实际情况调整 if similarity &gt; 0.6 and (val_subjects is None or batch_idx in val_subjects): correct_predictions += 1 accuracy = correct_predictions / len(dataloader.dataset) return accuracyclass Generator(nn.Module): def __init__(self): super(Generator, self).__init__() self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes) def block(in_feat, out_feat, normalize=True): layers = [nn.Linear(in_feat, out_feat)] if normalize: layers.append(nn.BatchNorm1d(out_feat, 0.8)) layers.append(nn.LeakyReLU(0.2, inplace=True)) return layers self.model = nn.Sequential( *block(opt.latent_dim + opt.n_classes, 128, normalize=False), *block(128, 256), *block(256, 512), *block(512, 1024), nn.Linear(1024, int(np.prod(img_shape))), nn.Tanh() ) def forward(self, noise, labels): # Concatenate label embedding and image to produce input gen_input = torch.cat((self.label_emb(labels), noise), -1) img = self.model(gen_input) img = img.view(img.size(0), *img_shape) return imgclass Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes) self.model = nn.Sequential( nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 512), nn.Dropout(0.4), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 512), nn.Dropout(0.4), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 1), ) def forward(self, img, labels): # Concatenate label embedding and image to produce input d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1) validity = self.model(d_in) return validity# Loss functionsadversarial_loss = torch.nn.MSELoss()# Initialize generator and discriminatorgenerator = Generator()discriminator = Discriminator()if cuda: generator.cuda() discriminator.cuda() adversarial_loss.cuda()# Configure data loader # 原始文件路径fMRI_file_path = &#x27;.//ROISignals_insomnia_aal116.mat&#x27;# 加载数据fMRI_data = loadmat(fMRI_file_path)[&#x27;ROISignals&#x27;]# 正常人为1,病人为0fMRI_label = torch.cat((torch.ones(32,1),torch.zeros(30,1)),dim=0).squeeze()# 低阶矩阵计算Low_X_ = []for i in range(fMRI_data.shape[2]): temp = np.corrcoef(fMRI_data[:,:,i],rowvar=False) Low_X_.append(temp)Low_X = np.array(Low_X_) #(62,116,116)# 高阶矩阵计算Hig_X_ = []for i in range(Low_X.shape[0]): temp = np.corrcoef(Low_X[:,:,i],rowvar=False) Hig_X_.append(temp)Hig_X = np.array(Hig_X_) #(62,116,116)random_index = np.random.permutation(len(fMRI_label))Hig_X = Hig_X[random_index]fMRI_label = fMRI_label[random_index]train_data = Hig_X[:50]train_label = fMRI_label[:50]test_data = Hig_X[50:]test_label = fMRI_label[:50]train_dataset = my_dataset(train_data,train_label)test_dataset = my_dataset(test_data,test_label)train_loader = data.DataLoader(train_dataset,batch_size=opt.batch_size,shuffle=True)test_loader = data.DataLoader(test_dataset,batch_size=opt.batch_size,shuffle=False)# Optimizersoptimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensorLongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor# ----------# Training# ----------for epoch in range(opt.n_epochs): for i, (imgs, labels) in enumerate(train_loader): batch_size = imgs.shape[0] # Adversarial ground truths valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False) fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False) # Configure input real_imgs = Variable(imgs.type(FloatTensor)) labels = Variable(labels.type(LongTensor)) # ----------------- # Train Generator # ----------------- optimizer_G.zero_grad() # Sample noise and labels as generator input z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim)))) gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size))) # Generate a batch of images gen_imgs = generator(z, gen_labels) # Loss measures generator&#x27;s ability to fool the discriminator validity = discriminator(gen_imgs, gen_labels) g_loss = adversarial_loss(validity, valid) g_loss.backward() optimizer_G.step() # --------------------- # Train Discriminator # --------------------- optimizer_D.zero_grad() # Loss for real images validity_real = discriminator(real_imgs, labels) d_real_loss = adversarial_loss(validity_real, valid) # Loss for fake images validity_fake = discriminator(gen_imgs.detach(), gen_labels) d_fake_loss = adversarial_loss(validity_fake, fake) # Total discriminator loss d_loss = (d_real_loss + d_fake_loss) / 2 d_loss.backward() optimizer_D.step() print( &quot;[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]&quot; % (epoch, opt.n_epochs, i, len(train_loader), d_loss.item(), g_loss.item()) ) batches_done = epoch * len(train_loader) + i # 存储训练过程的结果 if batches_done % opt.sample_interval == 0: #一个epoch中每隔多少间隔保存一次 gen_imgs_normalized = (gen_imgs - gen_imgs.min()) / (gen_imgs.max() - gen_imgs.min()) save_image(gen_imgs_normalized.data[:25], &quot;train_images/%d.png&quot; % batches_done, nrow=5, normalize=False) # 测试过程 with torch.no_grad(): best_acc = 0 generator.eval() correct_predictions = 0 for imgs, label in test_loader: z = Variable(FloatTensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim)))) gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, imgs.shape[0]))) gen_imgs = generator(z, gen_labels) for i in range(imgs.shape[0]): similarity = calculate_similarity(gen_imgs[i].cpu().numpy(), imgs[i].cpu().numpy()) # 阈值是一个经验值，根据实际情况调整 if similarity &gt; 0.6: correct_predictions += 1 accuracy = correct_predictions / len(test_loader.dataset) # 保存epoch中精度最好的模型 if best_acc &lt; accuracy: best_acc = accuracy #保存模型 torch.save(generator.state_dict(), &quot;save_model/best_model.pth&quot;) print(&#x27;epoch:&#x27;+str(epoch)+&#x27; 测试集acc:&#x27;+str(accuracy)+&quot; best_acc:&quot;+str(best_acc)) gen_imgs_normalized = (gen_imgs - gen_imgs.min()) / (gen_imgs.max() - gen_imgs.min()) save_image(gen_imgs_normalized.data[:25], &quot;test_images/acc_&#123;&#125;epoch_&#123;&#125;.png&quot;.format(accuracy,epoch), nrow=5, normalize=False) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import argparseimport torch.nn as nnimport torch.nn.functional as Fimport torchimport numpy as npfrom torch.autograd import Variablefrom torchvision.utils import save_imageimport osparser = argparse.ArgumentParser()parser.add_argument(&quot;--n_epochs&quot;, type=int, default=200, help=&quot;number of epochs of training&quot;)parser.add_argument(&quot;--batch_size&quot;, type=int, default=4, help=&quot;size of the batches&quot;)parser.add_argument(&quot;--lr&quot;, type=float, default=0.0002, help=&quot;adam: learning rate&quot;)parser.add_argument(&quot;--b1&quot;, type=float, default=0.5, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--b2&quot;, type=float, default=0.999, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--n_cpu&quot;, type=int, default=8, help=&quot;number of cpu threads to use during batch generation&quot;)parser.add_argument(&quot;--latent_dim&quot;, type=int, default=100, help=&quot;噪声的维度&quot;)parser.add_argument(&quot;--n_classes&quot;, type=int, default=2, help=&quot;类别数量&quot;)parser.add_argument(&quot;--img_size&quot;, type=int, default=116, help=&quot;功能连接矩阵的维度&quot;)parser.add_argument(&quot;--channels&quot;, type=int, default=1, help=&quot;矩阵通道&quot;)parser.add_argument(&quot;--sample_interval&quot;, type=int, default=400, help=&quot;interval between image sampling&quot;)opt = parser.parse_args()print(opt)img_shape = (opt.channels, opt.img_size, opt.img_size)cuda = True if torch.cuda.is_available() else Falseclass Generator(nn.Module): def __init__(self): super(Generator, self).__init__() self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes) def block(in_feat, out_feat, normalize=True): layers = [nn.Linear(in_feat, out_feat)] if normalize: layers.append(nn.BatchNorm1d(out_feat, 0.8)) layers.append(nn.LeakyReLU(0.2, inplace=True)) return layers self.model = nn.Sequential( *block(opt.latent_dim + opt.n_classes, 128, normalize=False), *block(128, 256), *block(256, 512), *block(512, 1024), nn.Linear(1024, int(np.prod(img_shape))), nn.Tanh() ) def forward(self, noise, labels): # Concatenate label embedding and image to produce input gen_input = torch.cat((self.label_emb(labels), noise), -1) img = self.model(gen_input) img = img.view(img.size(0), *img_shape) return img FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensorLongTensor = torch.cuda.LongTensor if cuda else torch.LongTensoros.makedirs(&quot;Generated_samples&quot;, exist_ok=True)generator = Generator()generator.cuda()# 指定保存的模型文件路径model_path = &#x27;save_model\\\\best_model.pth&#x27;# 加载保存的模型状态字典generator.load_state_dict(torch.load(model_path))# 将模型设置为评估模式generator.eval()# 设置生成样本数batch_size = 25z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size)))gen_imgs = generator(z , gen_labels)# 标准化gen_imgs_normalized = (gen_imgs - gen_imgs.min()) / (gen_imgs.max() - gen_imgs.min()) # 存储样本和标签np.savez(&#x27;Generated_samples/Generated_samples.npz&#x27;, array1=gen_imgs.detach().cpu(), array2=gen_labels.detach().cpu())#存储图片save_image(gen_imgs_normalized.data[:25], &quot;Generated_samples/acc_epoch_.png&quot;, nrow=25, normalize=False)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-生成","slug":"Code-生成","permalink":"http://example.com/tags/Code-%E7%94%9F%E6%88%90/"}]},{"title":"PaperReading-基于对抗训练和不确定性校正的一次性创伤脑分割","slug":"paper_基于对抗训练和不确定性校正的一次性创伤脑分割","date":"2023-10-28T10:05:44.012Z","updated":"2024-05-21T12:18:54.118Z","comments":true,"path":"2023/10/28/paper_基于对抗训练和不确定性校正的一次性创伤脑分割/","link":"","permalink":"http://example.com/2023/10/28/paper_%E5%9F%BA%E4%BA%8E%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83%E5%92%8C%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E6%A0%A1%E6%AD%A3%E7%9A%84%E4%B8%80%E6%AC%A1%E6%80%A7%E5%88%9B%E4%BC%A4%E8%84%91%E5%88%86%E5%89%B2/","excerpt":"","text":"One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification 1.问题总结 基于学习变换的单次分割方法存在以下问题:(1)增强样本的多样性受限。(2)学习变换引入了潜在的错误标签 2.解决方法 (1)通过增强样本的对抗性分布来提高增强数据的多样性和分割的鲁棒性。(2)根据分割过程中的不确定性，对学习变换带来的潜在标签误差进行校正。 3.总结 在这项工作中，我们提出了一种新的一次性分割方法，用于严重创伤性脑分割，这是一种困难的临床场景，可用的注释数据有限。我们的方法解决了sTBI脑分割中的关键问题，即需要多样化的训练数据和减少由外观变换引入的潜在标签错误。对抗训练的引入增强了数据的多样性和分割的鲁棒性，同时设计了不确定性校正来补偿潜在的标签错误。 在sTBI脑上的实验结果证明了我们提出的方法的有效性及其相对于最先进的替代方法的优势，突出了我们的方法在实现更准确的严重创伤脑分割方面的潜力，这可能有助于临床管道。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Paper-医学影像","slug":"Paper-医学影像","permalink":"http://example.com/tags/Paper-%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F/"}]},{"title":"PaperReading-使用学习变换的数据增强，用于一次性医学图像分割","slug":"paper_使用学习变换的数据增强_用于一次性医学图像分割","date":"2023-10-28T10:05:34.560Z","updated":"2024-05-21T12:19:32.013Z","comments":true,"path":"2023/10/28/paper_使用学习变换的数据增强_用于一次性医学图像分割/","link":"","permalink":"http://example.com/2023/10/28/paper_%E4%BD%BF%E7%94%A8%E5%AD%A6%E4%B9%A0%E5%8F%98%E6%8D%A2%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA_%E7%94%A8%E4%BA%8E%E4%B8%80%E6%AC%A1%E6%80%A7%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/","excerpt":"","text":"Data augmentation using learned transformations for one-shot medical image segmentation 1.问题总结 图像分割是医学应用中的重要任务。基于卷积网络的方法已经实现了非常好的效果。但是它们通常依赖于大量标记的数据集。标记医学图像往往需要大量的专业知识和时间，并且用于数据增强的典型手动调整方法无法捕获此类图像中复杂变化。 2.解决方法 通过学习合成多样的现实的标签实例来解决有限的标签数据问题。我们的数据增强新方法利用了没有标签的图像。使用基于学习的配准方法，对数据集中的图像之间的空间和外观变换集进行建模。这些模型捕获了未标记图像之间的解剖和图像多样性。我们合成新的实例通过采样变换并把他们应用到单个有标签的实例上。 3.总结 提出了一个基于学习的数据增强方法，并且在一次医学图像分割中进行了演示。 我们从一个带标签的图像和一些未带标签的图像开始。使用基于学习的配准方法，我们在带标签和未带标签的图像之间建模空间和外观转换集。这些转换可以捕获诸如非线性变形和图像强度的变换。我们合成新的标签通过采样转换并且并将其应用于标签，产生各种逼真的新图像。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Paper-医学影像","slug":"Paper-医学影像","permalink":"http://example.com/tags/Paper-%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F/"}]},{"title":"C++基础知识","slug":"cpp_tips","date":"2023-10-04T03:08:19.430Z","updated":"2024-05-22T02:28:06.759Z","comments":true,"path":"2023/10/04/cpp_tips/","link":"","permalink":"http://example.com/2023/10/04/cpp_tips/","excerpt":"","text":"虚析构函数 在 C++ 中，虚析构函数（virtual destructor）是在基类中声明并使用 virtual 关键字修饰的析构函数。 虚析构函数在处理基类指针指向派生类对象时非常有用。当使用基类指针来删除一个指向派生类对象的实例时，如果基类的析构函数不是虚拟的，只会调用基类的析构函数，而不会调用派生类的析构函数。这可能导致派生类对象中的资源无法正确释放，造成内存泄漏。 通过将基类的析构函数声明为虚拟的，可以确保在删除基类指针时正确调用派生类的析构函数，从而正确释放派生类对象的资源。下面是一个使用虚析构函数的示例： 1234567891011121314151617181920212223#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;class base&#123;public: virtual ~base()&#123; cout&lt;&lt;&quot;base&#x27;s ~&quot;; &#125;;&#125;;class child:public base&#123; public: ~child()&#123; cout&lt;&lt;&quot;child&#x27;s~&quot;; &#125;&#125;;int main()&#123; base* p = new child(); delete p; return 0;&#125; for循环遍历迭代器 123456789101112#include&lt;iostream&gt;#include&lt;list&gt;int main()&#123; std::list&lt;int&gt; a; a.push_back(1); a.push_back(2); a.push_back(3); for(auto &amp;i:a )&#123; std::cout&lt;&lt;i&lt;&lt;std::endl; &#125; return 0;&#125; cmake初级 文件夹设置 ├── CMakeLists.txt ├── sylar │ ├── CMakeLists.txt │ ├── main.cpp 第一个CMakeLists.txt 1234567cmake_minimum_required(VERSION 3.12)project(sylar)set(CMAKE_CXX_STANDARD 14)add_subdirectory(sylar)# add_executable(sylar sylar/main.cpp) 第二个CMakeLists.txt 1add_executable(sylar main.cpp) 可变参数列表 1234567891011121314151617181920#include &lt;iostream&gt;#include &lt;cstdarg&gt;// 可变参数列表的函数定义void print(int count, ...) &#123; va_list args; va_start(args, count); for (int i = 0; i &lt; count; ++i) &#123; int value = va_arg(args, int); std::cout &lt;&lt; value &lt;&lt; std::endl; &#125; va_end(args);&#125;int main() &#123; print(6,1,7,9,2,3,4); return 0;&#125; 操作符重载 12345678910111213141516171819#include &lt;iostream&gt;// 自定义类 MyClassclass MyClass &#123;public: int data; MyClass(int value) : data(value) &#123;&#125; friend std::ostream&amp; operator&lt;&lt;(std::ostream&amp; os, const MyClass&amp; obj) &#123; os &lt;&lt; &quot;MyClass: &quot; &lt;&lt; obj.data; return os; &#125;&#125;;int main() &#123; MyClass obj(42); std::cout &lt;&lt; obj &lt;&lt; std::endl; return 0;&#125; 单例模式 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;iostream&gt;class Singleton &#123;private: // 私有的静态成员变量，用于存储单例实例 static Singleton* instance; // 私有的构造函数，防止外部直接实例化对象 Singleton() &#123;&#125;public: // 静态成员函数，用于获取单例对象的唯一实例 static Singleton* getInstance() &#123; if (instance == nullptr) &#123; instance = new Singleton(); &#125; return instance; &#125; // 示例成员函数 void showMessage() &#123; std::cout &lt;&lt; &quot;Hello from Singleton!&quot; &lt;&lt; std::endl; &#125;&#125;;// 初始化静态成员变量Singleton* Singleton::instance = nullptr;int main() &#123; Singleton* instance1 = Singleton::getInstance(); Singleton* instance2 = Singleton::getInstance(); // instance1和instance2指向相同的对象 instance1-&gt;showMessage(); // 输出: Hello from Singleton! instance2-&gt;showMessage(); // 输出: Hello from Singleton! // 清理单例对象 delete instance1; instance1 = nullptr; return 0;&#125; 模板类 1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;boost/lexical_cast.hpp&gt;template &lt;class F, class T&gt;class LexicalCast &#123;public: T operator()(const F&amp; v) &#123; return boost::lexical_cast&lt;T&gt;(v); //将T转换为int &#125;&#125;;int main() &#123; LexicalCast&lt;std::string, int&gt; stringToInt; int num = stringToInt(&quot;12345&quot;); std::cout &lt;&lt; &quot;Converted integer: &quot; &lt;&lt; num &lt;&lt; std::endl; LexicalCast&lt;double, std::string&gt; doubleToString; std::string str = doubleToString(3.14159); std::cout &lt;&lt; &quot;Converted string: &quot; &lt;&lt; str &lt;&lt; std::endl; return 0;&#125; static 关键词 在 C++ 头文件（.h）中定义的静态函数，在对应的实现文件（.cpp）中不需要再使用 static 关键词修饰。 当在头文件中声明静态函数时，使用 static 关键词是为了将函数的作用域限制在当前文件中，避免与其他文件中同名的函数产生冲突。这样做是因为头文件通常会被多个源文件包含，如果在头文件中使用 static 修饰符，每个包含该头文件的源文件都会有一个独立的静态函数副本，可能导致链接错误。 而在实现文件中，已经处于单个文件的作用域中，不需要再次使用 static 关键词来限制函数的作用域。实现文件中的静态函数的定义只需要与头文件中的函数声明相匹配即可。 因此，头文件中的静态函数声明不需要再使用 static 关键词修饰，而在对应的实现文件中，只需要提供静态函数的定义即可。这样可以保持函数在整个程序中的唯一性，并正确地与其他文件中的代码进行链接。 在 C++ 中，# 是一种预处理操作符，称为字符串化操作符（stringizing operator）。它用于将宏参数转换为字符串字面值。","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"编程基础知识","slug":"编程基础知识","permalink":"http://example.com/tags/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}]},{"title":"reading note","slug":"knowledge_daily_study","date":"2023-09-30T15:34:12.030Z","updated":"2025-07-09T07:38:34.285Z","comments":true,"path":"2023/09/30/knowledge_daily_study/","link":"","permalink":"http://example.com/2023/09/30/knowledge_daily_study/","excerpt":"","text":"麻黄 麻黄碱属于肾上腺素受体激动剂，是从植物麻黄草中提取的一种生物碱。有以下药理作用：1、舒张支气管。2、增加心肌收缩力量，加大心脏血液输出量。3、能够兴奋中枢神经系统。在临床上用于慢性低血压、缓解支气管哮喘、对抗荨麻疹、血管神经性水肿。","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"编程基础知识","slug":"编程基础知识","permalink":"http://example.com/tags/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}]},{"title":"Coding-表格操作","slug":"coding_table_opeartion","date":"2023-09-30T15:33:46.788Z","updated":"2024-11-29T11:39:40.284Z","comments":true,"path":"2023/09/30/coding_table_opeartion/","link":"","permalink":"http://example.com/2023/09/30/coding_table_opeartion/","excerpt":"","text":"读取日期表格数据并显示 读取日期表格数据并显示 12345678910111213141516171819202122import matplotlib.pyplot as pltimport pandas as pddf = pd.read_csv(&#x27;plot.csv&#x27;)# 格式转为日期df[&#x27;date&#x27;] = pd.to_datetime(df[&#x27;date&#x27;])df.set_index(&#x27;date&#x27;, inplace=True)#输入折线图数据plt.plot(df.index,df[&quot;a2c&quot;],label=&#x27;a2c&#x27;,linewidth=1,color=&#x27;c&#x27;,marker=&#x27;&#x27;,markerfacecolor=&#x27;blue&#x27;,markersize=5)plt.xlabel(&quot;date&quot;)#横坐标为物品编号plt.ylabel(&#x27;loss&#x27;)#纵坐标为各类指标plt.title(&quot;&quot;)#折线图的名称#图例说明plt.legend()#显示网格plt.grid()#显示图像plt.show()","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-机器学习分类器","slug":"coding_python__classification_method","date":"2023-09-27T14:09:40.156Z","updated":"2024-11-29T11:30:22.482Z","comments":true,"path":"2023/09/27/coding_python__classification_method/","link":"","permalink":"http://example.com/2023/09/27/coding_python__classification_method/","excerpt":"","text":"T检验 SVM 朴素贝叶斯 K近邻 T检验 对标签为1和-1的样本进行T检验 12345678910111213141516from scipy.stats import ttest_indfrom statsmodels.stats.multitest import fdrcorrectiondef t_test(data, label): # 将正负样本分开 pos_data = data[label == 1] neg_data = data[label == -1] # 对正负样本做独立样本t检验 t_values, p_values = ttest_ind(pos_data, neg_data, axis=0) # 使用FDR控制方法去除不显著的特征 reject, p_values_corrected = fdrcorrection(p_values, alpha=0.1) significant_features = np.where(reject)[0] # 返回显著特征的索引 return data[:,significant_features] SVM SVM对莺尾花数据集分类，可以利用此代码进行数据集的初步分类可行性验证 1234567891011121314151617181920212223242526272829303132333435363738import numpy as npfrom sklearn.datasets import load_irisfrom sklearn.model_selection import GridSearchCV, KFold,train_test_splitfrom sklearn.svm import SVCiris = load_iris()data = iris.datalabels = iris.target# 划分训练集和测试集train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=1)print(train_data.shape)print(test_data.shape)# 定义参数范围x = np.logspace(-4, 4, num=9, base=2)param_grid = &#123; &quot;C&quot;: x,&#125;# 定义模型和交叉验证model = SVC(kernel=&quot;linear&quot;)cv = KFold(n_splits=10, shuffle=True, random_state=1)# 定义网格参数搜索法grid_search = GridSearchCV(model, param_grid, cv=cv, scoring=&quot;accuracy&quot;)# 进行交叉验证grid_search.fit(train_data, train_labels)# 输出最佳参数和交叉验证得分print(&quot;Best parameters: &quot;, grid_search.best_params_)print(&quot;Cross-validation score: &quot;, grid_search.best_score_)# 在测试集上评估模型test_score = grid_search.score(test_data, test_labels)print(&quot;Test set score: &quot;, test_score) 朴素贝叶斯 朴素贝叶斯对莺尾花数据集分类 1234567891011121314151617181920212223242526from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.naive_bayes import GaussianNBfrom sklearn.metrics import accuracy_score# 加载莺尾花数据集iris = load_iris()data = iris.datalabels = iris.target# 划分训练集和测试集train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=1)# 创建贝叶斯分类器对象classifier = GaussianNB()# 在训练集上训练模型classifier.fit(train_data, train_labels)# 在测试集上进行预测predictions = classifier.predict(test_data)# 计算分类准确率accuracy = accuracy_score(test_labels, predictions)print(&quot;Accuracy: &quot;, accuracy) K近邻 K近邻对莺尾花数据集分类 12345678910111213141516171819202122232425from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.metrics import accuracy_score# 加载莺尾花数据集iris = load_iris()data = iris.datalabels = iris.target# 划分训练集和测试集train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=1)# 创建KNN分类器对象classifier = KNeighborsClassifier(n_neighbors=5)# 在训练集上训练模型classifier.fit(train_data, train_labels)# 在测试集上进行预测predictions = classifier.predict(test_data)# 计算分类准确率accuracy = accuracy_score(test_labels, predictions)print(&quot;Accuracy: &quot;, accuracy)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-分类","slug":"Code-分类","permalink":"http://example.com/tags/Code-%E5%88%86%E7%B1%BB/"}]},{"title":"Coding-文件操作","slug":"coding_python_file_operation","date":"2023-09-27T14:08:37.227Z","updated":"2024-11-29T11:39:40.282Z","comments":true,"path":"2023/09/27/coding_python_file_operation/","link":"","permalink":"http://example.com/2023/09/27/coding_python_file_operation/","excerpt":"","text":"中文字符文件名换成拼音 其他格式图片转JPG 中文字符文件名换成拼音 将输入文件夹里的图片压缩有存储到输出文件夹中 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import osimport refrom pypinyin import pinyin, Styledef sanitize_filename(filename): # 将汉字转换为拼音 pinyin_list = pinyin(filename, style=Style.NORMAL) pinyin_string = &#x27;&#x27;.join([item[0] for item in pinyin_list]) # 移除非法字符，只保留字母、数字、&quot;-&quot;、&quot;_&quot;和&quot;.&quot; sanitized_filename = re.sub(r&#x27;[^\\w\\-_.]&#x27;, &#x27;&#x27;, pinyin_string) return sanitized_filenamedef sanitize_directory(directory): # 获取目录下的所有文件和文件夹 items = os.listdir(directory) # 遍历所有项 for item in items: item_path = os.path.join(directory, item) if os.path.isdir(item_path): # 如果是目录，递归处理子目录 sanitize_directory(item_path) # 修改目录名称 sanitized_dirname = sanitize_filename(item) new_dir_path = os.path.join(directory, sanitized_dirname) if item != sanitized_dirname: os.rename(item_path, new_dir_path) else: # 如果是文件，修改文件名称 sanitized_filename = sanitize_filename(item) new_file_path = os.path.join(directory, sanitized_filename) if item != sanitized_filename: os.rename(item_path, new_file_path)# 设置文件夹路径folder_path = &#x27;input_floder&#x27;# 递归修改目录和文件名称sanitize_directory(folder_path) 其他格式图片转JPG 12345678910111213141516171819202122232425262728293031323334import osfrom PIL import Imagedef convert_to_jpg(input_path, output_path): with Image.open(input_path) as image: # 转换为JPEG格式并保存 rgb_image = image.convert(&quot;RGB&quot;) rgb_image.save(output_path, format=&quot;JPEG&quot;)def convert_images_to_jpg(input_folder, output_folder): # 如果输出文件夹不存在，则创建该文件夹 if not os.path.exists(output_folder): os.makedirs(output_folder) # 遍历文件夹下的所有子文件夹和文件 for root, dirs, files in os.walk(input_folder): for file in files: # 获取文件的完整路径 input_path = os.path.join(root, file) # 检查文件是否为图片格式 if file.lower().endswith((&#x27;.png&#x27;, &#x27;.bmp&#x27;, &#x27;.gif&#x27;, &#x27;.tiff&#x27;)): # 构建输出路径 output_path = os.path.join(output_folder, os.path.splitext(file)[0] + &#x27;.jpg&#x27;) # 将文件转换为JPEG格式 convert_to_jpg(input_path, output_path)# 设置输入和输出文件夹路径input_folder = &#x27;input_folder&#x27;output_folder = &#x27;output_folder&#x27;# 转换图片格式为JPEGconvert_images_to_jpg(input_folder, output_folder)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"}]},{"title":"Coding-图像处理","slug":"coding_python_image_process","date":"2023-09-26T09:05:15.426Z","updated":"2024-01-23T03:55:27.746Z","comments":true,"path":"2023/09/26/coding_python_image_process/","link":"","permalink":"http://example.com/2023/09/26/coding_python_image_process/","excerpt":"","text":"恢复将归一化后的图片 多线程压缩图片 其他格式图片转JPG 将文件夹下的三通道图片转为单通道 恢复将归一化后的图片 变换 123456transform = transforms.Compose([transforms.Resize((224, 224)),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]) 存储验证集中预测错误的图片 12345678910111213141516171819202122232425with torch.no_grad(): for images, labels in val_loader: images = images.to(device) labels = labels.to(device) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total_samples += labels.size(0) total_correct += (predicted == labels).sum().item() # 找出识别错误的图片 misclassified_idx = (predicted != labels) misclassified_images.extend(images[misclassified_idx].cpu().numpy()) val_accuracy = total_correct / total_samples print(f&quot;Validation Accuracy: &#123;val_accuracy:.4f&#125;&quot;) # 保存识别错误的图片 for i, image in enumerate(misclassified_images): image = np.transpose(image, (1, 2, 0)) # 转换为通道在最后的形式 image = (image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]) * 255 #恢复过程 image = image.astype(np.uint8) # 将像素值还原为 0-255 范围 image = Image.fromarray(image) image.save(f&quot;error_images/misclassified_&#123;epoch&#125;_&#123;i&#125;.jpg&quot;) 多线程压缩图片 将输入文件夹里的图片压缩有存储到输出文件夹中 123456789101112131415161718192021222324252627282930313233343536373839404142import osimport shutilfrom concurrent.futures import ThreadPoolExecutorfrom PIL import Imageinput_folder = &#x27;input_folder&#x27;output_folder = &#x27;output_folder&#x27;# 创建输出文件夹if not os.path.exists(output_folder): os.makedirs(output_folder)# 获取输入文件夹中所有图片文件的文件名file_names = [f for f in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, f)) and any(f.endswith(ext) for ext in [&#x27;.jpg&#x27;, &#x27;.jpeg&#x27;, &#x27;.png&#x27;, &#x27;.bmp&#x27;, &#x27;.gif&#x27;])]def process_image(file_name): # 获取输入文件的路径 input_file_path = os.path.join(input_folder, file_name) output_file_path = os.path.join(output_folder, file_name) # 打开图片文件 with Image.open(input_file_path) as image: # 检查图片格式 if image.format in [&#x27;JPEG&#x27;, &#x27;JPG&#x27;, &#x27;PNG&#x27;]: # 压缩图片并保存到输出文件夹 compressed_image = image.copy() compressed_image.save(output_file_path, optimize=True, quality=25) # 检查图片大小是否超过500KB while os.path.getsize(output_file_path) &gt; 500 * 800: # 缩小图片尺寸 width, height = compressed_image.size new_width = int(width * 0.9) new_height = int(height * 0.9) compressed_image = compressed_image.resize((new_width, new_height)) # 保存压缩后的图片 compressed_image.save(output_file_path, optimize=True, quality=25)# 使用ThreadPoolExecutor来并行处理每一张图片with ThreadPoolExecutor() as executor: executor.map(process_image, file_names) 其他格式图片转JPG 12345678910111213141516171819202122232425262728293031323334import osfrom PIL import Imagedef convert_to_jpg(input_path, output_path): with Image.open(input_path) as image: # 转换为JPEG格式并保存 rgb_image = image.convert(&quot;RGB&quot;) rgb_image.save(output_path, format=&quot;JPEG&quot;)def convert_images_to_jpg(input_folder, output_folder): # 如果输出文件夹不存在，则创建该文件夹 if not os.path.exists(output_folder): os.makedirs(output_folder) # 遍历文件夹下的所有子文件夹和文件 for root, dirs, files in os.walk(input_folder): for file in files: # 获取文件的完整路径 input_path = os.path.join(root, file) # 检查文件是否为图片格式 if file.lower().endswith((&#x27;.png&#x27;, &#x27;.bmp&#x27;, &#x27;.gif&#x27;, &#x27;.tiff&#x27;)): # 构建输出路径 output_path = os.path.join(output_folder, os.path.splitext(file)[0] + &#x27;.jpg&#x27;) # 将文件转换为JPEG格式 convert_to_jpg(input_path, output_path)# 设置输入和输出文件夹路径input_folder = &#x27;input_folder&#x27;output_folder = &#x27;output_folder&#x27;# 转换图片格式为JPEGconvert_images_to_jpg(input_folder, output_folder) 将文件夹下的三通道图片转为单通道 123456789101112from PIL import Imageimport ospath = &#x27;input_floder&#x27;list_ = os.listdir(path)list_ = [ os.path.join(path,i) for i in list_]for i in list_: image = Image.open(i) gray_image = image.convert(&#x27;L&#x27;) gray_image.save(i)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-图像处理","slug":"Code-图像处理","permalink":"http://example.com/tags/Code-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"Coding-MLP-UNET-RESNET-2D OR 3D","slug":"coding_network_usual_use","date":"2023-09-12T07:49:08.125Z","updated":"2024-11-29T11:58:18.476Z","comments":true,"path":"2023/09/12/coding_network_usual_use/","link":"","permalink":"http://example.com/2023/09/12/coding_network_usual_use/","excerpt":"","text":"MLP 2D-Unet 2D-Resnet 3D-Resnet MLP 1234567891011121314151617181920212223242526import torch.nn as nnclass FCModel(nn.Module): def __init__(self, input_size, hidden_size, num_classes): super(FCModel, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.fc2 = nn.Linear(hidden_size, num_classes) def forward(self, x): batch_size, seq_length, input_size = x.size() x = x.view(batch_size * seq_length, input_size) # Reshape input to (batch_size * seq_length, input_size) h = self.fc1(x) out = self.fc2(h) out = out.view(batch_size, seq_length, -1) # Reshape output back to (batch_size, seq_length, num_classes) out = out[:, -1, :] # Take the last time step&#x27;s output return out# 设置超参数input_size = 13456hidden_size = 64num_classes = 2# 创建模型实例model = FCModel(input_size, hidden_size, num_classes) 2D-Unet 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import torchimport torch.nn as nnimport torch.nn.functional as F# UNet的一大层，包含了两层小的卷积class DoubleConv(nn.Module): def __init__(self, in_ch, out_ch): super(DoubleConv, self).__init__() self.conv = nn.Sequential( nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True), nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True) ) def forward(self, x): x = self.conv(x) return x# 定义输入进来的第一层class InConv(nn.Module): def __init__(self, in_ch, out_ch): super(InConv, self).__init__() self.conv = DoubleConv(in_ch, out_ch) def forward(self, x): x = self.conv(x) return x# 定义encoder中的向下传播，包括一个maxpool和一大层 class Down(nn.Module): def __init__(self, in_ch, out_ch): super(Down, self).__init__() self.mpconv = nn.Sequential( nn.MaxPool2d(2), DoubleConv(in_ch, out_ch) ) def forward(self, x): x = self.mpconv(x) return x# 定义decoder中的向上传播class Up(nn.Module): def __init__(self, in_ch, out_ch, bilinear=True): super(Up, self).__init__() # 定义了self.up的方法 if bilinear: self.up = nn.Upsample(scale_factor=2, mode=&#x27;bilinear&#x27;, align_corners=True) else: self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2) # // 除以的结果向下取整 self.conv = DoubleConv(in_ch, out_ch) def forward(self, x1, x2): # x2是左侧的输出，x1是上一大层来的输出 x1 = self.up(x1) diffY = x2.size()[2] - x1.size()[2] diffX = x2.size()[3] - x1.size()[3] x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2)) x = torch.cat([x2, x1], dim=1) # 将两个tensor拼接在一起 dim=1：在通道数（C）上进行拼接 x = self.conv(x) return x# 定义最终的输出class OutConv(nn.Module): def __init__(self, in_ch, out_ch): super(OutConv, self).__init__() self.conv = nn.Conv2d(in_ch, out_ch, 1) def forward(self, x): x = self.conv(x) return xclass Unet(nn.Module): def __init__(self, in_channels, classes): # in_channels 图片的通道数，1为灰度图，3为彩色图 super(Unet, self).__init__() self.n_channels = in_channels self.n_classes = classes self.inc = InConv(in_channels, 64) self.down1 = Down(64, 128) self.down2 = Down(128, 256) self.down3 = Down(256, 512) self.down4 = Down(512, 512) self.up1 = Up(1024, 256) self.up2 = Up(512, 128) self.up3 = Up(256, 64) self.up4 = Up(128, 64) self.outc = OutConv(64, classes) def forward(self, x): x1 = self.inc(x) x2 = self.down1(x1) x3 = self.down2(x2) x4 = self.down3(x3) x5 = self.down4(x4) x = self.up1(x5, x4) x = self.up2(x, x3) x = self.up3(x, x2) x = self.up4(x, x1) x = self.outc(x) return x 2D-Resnet 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105import torchimport torch.nn as nnclass BasicBlock(nn.Module): expansion = 1 def __init__(self, in_channels, out_channels, stride=1): super(BasicBlock, self).__init__() # 第一个卷积层 self.conv1 = nn.Conv2d( in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False ) self.bn1 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) # 第二个卷积层 self.conv2 = nn.Conv2d( out_channels, out_channels * self.expansion, kernel_size=3, stride=1, padding=1, bias=False, ) self.bn2 = nn.BatchNorm2d(out_channels * self.expansion) # 残差连接（shortcut connection） self.shortcut = nn.Sequential() if stride != 1 or in_channels != out_channels * self.expansion: self.shortcut = nn.Sequential( nn.Conv2d( in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False, ), nn.BatchNorm2d(out_channels * self.expansion), ) def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out += self.shortcut(residual) out = self.relu(out) return outclass ResNet(nn.Module): def __init__(self, block, num_blocks, num_classes=10): super(ResNet, self).__init__() self.in_channels = 64 # 第一个卷积层 self.conv1 = nn.Conv2d( 3, 64, kernel_size=3, stride=1, padding=1, bias=False ) self.bn1 = nn.BatchNorm2d(64) self.relu = nn.ReLU(inplace=True) # ResNet的四个阶段 self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1) self.layer2 = self.make_layer(block, 128, num_blocks[1], stride=2) self.layer3 = self.make_layer(block, 256, num_blocks[2], stride=2) self.layer4 = self.make_layer(block, 512, num_blocks[3], stride=2) # 全局平均池化层和全连接层 self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Linear(512 * block.expansion, num_classes) def make_layer(self, block, out_channels, num_blocks, stride): layers = [] layers.append(block(self.in_channels, out_channels, stride)) self.in_channels = out_channels * block.expansion for _ in range(1, num_blocks): layers.append(block(self.in_channels, out_channels)) return nn.Sequential(*layers) def forward(self, x): out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.layer1(out) out = self.layer2(out) out = self.layer3(out) out = self.layer4(out) out = self.avg_pool(out) out = torch.flatten(out, 1) out = self.fc(out) return outdef ResNet18(num_classes=10): return ResNet(BasicBlock, [2, 2, 2, 2], num_classes) 3D-Resnet 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103class BasicBlock(nn.Module): expansion = 1 def __init__(self, in_channels, out_channels, stride=1): super(BasicBlock, self).__init__() # 第一个卷积层 self.conv1 = nn.Conv3d( in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False ) self.bn1 = nn.BatchNorm3d(out_channels) self.relu = nn.ReLU(inplace=True) # 第二个卷积层 self.conv2 = nn.Conv3d( out_channels, out_channels * self.expansion, kernel_size=3, stride=1, padding=1, bias=False, ) self.bn2 = nn.BatchNorm3d(out_channels * self.expansion) # 残差连接（shortcut connection） self.shortcut = nn.Sequential() if stride != 1 or in_channels != out_channels * self.expansion: self.shortcut = nn.Sequential( nn.Conv3d( in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False, ), nn.BatchNorm3d(out_channels * self.expansion), ) def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out += self.shortcut(residual) out = self.relu(out) return outclass ResNet(nn.Module): def __init__(self, block, num_blocks, num_classes=10): super(ResNet, self).__init__() self.in_channels = 64 # 第一个卷积层 self.conv1 = nn.Conv3d( 1, 64, kernel_size=3, stride=1, padding=1, bias=False ) self.bn1 = nn.BatchNorm3d(64) self.relu = nn.ReLU(inplace=True) # ResNet的四个阶段 self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1) self.layer2 = self.make_layer(block, 128, num_blocks[1], stride=2) self.layer3 = self.make_layer(block, 256, num_blocks[2], stride=2) self.layer4 = self.make_layer(block, 512, num_blocks[3], stride=2) # 全局平均池化层和全连接层 self.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1)) self.fc = nn.Linear(512 * block.expansion, num_classes) def make_layer(self, block, out_channels, num_blocks, stride): layers = [] layers.append(block(self.in_channels, out_channels, stride)) self.in_channels = out_channels * block.expansion for _ in range(1, num_blocks): layers.append(block(self.in_channels, out_channels)) return nn.Sequential(*layers) def forward(self, x): out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.layer1(out) out = self.layer2(out) out = self.layer3(out) out = self.layer4(out) out = self.avg_pool(out) out = torch.flatten(out, 1) out = self.fc(out) return outdef ResNet18_3D(num_classes=10): return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-模型库","slug":"Code-模型库","permalink":"http://example.com/tags/Code-%E6%A8%A1%E5%9E%8B%E5%BA%93/"}]},{"title":"Coding-医学图像处理","slug":"coding_python_medical","date":"2023-09-04T02:09:53.960Z","updated":"2024-01-23T03:55:36.622Z","comments":true,"path":"2023/09/04/coding_python_medical/","link":"","permalink":"http://example.com/2023/09/04/coding_python_medical/","excerpt":"","text":"读取nii文件，存储nii文件 读取nii文件，存储nii文件 12345678910111213141516import nibabel as nib# 读取NIfTI文件nii_file = &#x27;1_output.nii&#x27;nii_img = nib.load(nii_file)# 获取图像数据和元数据data = nii_img.get_fdata()header = nii_img.header# 进行必要的操作，例如处理数据或分析# 保存NIfTI文件output_file = &#x27;2_output.nii&#x27;output_img = nib.Nifti1Image(data, affine=None, header=header)nib.save(output_img, output_file)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-图像处理","slug":"Code-图像处理","permalink":"http://example.com/tags/Code-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"PaperReading-DMC-Fusion:基于分类器特征合成的深度多级联融合针对医学多模态图像","slug":"DMC-Fusion Deep Multi-cascade Fusion with Classifier-Based Feature Synthesis for Medical Multi-modal Images","date":"2023-09-03T11:38:59.208Z","updated":"2024-01-20T03:02:04.245Z","comments":true,"path":"2023/09/03/DMC-Fusion Deep Multi-cascade Fusion with Classifier-Based Feature Synthesis for Medical Multi-modal Images/","link":"","permalink":"http://example.com/2023/09/03/DMC-Fusion%20Deep%20Multi-cascade%20Fusion%20with%20Classifier-Based%20Feature%20Synthesis%20for%20Medical%20Multi-modal%20Images/","excerpt":"","text":"DMC-Fusion: Deep Multi-cascade Fusion with Classifier-Based Feature Synthesis for Medical Multi-modal Images 1.摘要 多模态医学图像融合是临床精确诊断和手术计划的重要课题。尽管像Densefuse这样的单特征融合策略取得了令人鼓舞的效果，但它往往不能完全保留源图像的特征。本文提出了一种基于分类器特征合成的深度多融合框架，用于多模态医学图像的自动融合。该算法由基于密集连接（dense connections）的预训练自编码器、特征分类器和一种分别融合高频和低频的多级联融合解码器。编码器和解码器从MS-COCO数据集传输，并在多模态医学图像公共数据集上同时进行预训练以提取特征。通过高斯高通滤波和峰值信噪比阈值法对特征进行分类，然后将预训练的Dense-Block和解码器的每一层特征映射划分为高频和低频序列。具体而言，在所提出的特征融合块中，采用参数自适应脉冲耦合神经网络和L1加权分别进行高频和低频融合。最后，我们在全解码特征阶段设计了一种新型的多级融合解码器，以选择性地融合不同模态的有用信息。我们还使用融合图像验证了我们的方法对脑部疾病的分类，并进行了统计显著性检验，以说明分类性能的提高是由于融合。实验结果表明，该方法在定性和定量评价方面都达到了最先进的水平。 2.引言 3.论文结构 4.总结 5.文章笔记 MRI和PET的基本原理的叙述： 磁共振成像(MRI)是一种利用磁场和无线电波对人体内部器官进行成像的结构方式。这种非侵入性诊断工具测量所需身体部位的解剖结构。而正电子发射断层扫描(PET)是一种功能模式，使用放射性示踪剂来量化人体组织和器官中的代谢活动。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[]},{"title":"Coding-彩色图片分类-基于2D-Unet","slug":"coding_network_example","date":"2023-09-02T13:09:09.106Z","updated":"2024-01-23T03:54:55.135Z","comments":true,"path":"2023/09/02/coding_network_example/","link":"","permalink":"http://example.com/2023/09/02/coding_network_example/","excerpt":"","text":"摘要 main.py 摘要 训练一个图片分类神经网络（2D-Unet），包括 1.自定义dataload制作 2,网络定义 3.训练过程 4.测试过程 5.模型评估（准确率） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120import glob # 导入用于文件路径匹配的模块from torchvision import transforms # 导入图像转换模块from torch.utils import data # 导入PyTorch数据工具模块from PIL import Image # 导入PIL图像处理库import matplotlib.pyplot as plt # 导入绘图库import torch # 导入PyTorch库import torch.nn as nn # 导入PyTorch神经网络模块import torch.nn.functional as F # 导入PyTorch函数库from unet import Unet # 导入自定义的U-Net模型import numpy as np # 导入NumPy库# 标准化数据transforms = transforms.Compose([ transforms.ToTensor(), # 将图像转换为张量 transforms.Resize((256, 256)), # 调整图像大小为256x256 transforms.Normalize(mean=0.5, std=0.5) # 标准化图像数据])class my_dataset(data.Dataset): def __init__(self, imgs_path, annos_path): self.imgs_path = imgs_path # 图像文件路径 self.annos_path = annos_path # 标签文件路径 def __getitem__(self, index): img_path = self.imgs_path[index] # 获取图像路径 pil_img = Image.open(img_path) # 使用PIL打开图像 pil_img = transforms(pil_img) # 对图像进行预处理 anno_path = self.annos_path[index] # 获取标签路径 anno_img = Image.open(anno_path) # 使用PIL打开标签图像 pil_anno = transforms(anno_img) # 对标签图像进行预处理 return pil_img, pil_anno def __len__(self): return len(self.imgs_path) # 返回数据集的长度def train(model, train_loader, criterion, optimizer, device): model.train() # 设置模型为训练模式 train_loss = 0 for data, label in train_loader: data = data.to(device) optimizer.zero_grad() # 清除梯度 output = model(data) # 前向传播 loss = criterion(output, label.to(device).float()) # 计算损失 loss.backward() # 反向传播，计算梯度 optimizer.step() # 更新模型参数 train_loss += loss.item() * data.size(0) train_loss /= len(train_loader.dataset) # 计算平均训练损失 return train_lossdef validate(model, val_loader, criterion, device): model.eval() # 设置模型为评估模式 val_loss = 0 with torch.no_grad(): for data, label in val_loader: data = data.to(device) output = model(data) # 前向传播 loss = criterion(output, label.to(device).float()) # 计算损失 val_loss += loss.item() * data.size(0) val_loss /= len(val_loader.dataset) # 计算平均验证损失 return val_loss if __name__ ==&#x27;__main__&#x27;: # 训练数据集导入 imgs_path = glob.glob(&#x27;facade/train_picture/*.png&#x27;) # 匹配训练图像文件路径 label_path = glob.glob(&#x27;facade/train_label/*.jpg&#x27;) # 匹配训练标签文件路径 # 测试数据集导入 test_imgs_path = glob.glob(&#x27;facade/test_picture/*.png&#x27;) # 匹配测试图像文件路径 test_label_path = glob.glob(&#x27;facade/test_label/*.jpg&#x27;) # 匹配测试标签文件路径 # 对数据和标签排序，确保一一对应 imgs_path = sorted(imgs_path) label_path = sorted(label_path) test_imgs_path = sorted(test_imgs_path) test_label_path = sorted(test_label_path) train_dataset = my_dataset(imgs_path, label_path) test_dataset = my_dataset(test_imgs_path, test_label_path) # 创建测试数据集对象 train_loader = data.DataLoader(train_dataset, batch_size=4, shuffle=True) # 创建训练数据加载器 test_loader = data.DataLoader(test_dataset, batch_size=4, shuffle=False) # 创建测试数据加载器 # 创建U-Net模型 in_channels = 3 # 输入通道数 out_channels = 3 # 输出通道数 model = Unet(in_channels, out_channels) # 创建U-Net模型对象 criterion = nn.MSELoss() # 创建均方误差损失函数对象 optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # 创建优化器对象 # 将模型和数据移动到GPU上 device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;) # 检查是否有可用的GPU model.to(device) # 将模型移动到GPU上 train_losses = [] # 保存训练损失的列表 val_losses = [] # 保存验证损失的列表 best_val_loss = np.inf # 初始化最佳验证损失为正无穷 best_model = None # 初始化最佳模型为空 epoch_times = 300 # 设定迭代次数 # 训练模型 for epoch in range(epoch_times): train_loss = train(model, train_loader, criterion, optimizer, device) # 训练模型 val_loss = validate(model, test_loader, criterion, device) # 验证模型 train_losses.append(train_loss) # 保存训练损失 val_losses.append(val_loss) # 保存验证损失 if val_loss &lt; best_val_loss: best_val_loss = val_loss best_model = model.state_dict() torch.save(best_model, &#x27;ckpt/model.ckpt&#x27;) # 保存最佳模型参数 print(&quot;best_val_loss: &quot; + str(val_loss)) with open(&quot;ckpt/model_loss.txt&quot;, &quot;w&quot;) as f: f.write(str(val_loss)) print(&#x27;Epoch [&#123;&#125;/&#123;&#125;], Train Loss: &#123;:.4f&#125;, Val Loss: &#123;:.4f&#125;&#x27;.format(epoch+1, epoch_times, train_loss, val_loss)) val.py (导入模型进行生成测试) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from unet import Unet # 导入自定义的U-Net模型import torch # 导入PyTorch库from torch.utils import data # 导入PyTorch数据工具模块from PIL import Image # 导入PIL图像处理库from torchvision import transforms # 导入图像转换模块import glob # 导入用于文件路径匹配的模块# 标准化数据transforms = transforms.Compose([ transforms.ToTensor(), # 将图像转换为张量 transforms.Resize((256, 256)), # 调整图像大小为256x256 transforms.Normalize(mean=0.5, std=0.5) # 标准化图像数据])class my_dataset(data.Dataset): def __init__(self, imgs_path, annos_path): self.imgs_path = imgs_path # 图像文件路径 self.annos_path = annos_path # 标签文件路径 def __getitem__(self, index): img_path = self.imgs_path[index] # 获取图像路径 pil_img = Image.open(img_path) # 使用PIL打开图像 pil_img = transforms(pil_img) # 对图像进行预处理 anno_path = self.annos_path[index] # 获取标签路径 anno_img = Image.open(anno_path) # 使用PIL打开标签图像 pil_anno = transforms(anno_img) # 对标签图像进行预处理 return pil_img, pil_anno def __len__(self): return len(self.imgs_path) # 返回数据集的长度# 测试数据集导入test_imgs_path = glob.glob(&#x27;facade/test_picture/*.png&#x27;) # 匹配测试图像文件路径test_label_path = glob.glob(&#x27;facade/test_label/*.jpg&#x27;) # 匹配测试标签文件路径test_dataset = my_dataset(test_imgs_path, test_label_path) # 创建测试数据集对象test_loader = data.DataLoader(test_dataset, batch_size=1, shuffle=False) # 创建测试数据加载器model = Unet(3, 3) # 创建U-Net模型对象checkpoint = torch.load(&#x27;ckpt/model.ckpt&#x27;) # 加载模型参数model.load_state_dict(checkpoint) # 加载模型参数for data, label in test_loader: data = data * 0.5 + 0.5 # 反标准化图像数据 output = model(data) # 前向传播 output = torch.squeeze(output, 0) # 去除输出张量的维度为1的维度 array = output.cpu().detach().numpy().transpose(1, 2, 0) # 将输出张量转换为NumPy数组，并调整通道顺序为HWC image = Image.fromarray((array * 255).astype(&#x27;uint8&#x27;)) # 创建PIL图像对象 image.save(&#x27;image1.jpg&#x27;) # 保存图像为JPEG文件 unet.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import torchimport torch.nn as nnimport torch.nn.functional as F# UNet的一大层，包含了两层小的卷积class DoubleConv(nn.Module): def __init__(self, in_ch, out_ch): super(DoubleConv, self).__init__() self.conv = nn.Sequential( nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True), nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True) ) def forward(self, x): x = self.conv(x) return x# 定义输入进来的第一层class InConv(nn.Module): def __init__(self, in_ch, out_ch): super(InConv, self).__init__() self.conv = DoubleConv(in_ch, out_ch) def forward(self, x): x = self.conv(x) return x# 定义encoder中的向下传播，包括一个maxpool和一大层 class Down(nn.Module): def __init__(self, in_ch, out_ch): super(Down, self).__init__() self.mpconv = nn.Sequential( nn.MaxPool2d(2), DoubleConv(in_ch, out_ch) ) def forward(self, x): x = self.mpconv(x) return x# 定义decoder中的向上传播class Up(nn.Module): def __init__(self, in_ch, out_ch, bilinear=True): super(Up, self).__init__() # 定义了self.up的方法 if bilinear: self.up = nn.Upsample(scale_factor=2, mode=&#x27;bilinear&#x27;, align_corners=True) else: self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2) # // 除以的结果向下取整 self.conv = DoubleConv(in_ch, out_ch) def forward(self, x1, x2): # x2是左侧的输出，x1是上一大层来的输出 x1 = self.up(x1) diffY = x2.size()[2] - x1.size()[2] diffX = x2.size()[3] - x1.size()[3] x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2)) x = torch.cat([x2, x1], dim=1) # 将两个tensor拼接在一起 dim=1：在通道数（C）上进行拼接 x = self.conv(x) return x# 定义最终的输出class OutConv(nn.Module): def __init__(self, in_ch, out_ch): super(OutConv, self).__init__() self.conv = nn.Conv2d(in_ch, out_ch, 1) def forward(self, x): x = self.conv(x) return xclass Unet(nn.Module): def __init__(self, in_channels, classes): # in_channels 图片的通道数，1为灰度图，3为彩色图 super(Unet, self).__init__() self.n_channels = in_channels self.n_classes = classes self.inc = InConv(in_channels, 64) self.down1 = Down(64, 128) self.down2 = Down(128, 256) self.down3 = Down(256, 512) self.down4 = Down(512, 512) self.up1 = Up(1024, 256) self.up2 = Up(512, 128) self.up3 = Up(256, 64) self.up4 = Up(128, 64) self.outc = OutConv(64, classes) def forward(self, x): x1 = self.inc(x) x2 = self.down1(x1) x3 = self.down2(x2) x4 = self.down3(x3) x5 = self.down4(x4) x = self.up1(x5, x4) x = self.up2(x, x3) x = self.up3(x, x2) x = self.up4(x, x1) x = self.outc(x) return x","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-分类","slug":"Code-分类","permalink":"http://example.com/tags/Code-%E5%88%86%E7%B1%BB/"}]},{"title":"PaperReading-生成对抗网络在医学成像中的应用综述","slug":"paper_生成对抗网络在医学成像中的应用综述","date":"2023-09-01T14:06:29.256Z","updated":"2024-06-13T02:43:16.122Z","comments":true,"path":"2023/09/01/paper_生成对抗网络在医学成像中的应用综述/","link":"","permalink":"http://example.com/2023/09/01/paper_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E5%9C%A8%E5%8C%BB%E5%AD%A6%E6%88%90%E5%83%8F%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E7%BB%BC%E8%BF%B0/","excerpt":"","text":"Generative adversarial network in medical imaging: A review 1.摘要 生成对抗网络由于其无需明确建模概率密度函数即可生成数据的能力而在计算机视觉界受到了广泛的关注。鉴别器带来的对抗损失提供了一种巧妙的方法，将未标记的样本整合到训练中，并实现更高的阶一致性。事实证明，这在许多情况下都很有用，例如域适应、数据增强和图像到图像的转换。这些特性吸引了医学影像界的研究人员，并在许多传统和新型应用中得到了迅速的应用，如图像重建、分割、检测、分类和交叉模态合成。根据我们的观察，这一趋势将继续下去，因此我们对使用对抗训练方案的医学成像的最新进展进行了回顾，希望能使对这项技术感兴趣的研究人员受益。 2.引言 从2012年开始，随着深度学习在计算机视觉领域的复兴(Krizhevsky et al, 2012)，深度学习方法在医学成像领域的应用急剧增加。 gan是一种特殊类型的神经网络模型，它同时训练两个网络，一个专注于图像生成，另一个专注于识别。 深度学习的根源可以追溯到20世纪80年代(Fukushima and Miyake, 1982)，而对抗训练的概念相对较新，最近取得了重大进展(Goodfellow et al .， 2014)。本文介绍了gan的总体概况，描述了它们在医学成像中的应用前景，并确定了一些需要解决的挑战，以使它们在其他医学成像相关任务中成功应用 为了全面概述gan在医学成像方面的所有相关工作，我们检索了PubMed、arXiv… 本文的其余部分结构如下… 3.论文结构 2.背景 2.1 Vanilla GAN 2.2 优化gan的挑战 2.3 gan的变体 2.3.1 D的变化目标 2.3.2 G的变化目标 3.在医学成像中的应用 3.1. 重建 3.2 医学图像合成 3.2.1 无条件的合成 3.2.2 交叉模态合成 3.2.3 其他条件合成 3.3 分割 3.4 分类 3.5 检测 3.6 配准 3.7 其他工作 4.讨论 4.1 未来的挑战 4.2 有趣的未来应用 4.文章笔记 GAN存在的问题 并不能保证G和D的训练在JS发散的情况下达到平衡。因此，一个网络可能不可避免地比另一个网络更强大，在大多数情况下是D。当D变得过于强大而不是G时，生成的样本变得太容易与真实样本分离，从而达到D的梯度接近于零的阶段，无法为G的进一步训练提供指导。由于难以生成有意义的高频细节，这种情况在生成高分辨率图像时更常见。 另一个在训练gan时经常遇到的问题是模态崩溃，顾名思义，模态崩溃是指G学习到的分布集中在数据分布的几个有限模态上。因此，它产生的不是不同的图像，而是一组有限的样本。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Paper-医学影像","slug":"Paper-医学影像","permalink":"http://example.com/tags/Paper-%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F/"},{"name":"Paper-综述文章","slug":"Paper-综述文章","permalink":"http://example.com/tags/Paper-%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/"}]},{"title":"PaperReading-BPGAN-使用光谱归一化和定位的多生成多对抗网络进行CT-MRI双向预测","slug":"paper_BPGAN-使用光谱归一化和定位的多生成多对抗网络进行ct-mri双向预测","date":"2023-08-31T14:21:06.390Z","updated":"2024-06-12T15:57:54.671Z","comments":true,"path":"2023/08/31/paper_BPGAN-使用光谱归一化和定位的多生成多对抗网络进行ct-mri双向预测/","link":"","permalink":"http://example.com/2023/08/31/paper_BPGAN-%E4%BD%BF%E7%94%A8%E5%85%89%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96%E5%92%8C%E5%AE%9A%E4%BD%8D%E7%9A%84%E5%A4%9A%E7%94%9F%E6%88%90%E5%A4%9A%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E8%BF%9B%E8%A1%8Cct-mri%E5%8F%8C%E5%90%91%E9%A2%84%E6%B5%8B/","excerpt":"","text":"BPGAN: Bidirectional CT-to-MRI prediction using multi-generative multi-adversarial nets with spectral normalization and localization 1.摘要 磁共振成像(MRI)和计算机断层扫描(CT)是广泛应用于临床和研究的筛查、诊断和图像引导治疗的检测技术。然而，CT在采集过程中对患者施加电离辐射。与CT相比，MRI更安全，没有任何辐射，但更昂贵，采集时间更长。因此，在放疗规划的情况下，有必要从同一受试者的另一个给定的模态图像中估计一个模态图像。考虑到目前MRI和CT图像之间没有双向预测模型，我们提出了一种双向预测方法，即使用多生成多对抗网络(BPGAN)以成对和非成对的方式从另一模态图像中预测任意模态。在BPGAN中，采用循环一致性策略，通过将相同的病理特征从一个域投射到另一个域来学习两个非线性映射。在技术上，引入病理先验信息来约束特征生成，以攻击病理变异的潜在风险，并采用边缘保留度量来保留几何畸变和解剖结构。在算法上，设计了谱归一化来控制鉴别器的性能，使预测器更好更快地学习;提出了局部化来对预测器施加正则化，以减少泛化误差。实验结果表明，BPGAN比目前最先进的方法产生更好的预测。其中，BPGAN在两个基线数据集上的MAE和SSIM的平均增量分别为33.2%和37.4%，SSIM分别为24.5%和44.6% 2.引言 MRI和CT在各种医疗病例中都是重要的和广泛应用的技术。与MRI相比，CT的成像时间更短，空间分辨率更高，适用于骨骼和胸部的检测。但CT软组织信息对比度较低。考虑到电离辐射和CT的不同耐受性，MRI更适合正电子发射断层扫描(PET)中的衰减校正(AC)和现代放疗治疗计划中的剂量计算，MRI优越的软组织对比度有助于精确描绘肿瘤和危险器官。但MRI价格昂贵，且出于患者舒适度和依从性的考虑，采集时间较长，而标准的MRI引导临床治疗包括CT和MRI图像的采集。因此，由相应的和真实的MRI/CT图像准确合成的无偏伪CT/MRI图像(pCT/pMRI)，在无法获得真实CT/MRI信息的情况下，在临床应用中是非常有用的。 为了减少不必要的电离剂量和患者的额外费用，临床上需要从另一个模态图像中估计一个模态图像。在这一需求的启发下，人们提出了许多创新性的单向预测方法，但实现这一目标仍然存在两个主要挑战 1)几乎所有的预测算法都只能进行单向预测，即从给定的MRI预测pCT或从给定的CT预测pMRI 2)在双向预测中，预测器可能产生目标图像中未显示的特征，这是一个潜在的风险。 为了解决这两个具有挑战性的问题，需要解决两个子问题:首先，学习MRI和CT域之间的双向映射;其次，预测的伪MRI和CT所描述的病理信息应与原始图像所描述的病理信息相同。CycleGAN和conditional GAN在这两个子问题上取得了巨大进展。然而，CycleGAN在几何变换方面存在固有的模糊性。 基于以上分析，我们提出了一种基于多生成多对抗网络(BPGAN)的双向预测方法: (1) 提出了一种新的双向预测方法，以配对和不配对的方式从另一个给定模态中预测任意模态图像，这是跨模态预测的第一个端到端双向预测模型。 (2) 引入病理辅助信息约束特征生成，打击病理变异的潜在风险，采用局部预测器消除预测器反求的约束，搜索给定模态图像对应的全局坐标，降低泛化误差; (3) 设计了谱归一化来控制鉴别器的性能，保证了在控制Lipschitz界方面的理论论证，在稳定性和收敛性方面做出了重大贡献; (4) 更全面的评价，包括对pCT/pMRI影像的客观评价和对诊断质量的主观评价。大量实验表明，该方法在主观上和客观上都取得了令人满意的预测结果。 本文的其余部分组织如下:… 3.总结 提出了一种基于高斯的交叉模态医学图像双向预测方案，该方案采用多生成多对抗网络进行光谱归一化和定位。为了消除病理变异的潜在风险，在同一类中加入辅助信息来生成特征，并采用局部定位来直接访问局部几何，而不是在全局GAN中反转预测器。然后利用光谱归一化控制鉴别器的性能，间接提高了预测图像的质量。此外，边缘保留度量用于保留解剖结构，总变异损失用于抑制训练过程中的噪声。总的来说，所提出的BPGAN产生了有希望的交叉模态预测结果。特别的是，它在基准上优于30% MAE, 20% SSIM, 20% FSIM, 50% MSIM, 15% GAN-train和10% GAN-test的平均增量。然后，主任医师的专业评估进一步证明BPGAN产生了令人信服的诊断质量，这与广泛的定量评估是一致的。 4.文章笔记 病理不变性： 在临床上，同一患者的同一器官的病理信息在预测时应该是相同的，本文称之为病理不变性。 CycleGAN的缺点 CycleGAN (Zhu et al .， 2017)在几何变换方面存在固有的模糊性。具体来说，CycleGAN中循环一致性的核心是保证GA(GB(x))→x和GB(GA(y))→y，但GB(x)→y和GA(y)→x不能保证几何畸变，它们是完全预期的。 谱归一化 归一化（Spectral Normalization）是一种在神经网络中常用的正则化技术，旨在稳定和改进生成对抗网络（GANs）和其他深度学习模型的训练过程。在谱归一化中，权重矩阵的每一行都被约束在单位球（L2 范数为1的球）上。这样做的目的是通过限制权重的范围来控制模型的复杂度，并提高模型的泛化性能。谱归一化的主要步骤是通过计算权重矩阵的特征值分解来估计权重矩阵的最大奇异值（spectral norm）。然后将权重矩阵除以最大奇异值以进行归一化。这可以通过迭代幂法（power iteration）来实现，迭代幂法通过多次迭代权重矩阵和其转置矩阵的乘积来逐渐逼近最大奇异值。谱归一化的优点包括： (1)改善模型的稳定性：通过限制权重矩阵的范围，谱归一化可以降低模型训练过程中的梯度爆炸和梯度消失问题，从而提高模型的稳定性。 (2)提高生成对抗网络（GANs）的训练效果：谱归一化在生成器和判别器网络中应用广泛，可以使GANs的训练更加稳定，生成的样本质量更高。 (3)不增加额外的模型参数：与其他正则化方法（如权重衰减）相比，谱归一化不需要引入额外的超参数或调整权重衰减系数，因此更易于使用。 总之，谱归一化是一种用于正则化神经网络的技术，通过限制权重矩阵的谱范数来提高模型的稳定性和泛化性能。它在生成对抗网络和其他深度学习模型中具有重要的应用价值。 几种损失: 双向条件对抗性损失(Bidirectional conditional adversarial loss): 边缘保持损失(Edge retention loss):Edge retention loss是一种用于保留医学图像中边缘信息的损失函数，它被用于这篇论文中的医学图像配准任务。在医学图像配准任务中，保留边缘信息对于保持图像的解剖结构非常重要，因为它可以帮助医生更准确地诊断和治疗疾病。具体来说，Edge retention loss的计算方式是通过计算生成的图像与真实图像之间的边缘信息的差异来实现的。边缘信息可以通过计算图像的梯度来获得，因为梯度可以反映图像中像素值的变化。在这篇论文中，Matting Laplacian矩阵被用于计算图像的梯度，因为它可以帮助保留图像中的边缘信息。通过计算生成的图像与真实图像之间的边缘信息的差异。 内容损失（Content Loss） 在CNN网络中，一般认为较低层的特征描述了图像的具体视觉特征（即纹理、颜色等），较高层的特征则是较为抽象的图像内容描述。所以要比较两幅图像的内容相似性，可以比较两幅图像在CNN网络中高层特征的相似性（欧式距离）。 风格损失（Style Loss） 而要比较两幅图像的风格相似性，则可以比较它们在CNN网络中较低层特征的相似性。不过值得注意的是，不能像内容相似性计算一样，简单的采用欧式距离度量，因为低层特征包含较多的图像局部特征（即空间信息过于显著），比如两幅风格相似但内容完全不同的图像，若直接计算它们的欧式距离，则可能会产生较大的误差，认为它们风格不相似。论文中使用了Gram矩阵，用于计算不同响应层之间的联系，即在保留低层特征的同时去除图像内容的影响，只比较风格的相似性。 感知损失perceptual loss（VGG损失） 对于图像风格化，图像超分辨率重建等任务来说，早期都使用了图像像素空间的L2 loss，但是L2 loss与人眼感知的图像质量并不匹配，恢复出来的图像往往细节表现不好。 现在的研究中，L2 loss逐步被人眼感知loss所取代。人眼感知loss也被称为perceptual loss（感知损失），它与MSE（L2损失）采用图像像素进行求差的不同之处在于所计算的空间不再是图像空间。 研究者们常使用VGG等网络的特征，令φ来表示损失网络，Cj表示网络的第j层，CjHjWj表示第j层的特征图的大小，感知损失的定义与L2 loss同样的形式，只是计算的空间被转换到了特征空间。 TV Loss(Total Variation Loss) 全名为总变分损失函数，TV Loss作为一种正则项配合损失函数去调节网络学习。 即求每一个像素与其下方像素和右方像素的差的平方相加再开根号的和。 TV值和噪声是线性相关的，噪声越大TV值也会越大，所以TV值可以作为在图像复原或超分辨等任务中的一种指导正侧项，TVloss越小则图像噪声越小，图像更加平滑。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Paper-医学影像","slug":"Paper-医学影像","permalink":"http://example.com/tags/Paper-%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F/"}]},{"title":"PaperReading-医学影像中的扩散模型:综合综述","slug":"paper_医学影像中的扩散模型 综合综述","date":"2023-08-31T14:15:40.207Z","updated":"2024-05-22T02:27:56.921Z","comments":true,"path":"2023/08/31/paper_医学影像中的扩散模型 综合综述/","link":"","permalink":"http://example.com/2023/08/31/paper_%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E4%B8%AD%E7%9A%84%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%20%E7%BB%BC%E5%90%88%E7%BB%BC%E8%BF%B0/","excerpt":"","text":"DIFFUSION MODELS IN MEDICAL IMAGING: A COMPREHENSIVE SURVEY 1.摘要 去噪扩散模型是一类生成模型，最近在各种深度学习问题中引起了极大的兴趣。扩散概率模型定义了一个正向扩散阶段，在这个阶段中，输入数据通过加入高斯噪声在几个步骤中逐渐受到扰动，然后学习反向扩散过程以从有噪声的数据样本中恢复所需的无噪声数据。扩散模型因其强大的风格覆盖和生成样本的质量而广受欢迎，尽管它们已知的计算负担。利用计算机视觉的进步，医学成像领域也被观察到对扩散模型的兴趣日益增长。为了帮助研究人员浏览这一丰富的内容，本调查旨在提供医学成像学科中扩散模型的全面概述。具体来说，我们首先介绍扩散模型和三种通用扩散建模框架(即扩散概率模型、噪声条件评分网络和随机微分方程)背后的坚实理论基础和基本概念。然后，我们对医学领域的扩散模型进行了系统的分类，并提出了基于其应用，成像方式，感兴趣的器官和算法的多视角分类。为此，我们涵盖了扩散模型在医学领域的广泛应用，包括图像到图像的转换、重建、配准、分类、分割、去噪、2/3D生成、异常检测和其他与医学相关的挑战。此外，我们强调了一些选定方法的实际用例，然后讨论了扩散模型在医学领域的局限性，并提出了满足该领域需求的几个方向。最后，我们在GitHub上收集了概述的研究及其可用的开源实现。我们的目标是定期更新其中的相关最新论文。 2.引言 （1）在过去十年中，使用神经网络的生成建模一直是深度学习的主导力量。自其出现以来，生成模型在图像、音频、文本和点云等各个领域产生了巨大的影响。（2）在过去的几年里，由于一般深度学习架构的发展，人们对生成模型的兴趣重新燃起，揭示了视觉保真度和采样速度的提高。具体来说，已经出现了生成对抗网络(GANs)、变分自编码器(VAEs)和归一化流。除此之外，基于扩散过程的生成模型为现有的VAEs、EBMs、gan和规范化流提供了一种替代方案，这些模型不需要对后验分布进行对齐、估计难以处理的配分函数、引入额外的判别器网络或分别放置网络约束。（3）迄今为止，已经发现扩散模型在许多领域都很有用，从生成建模任务(如图像生成、图像超分辨率、图像绘制)到判别任务(如图像分割、分类和异常检测)。（4）最近，医学影像领域基于扩散的技术数量呈指数级增长。我们的主要贡献包括: 1）是第一篇全面涵盖扩散模型在医学成像领域应用的调查论文。具体来说，我们将全面概述所有可用的相关论文(直到2022年10月)，并展示2023年4月之前的一些最新技术。 2）我们设计了一个医学界扩散模型的多视角分类，为扩散模型及其应用的研究提供了一个系统的分类。我们将现有的扩散模型分为两类:基于变分的模型和基于分数的模型。此外，我们将扩散模型的应用分为九类:图像到图像的翻译、重建、配准、分类、分割、去噪、图像生成、异常检测和其他应用。 3）我们没有将注意力限制在应用上，并提供了一个新的分类法，其中每篇论文分别根据所提出的算法以及相关器官和成像方式进行了广泛的分类。 3）最后，我们讨论了挑战和开放的问题，并确定了新的趋势，提出了关于扩散模型在医疗领域的算法和应用的未来发展的开放问题 本次调查的动机和独特性。生成方法在医学成像领域取得了重大进展，其中一些论文只关注特定的应用，而另一些则专注于特定的图像形态。尽管在这一领域得到充分发展之前就已经发表了综述文章，但自那时以来，医学领域已经取得了许多进展。另一方面，这些调查都没有关注扩散模型在医学成像中的应用，这是推动这一研究方向向前发展的核心方面。因此，这些调查留下了明显的空白。此外，我们相信医学界可以通过回顾我们的调查提供的扩散模型的过去和未来的研究方向，从视觉扩散模型的成功产品中获得启示。 搜索策略。我们搜索了DBLP、Google Scholar和Arxiv Sanity Preserver，使用定制的搜索查询，因为它们允许定制搜索查询，并提供所有学术出版物的列表:同行评议的期刊论文或在会议或研讨会论文集中发表的论文，非同行评议的论文和预印本。值得注意的是，我们根据对其新颖性、贡献、意义的仔细评估，以及是否为医学成像领域的第一篇介绍论文，选择了论文进行详细检查。 论文的组织。 3.论文结构 理论 2.1 扩散模型在哪里适合生成式学习? 2.2 变分视角 2.2.1 去噪扩散概率模型(ddpm) 2.3 分数视角 2.3.1 噪声条件评分网络(ncsn) 2.3.2 随机微分方程(SDEs) 临床重要性 应用中的扩散模型 4.1 图像到图像的转换 4.2 重建 4.3 配准 4.4 分类 4.5 分割 4.6 去噪 4.7 图像生成 4.8 异常检测 4.9 其他应用和多任务 4.10 对比概述 未来方向和开放挑战 4.总结 本文综述了扩散模型的相关文献，重点介绍了扩散模型在医学成像领域的应用。具体来说，我们研究了扩散模型在异常检测、医学图像分割、去噪、分类、重建、配准、生成等任务中的应用。特别是，对于这些应用程序中的每一个，我们都从不同的角度提供了核心技术的分类和高级抽象。此外，我们基于技术对现有模型进行了表征，其中我们确定了基于ddpm, ncsn和SDEs的扩散建模的三种主要公式。最后，我们概述了未来研究的可能途径。 虽然我们的调查强调了医学成像中基于扩散的技术的快速增长，但我们也承认，该领域仍处于早期阶段，可能会发生变化。随着扩散模型越来越受欢迎，在这一领域的研究也越来越多，我们的调查为希望在工作中使用这些模型的研究人员和从业者提供了一个重要的起点和参考。我们希望这项调查将激发进一步的兴趣和探索扩散模型在医学领域的潜力。值得注意的是，本调查中引用的一些论文是预印本。然而，我们尽一切努力只包括来自信誉良好的来源的高质量研究，我们相信，包括预印本提供了对这个快速发展的领域当前最先进技术的全面概述。总的来说，我们相信我们的调查为扩散模型在医学成像中的应用提供了有价值的见解，并突出了未来研究的有前途的领域。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Paper-医学影像","slug":"Paper-医学影像","permalink":"http://example.com/tags/Paper-%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F/"},{"name":"Paper-综述文章","slug":"Paper-综述文章","permalink":"http://example.com/tags/Paper-%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/"}]},{"title":"PaperReading-用于医学图像分类的双制导扩散网络","slug":"paper_用于医学图像分类的双制导扩散网络","date":"2023-08-31T14:15:40.186Z","updated":"2024-05-21T12:19:54.600Z","comments":true,"path":"2023/08/31/paper_用于医学图像分类的双制导扩散网络/","link":"","permalink":"http://example.com/2023/08/31/paper_%E7%94%A8%E4%BA%8E%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%9A%84%E5%8F%8C%E5%88%B6%E5%AF%BC%E6%89%A9%E6%95%A3%E7%BD%91%E7%BB%9C/","excerpt":"","text":"DiffMIC Dual-Guidance Diffusion Network for Medical Image Classification 1.摘要 近年来，扩散概率模型在生成图像建模中表现出了显著的性能，引起了计算机视觉界的广泛关注。然而，尽管大量基于扩散的研究集中在生成任务上，但很少有研究将扩散模型应用于一般医学图像分类。在本文中，我们提出了第一个基于扩散的模型(称为DiffMIC)，通过消除医学图像中的意外噪声和扰动并鲁棒地捕获语义表示来解决一般医学图像分类问题。为了实现这一目标，我们设计了一种双条件引导策略，该策略将每个扩散步骤设定为多个粒度，以提高逐步的区域注意力。此外，我们提出在扩散前向过程中通过强制最大均值差异正则化来学习每个粒度的互信息。我们评估了DiffMIC在三种不同图像模式下的医学分类任务中的有效性，包括超声图像上的胎盘成熟度分级、皮肤镜图像上的皮肤病变分级和眼底图像上的糖尿病视网膜病变分级。我们的实验结果表明，DiffMIC在很大程度上优于最先进的方法，表明了所提出模型的通用性和有效性。 2.引言 医学图像分析不可或缺，医学图像分类是医学图像分析的一个基本步骤。深度学习方法可以帮助医生解读医学图像，这些方法有可能减少人工分类所需的时间和精力，并提高结果的准确性和一致性。然而，由于存在各种模糊病变和细粒度组织，如超声(US)、皮肤镜和眼底图像，各种形式的医学图像仍然对现有方法提出挑战。此外，在硬件限制下生成医学图像可能会导致噪声和模糊效果，从而降低图像质量，因此需要更有效的特征表示建模以实现鲁棒分类。 最近，去噪扩散概率模型在图像生成和合成任务中取得了优异的效果。虽然有一些先驱作品试图将扩散模型用于图像分割和目标检测任务，但其在高级视觉方面的潜力尚未得到充分挖掘。 我们提出了一种新的基于扩散去噪的模型DiffMIC，用于准确分类不同的医学图像模态。 （1）据我们所知，我们是第一个提出基于扩散的一般医学图像分类模型。由于医学图像的扩散过程是随机的，因此我们的方法可以适当地消除医学图像中的不良噪声。 （2）特别地，我们引入了双粒度条件指导(DCG)策略来指导去噪过程，在扩散过程中使用全局和局部先验来调节每一步。通过在较小的斑块上进行扩散过程，我们的方法可以区分具有细粒度能力的关键组织。 （3）此外，我们引入了特定条件的最大平均差异(MMD)正则化来学习每个粒度潜在空间中的互信息，使网络能够建模整个图像和补丁共享的鲁棒特征表示。 （4）我们评估了DiffMIC在胎盘成熟度分级、皮肤病变分级和糖尿病视网膜病变分级三个二维医学图像分类任务中的有效性。实验结果表明，我们的基于扩散的分类方法在所有三个任务上都一致且显著地超过了最先进的方法。 3.总结 本文提出了一种基于扩散的医学图像分类网络(DiffMIC)。我们的DiffMIC的主要思想是在普通DDPM上引入双粒度条件指导，并强制执行特定于条件的MMD正则化以提高分类性能。在三个不同图像模式的医学图像分类数据集上的实验结果表明，我们的网络比最先进的方法具有更好的性能。作为第一个基于扩散的一般医学图像分类模型，我们的DiffMIC有可能成为该领域未来研究的基本基线 4.文章笔记 通过有限的跨模态信息生成的医学图像可能会导致噪声和模糊效果，从而降低诊断准确性，因此需要更有效的特征表示建模以实现鲁棒分类","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Paper-医学影像","slug":"Paper-医学影像","permalink":"http://example.com/tags/Paper-%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F/"}]},{"title":"PaperReading-多模态医学图像融合技术综述","slug":"paper_多模态医学图像融合技术综述","date":"2023-08-30T04:42:18.434Z","updated":"2024-05-22T02:28:01.207Z","comments":true,"path":"2023/08/30/paper_多模态医学图像融合技术综述/","link":"","permalink":"http://example.com/2023/08/30/paper_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/","excerpt":"","text":"A Review of Multimodal Medical Image Fusion Techniques 1.摘要 医学图像融合是将来自多种成像方式的多幅图像进行融合，得到信息量大的融合图像，以提高医学图像的临床适用性的过程。本文对多模态医学图像融合方法进行了综述，重点介绍了该领域的最新进展，包括:(1)当前的融合方法，包括基于深度学习的融合方法;(2)医学图像融合的成像方式;(3)主要数据集上医学图像融合的性能分析。最后，本文的结论是，目前多模态医学图像融合的研究成果较为显著，发展趋势呈上升趋势，但研究领域存在诸多挑战。 2.引言 多模态医学图像融合被广泛研究。 医学图像融合通过融合同一部位的不同成像信息以获得更好的对比度，融合质量和感知体验。融合结果应满足以下条件： (a)融合后的图像应完全保留源图像的信息; (b)融合后的图像不应产生任何合成信息，如伪影; ©应避免不良状态，如误登记和噪音 传统的医学图像融合方法分为空间域和变换域。随着深度学习热潮的到来，出现了基于深度学习的医学图像融合方法，但只有CNN和U-Net网络得到了应用 本文结合近年来医学图像融合的相关论文，对该领域的研究进展及未来发展进行综述，分为以下几个部分： (1)对当前融合方法的介绍 (2)多模态融合的模式 (3)对同一数据库中具有相同评价指标的不同医学图像融合方法的数据进行比较 (4)讨论医学图像融合方法面临的挑战和未来的研究趋势 3.总结 医学图像融合方法存在的问题： 医学图像融合的评价指标多，评价指标的非唯一性限制了评价指标的应用前景 医学图像融合的创新性低，融合结果中存在的颜色失真、特征信息提取等问题只是得到了改善，而没有完全解决。其中深度学习提高了融合的效果，但研究也存在如下问题： （1）如何获取海量的数据集 （2）如何简化训练模型或提出新的训练模型 （3）部分融合方法依赖于精确的图像配准，独立性小 不同传感器获取的医学图像信息存在差异。目前的研究热点是两模融合，而三模融合的研究很少。 4.文章笔记 U-Net是基于全卷积神经网络改进的，使用数据增强可以训练少量的样本。这一优点正好弥补了医学图像数据样本量小的缺点 CNN是医学领域的新挑战;主要原因是(a)需要大量带注释的训练集数据，(b)训练时间长，©收敛问题复杂，过拟合需要反复调整。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Paper-医学影像","slug":"Paper-医学影像","permalink":"http://example.com/tags/Paper-%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F/"},{"name":"Paper-综述文章","slug":"Paper-综述文章","permalink":"http://example.com/tags/Paper-%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/"}]},{"title":"PaperReading-综述-MRI与PET交叉模态合成研究进展","slug":"paper_MRI与PET交叉模态合成研究进展","date":"2023-08-30T04:42:17.929Z","updated":"2024-05-22T02:27:47.603Z","comments":true,"path":"2023/08/30/paper_MRI与PET交叉模态合成研究进展/","link":"","permalink":"http://example.com/2023/08/30/paper_MRI%E4%B8%8EPET%E4%BA%A4%E5%8F%89%E6%A8%A1%E6%80%81%E5%90%88%E6%88%90%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95/","excerpt":"","text":"A Review on Cross-modality Synthesis from MRI to PET 1.摘要 随着时间的推移，医学影像合成越来越受欢迎。在各具特色的成像技术中，MRI和PET在医疗领域有着重要的意义。但由于PET的某些局限性，如费用、辐射暴露和缺乏可用性，人们倾向于采用跨模态合成的方法。深度学习为该领域模型的发展铺平了道路，完成了跨模态合成任务。使用这些模型合成生物医学图像可以节省患者的时间、金钱和精力，并改善疾病诊断。本文旨在总结以MRI-PET交叉模态合成为最终目标的深度学习模型 2.引言 医学成像技术在医疗保健领域发挥了显著的作用,每种类型的成像技术都提供了一些人体解剖或功能信息，这意味着像MR和PET这样的多模态生物医学图像将提供补充信息，从而实现细致和快速的诊断。 磁共振成像(MRI)和正电子发射断层扫描(PET)这两种扫描在一起使用时可以提供补充信息，帮助医生做出强有力的临床判断，但是每个患者同时拥有这两种扫描的可能性很低，这有多种原因 1)PET模式的可用性有限。发展中国家的大多数医疗中心不提供PET扫描。 2)与MRI相比，PET是一种昂贵的方式。 3) PET中放射性示踪剂的使用，增加了终生癌症风险。 尽管存在这些障碍，但PET由于其独特的分子成像特性是必不可少的。而核磁共振成像作为一种更安全的成像方式，在这些限制方面具有优势。因此，为了有效解决这一问题，采用了跨模态合成策略。 跨模态合成是在源模态下评估同一受试者的目标模态图像的过程，即在我们的案例中是MR-to-PET。该方法不仅克服了PET的划界因素，而且简化了患者的治疗，减少了患者的整体扫描时间，减少了诊所和医院的工作量，提高了疾病预测的准确性，并且可以为医学成像数据集做出贡献。简而言之，它节省了人们的时间、金钱和精力。从而改善社会的健康和生活。 属于同一受试者的MR和PET扫描具有不同的外观，这使得学习跨域映射成为一项具有挑战性的任务。深度学习已被认为是实现计算机视觉任务的重要技术。随着GAN的体系结构及其变体得到了广泛的认可,在完成跨模态合成时，同样创建了基于GAN的模型 在本文中，我们总结了现有的用于MRI-PET交叉模态合成的基于深度学习的模型。 3.论文结构 1)文献调查 A.基于深度学习的影像数据补全改善脑部疾病诊断 本文使用3D CNN来估计缺失数据。该模型以MRI为输入，预测相应的PET。 B. MRI到FDG-PET:使用3D U-Net进行多模态阿尔茨海默病分类的跨模态合成 本文采用三维U-Net模型来捕捉这些模态之间的非局部相关和非线性关系。 C. 合成PET从PET使用周期一致的生成对抗网络用于阿尔茨海默病诊断 本文开发了一个两阶段的框架，该框架在初级阶段使用周期一致的GAN (3DcGAN)进行mri -PET合成，在后续阶段构建用于AD诊断的分类方法(LM3 IL)。 D.用pix2pix从MRI推断PET 为了有一个共同的框架，引入了pix2pix GAN，本文使用相同的架构来演示使用成对数据集的MRI到PET翻译任务。 E.使用不同归一化的对抗U-Net从MRI到PET的交叉模态合成 为了减少内部协变偏移问题和对数据集中范围更广的特征的偏倚，深度网络采用了归一化技术。本文指出了BN的某些缺点，用各种归一化方法对对抗U-Net模型进行了实验，以找出其中最适合该跨模态综合任务的方法。为了解决批量归一化中生物医学图像的稀少性与小批量大小要求之间的矛盾，采用批范数(Batch Norm, BN)、层范数(Layer Norm, LN)、实例范数(Instance Norm, in)和组范数(Group Norm, GN)四种归一化方法，对不同小批量大小的对抗U-Net的性能进行了评价。定量结果表明，IN对所有样本的每个通道相对于该通道的均值和标准差进行标准化，比其他归一化技术具有更好的性能。 F.基于条件流的模态迁移生成模型 G.双向映射生成对抗网络用于脑MR到PET合成 这项工作提出了一种类似于CGAN的方法，在这种方法中，它被端到端地指导，最终目标是AD分类。当使用分类目标进行训练时，可能会影响生成语用图像的性能。这一限制是克服自适应微调GAN损耗和分类损耗。同时，通过损失微调使一般GAN训练变得稳定。 I.FREA-Unet:模态传输的频率感知U-net 本文注意到PET扫描中存在不同的频率尺度以及深度学习模型中注意学习的本质，提出了端到端的频率感知U-net模型。为了反映合成PET与真实PET的不同频率尺度，该模型在解码部分从两个不同的层获得低频和高频PET图像，并在每一层上附加一个可训练的注意模块。根据本文的定义，低频/高频层的注意力是指从频率层提取的激活图与U-net解码路径中前一层到最后一层的产出之间的兼容性分数。然后在频率尺度层中使用这些分数来突出相关特征。将获得的低频/高频层的输出融合并馈送到最后一个解码器层，以生成最终的真实PET图像。利用从注意力模块获得的不同权重分别优化低/高频尺度，使模型能够生成更精确的合成PET，具有更高的分辨率和保留的器官结构和分辨率。 2)比较 4.总结 PET数据的缺乏一直是疾病准确诊断的障碍。跨模态综合方法是解决这一问题的有效方法。在本文中，我们总结了为实现MRI-PET交叉模态合成任务而开发的深度学习模型，并对这些模型进行了比较。可以看出，由于GAN在图像合成任务中的出色性能，大多数都提出了基于GAN的架构。此外，我们还简要概述了数据集和用于评估合成PET图像的常用定量矩阵。 5.文章笔记 MRI和PET的基本原理的叙述： 磁共振成像(MRI)是一种利用磁场和无线电波对人体内部器官进行成像的结构方式。这种非侵入性诊断工具测量所需身体部位的解剖结构。而正电子发射断层扫描(PET)是一种功能模式，使用放射性示踪剂来量化人体组织和器官中的代谢活动。 UNet优点： 码器和解码器网络之间的跳跃连接有助于保留输出图像中的空间/定位信息，并使用二值交叉熵作为损失函数，有助于生成平滑输出。 U-Net架构以其通过跳过连接将低级特征从编码器传输到解码器的能力而闻名，在这一过程中对于保留MR和PET切片之间常见的低级特征非常重要 评估合成医学图像的标准: • Mean Absolute Error (MAE) • Mean Squared Error (MSE) • Peak Signal-to-Noise Ratio (PSNR) • Structure Similarity Index (SSIM) • Multi-Scale Structural Similarity (MS-SSIM) • Frechet Inception Distance (FID) 注意力的优点: 注意力学习背后的主要思想是更多地关注与任务相关的特征，而不是简单地平等地关注图像的所有部分。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Paper-医学影像","slug":"Paper-医学影像","permalink":"http://example.com/tags/Paper-%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F/"},{"name":"Paper-综述文章","slug":"Paper-综述文章","permalink":"http://example.com/tags/Paper-%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/"}]},{"title":"PaperReading-基于不完整多模态数据以诊断导向神经图像合成的疾病图像特异性学习","slug":"paper_基于不完整多模态数据以诊断导向神经图像合成的疾病图像特异性学习","date":"2023-08-30T04:42:17.921Z","updated":"2024-05-21T12:18:43.292Z","comments":true,"path":"2023/08/30/paper_基于不完整多模态数据以诊断导向神经图像合成的疾病图像特异性学习/","link":"","permalink":"http://example.com/2023/08/30/paper_%E5%9F%BA%E4%BA%8E%E4%B8%8D%E5%AE%8C%E6%95%B4%E5%A4%9A%E6%A8%A1%E6%80%81%E6%95%B0%E6%8D%AE%E4%BB%A5%E8%AF%8A%E6%96%AD%E5%AF%BC%E5%90%91%E7%A5%9E%E7%BB%8F%E5%9B%BE%E5%83%8F%E5%90%88%E6%88%90%E7%9A%84%E7%96%BE%E7%97%85%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BC%82%E6%80%A7%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"Disease-Image-Specific Learning for Diagnosis-Oriented Neuroimage Synthesis With Incomplete Multi-Modality Data 1.摘要 在多源数据的分类任务中，数据不完整是一个普遍存在的问题，特别是在多模态神经图像的疾病诊断中，为了跟踪这一问题，人们提出了一些方法，通过输入缺失的神经图像来利用所有可用的被试。然而，这些方法通常将图像合成和疾病诊断视为两个独立的任务，从而忽略了不同模态所传达的特异性，即不同模态可能突出大脑中不同的疾病相关区域。为此，我们提出了一种疾病图像特异性深度学习(DSDL)框架，用于使用不完整多模态神经图像用于联合神经图像合成和疾病诊断。具体而言，以每次全脑扫描为输入，我们首先设计了一个带有空间余弦模块的疾病图像特异性网络(DSNet)，以隐式建模疾病图像特异性。然后，我们开发了一个特征一致性生成对抗网络(FGAN)来补全缺失的神经图像，其中合成图像的特征映射(由DSNet生成)与其各自的真实图像被鼓励保持一致，同时保留疾病图像特定信息。由于我们的FGAN与DSNet相关，缺失的神经图像可以以诊断为导向的方式合成。在三个数据集上的实验结果表明，我们的方法不仅可以生成合理的神经图像，而且在阿尔茨海默病识别和轻度认知障碍转换预测两项任务上都取得了较好的效果 2.引言 多模态神经影像学数据已被证明可以提供互补信息，以提高疾病的的计算机辅助诊断性能。数据缺失问题一直是使用多模态神经成像数据进行脑疾病自动诊断的常见挑战。 传统方法只使用模态完整的被试而丢弃模态不完整的被试，这种策略减少了训练样本数量，忽略了数据缺失受试者提供的有用信息，从而降低了诊断性能。已经提出了几种数据输入方法，利用数据完整受试者的特征来估计缺失数据受试者的手工特征。然而，这些方法依赖于手工制作的图像特征，可能无法区分脑部疾病的诊断，从而导致次优的学习性能。 更有希望的替代方法是通过深度学习直接估计缺失数据。然而之前的方法平等地处理每个脑容量中的所有体素，从而忽略了多模态神经成像数据中传达的疾病图像特异性。疾病图像的特异性是双重的： （1）并不是MRI/PET扫描的所有区域都与特定的脑部疾病有关 （2）与疾病相关的大脑区域可能在不同模态中是有差别的 对于第一个方面，现有的深度学习方法通常在图像合成过程中平等对待所有大脑区域，某些区域与疾病是高度相关的相比于其他区域。对于第二个方面，现有方法直接基于另一种模态图像合成一种模态图像，而不考虑疾病相关区域的模态差距。值得注意的是，已有研究表明，疾病诊断模型可以通过感兴趣区域和解剖标志来隐式或显式地捕捉疾病图像特异性。因此，为了捕获和利用疾病图像的特异性，直观地需要将疾病诊断和图像合成整合到一个统一的框架中，以诊断为导向的方式输入缺失的神经图像 在本文中，我们提出了一个疾病图像特异性深度学习(DSDL)框架，用于使用不完整的多模态神经图像用于联合疾病诊断和图像合成(见图1)。如图1a和1b所示，我们的方法主要包含两个单模态疾病图像特异性网络(DSNet)，用于基于MRI和pet的疾病诊断，以及一个用于图像合成的特征一致性生成对抗网络(FGAN)。本文中，DSNet对基于MRI和pet的特征图中的疾病图像特异性进行编码，以辅助FGAN的训练，而FGAN则对缺失图像进行补全以提高诊断性能。由于DSNet和FGAN可以联合训练，因此可以以诊断为导向的方式合成缺失的神经图像。使用完整的MRI和PET扫描(植入后)，我们可以通过提出的多模态DSNet进行疾病诊断(如图1c所示)。在三个公开数据集上的实验结果表明，我们的方法不仅可以合成合理的MRI和PET图像，而且在AD识别和MCI转换预测方面都取得了最先进的结果。 与我们之前的工作相比，本工作的贡献如下： (1)提出了一种统一的DSDL框架，用于不完整多模态神经图像的联合图像合成和AD诊断。缺失的图像以诊断为导向的方式输入，因此从诊断的角度来看，合成的神经图像与真实的神经图像更一致。 (2)设计了空间余弦模型，对全脑MRI/PET扫描的疾病图像特异性进行隐式自动建模。 (3)提出了一种特征一致性约束，可以帮助图像合成模型在模态转换过程中保留疾病相关信息 3.总结 我们提出了一种基于不完全多模态数据的面向任务的神经图像合成的疾病图像特异性深度学习框架，其中使用诊断网络为图像合成网络提供疾病图像特异性。具体来说，我们设计了一个单模态疾病图像特异性网络(DSNet)，对全脑图像进行训练，以隐式捕获MRI和PET传达的疾病相关信息。然后，我们开发了一个特征一致性生成对抗网络(FGAN)来合成缺失的神经图像，通过鼓励每个合成图像的特征映射与其各自的真实图像保持一致。我们进一步提出了一个多模态DSNet (mDSNet)，用于使用完整的(植入后)MRI和PET扫描进行疾病诊断。在三个公共数据集上的实验表明，我们的方法可以生成合理的神经图像，并在AD识别和MCI转换预测方面达到了最先进的性能。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Paper-医学影像","slug":"Paper-医学影像","permalink":"http://example.com/tags/Paper-%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F/"}]},{"title":"Hello World","slug":"hello-world","date":"2023-08-30T03:04:39.444Z","updated":"2023-08-30T03:04:39.444Z","comments":true,"path":"2023/08/30/hello-world/","link":"","permalink":"http://example.com/2023/08/30/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"},{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"},{"name":"笔记","slug":"笔记","permalink":"http://example.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Code-量化交易","slug":"Code-量化交易","permalink":"http://example.com/tags/Code-%E9%87%8F%E5%8C%96%E4%BA%A4%E6%98%93/"},{"name":"编程基础知识","slug":"编程基础知识","permalink":"http://example.com/tags/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"Paper-Robot_list","slug":"Paper-Robot-list","permalink":"http://example.com/tags/Paper-Robot-list/"},{"name":"Paper-ICRA","slug":"Paper-ICRA","permalink":"http://example.com/tags/Paper-ICRA/"},{"name":"Paper-TRO","slug":"Paper-TRO","permalink":"http://example.com/tags/Paper-TRO/"},{"name":"Paper-CVPR","slug":"Paper-CVPR","permalink":"http://example.com/tags/Paper-CVPR/"},{"name":"Code-isaacsim","slug":"Code-isaacsim","permalink":"http://example.com/tags/Code-isaacsim/"},{"name":"Code-强化学习","slug":"Code-强化学习","permalink":"http://example.com/tags/Code-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"},{"name":"Code-模型库","slug":"Code-模型库","permalink":"http://example.com/tags/Code-%E6%A8%A1%E5%9E%8B%E5%BA%93/"},{"name":"Code-机器人","slug":"Code-机器人","permalink":"http://example.com/tags/Code-%E6%9C%BA%E5%99%A8%E4%BA%BA/"},{"name":"Code-图像处理","slug":"Code-图像处理","permalink":"http://example.com/tags/Code-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"},{"name":"Code-文件处理","slug":"Code-文件处理","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"},{"name":"Code-分类","slug":"Code-分类","permalink":"http://example.com/tags/Code-%E5%88%86%E7%B1%BB/"},{"name":"Qt","slug":"Qt","permalink":"http://example.com/tags/Qt/"},{"name":"Code-生成","slug":"Code-生成","permalink":"http://example.com/tags/Code-%E7%94%9F%E6%88%90/"},{"name":"医学","slug":"医学","permalink":"http://example.com/tags/%E5%8C%BB%E5%AD%A6/"},{"name":"Paper-医学影像","slug":"Paper-医学影像","permalink":"http://example.com/tags/Paper-%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F/"},{"name":"Paper-综述文章","slug":"Paper-综述文章","permalink":"http://example.com/tags/Paper-%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/"}]}