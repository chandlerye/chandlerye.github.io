{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"分类","date":"2023-08-30T09:13:13.000Z","updated":"2023-08-30T09:26:25.168Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2023-08-30T09:14:13.000Z","updated":"2023-08-30T09:27:18.175Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"Sylar项目-日志系统","date":"2024-01-30T02:57:51.676Z","updated":"2024-01-02T07:43:16.629Z","comments":true,"path":"draft/coding_sylar_data_system.html","permalink":"http://example.com/draft/coding_sylar_data_system.html","excerpt":"","text":"A Review on Cross-modality Synthesis from MRI to PET 1.摘要 2.引言 3.论文结构 4.总结"},{"title":"Note-ALL","date":"2024-01-30T09:21:20.595Z","updated":"2024-01-30T09:21:20.595Z","comments":true,"path":"draft/note-all.html","permalink":"http://example.com/draft/note-all.html","excerpt":"","text":"JS散度 一般地，JS散度是对称的，其取值是 0 到 1 之间。如果两个分布 P,Q 离得很远，完全没有重叠的时候，即使两个分布的中心离的很近，而JS散度值是一个常数。这在学习算法中是比较致命的，这就意味这这一点的梯度为 0。梯度消失了。"},{"title":"衡量分布差异性的损失函数","date":"2024-01-30T10:00:11.726Z","updated":"2024-01-30T10:00:11.726Z","comments":true,"path":"draft/note-distant.html","permalink":"http://example.com/draft/note-distant.html","excerpt":"","text":"EMD 对于离散的概率分布，Wasserstein距离也被描述为推土距离(EMD)。如果我们将分布想象为两个有一定存土量的土堆，那么EMD就是将一个土堆转换为另一个土堆所需的最小总工作量。"},{"title":"Note-特征融合代码","date":"2024-02-04T01:47:09.506Z","updated":"2024-02-04T01:47:09.506Z","comments":true,"path":"draft/note-feature_fusion.html","permalink":"http://example.com/draft/note-feature_fusion.html","excerpt":"","text":"特征不融合 Feature不融合，多尺度的feture分别进行预测，然后对预测结果进行综合，如Single Shot MultiBox Detector (SSD) , Multi-scale CNN(MS-CNN) 多尺度特征融合（Multi-scale Feature Fusion） 金字塔池化（Pyramid Pooling） 论文：Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition 特征金字塔网络（Feature Pyramid Networks） 论文：Feature Pyramid Networks for Object Detection 短连接（Short Connections） 论文：Deeply Supervised Salient Object Detection with Short Connections 密集连接的卷积网络（Densely Connected Convolutional Networks） 论文：Densely Connected Convolutional Networks 双路径网络（Dual Path Networks） 论文：Dual Path Networks 残差连接（Residual Connections） 论文：Deep Residual Learning for Image Recognition 注意力机制（Attention Mechanism） 论文：Show, Attend and Tell: Neural Image Caption Generation with Visual Attention Non-local Neural Networks CBAM: Convolutional Block Attention Module 胶囊网络（Capsule Networks） Dynamic Routing Between Capsules 图卷积网络（Graph Convolutional Networks） Semi-Supervised Classification with Graph Convolutional Networks Graph Convolutional Networks"}],"posts":[{"title":"Note-1922年《劳动法大纲》","slug":"note-LaoDongFaDaGang","date":"2024-02-26T03:43:42.938Z","updated":"2024-02-26T03:53:59.359Z","comments":true,"path":"2024/02/26/note-LaoDongFaDaGang/","link":"","permalink":"http://example.com/2024/02/26/note-LaoDongFaDaGang/","excerpt":"","text":"中国劳动组合书记部拟定的劳动法案大纲 （一九二二年八月） （一）承认劳动者之集会结社权。 （二）承认劳动者之同盟罢工权。 （三）承认劳动者之团体的契约缔结权。 （四）承认劳动者之国际的联合。 （五）日工不得过八小时，夜工不得过六小时，每星期连续四十二小时〈休息〉。 （六）十八岁以下青年男女工人及吃力的工作，不得过六小时。 （七）禁止超过法定的工作时间。如在特别情形须得工会同意才得增加工作时间。 （八）农工的工作时间虽可超过八小时，但所超过之工作时间的工值，须按照八小时制的基础计算。 （九）须以法律担保一般不掠夺别人劳动之农人的农产品价格，此项价格由农人代表提出，以法律规定之。 （十）吃力的工作及有碍卫生的工作，对于十八岁以下的男女工人，绝对禁止超过法定时间。绝对禁止女工及十八岁以下男工作夜工。 （十一）体力的女工产前产后各八星期休工；其他工作之女工，产前产后各六星期休工，均照常领取工资。 （十二）禁止雇用十六岁以下之男女童工。 （十三）为保障工人适当以至低限度的工钱，国家须制定这种保障法律。当立此项法律时，须准全国总工会代表出席。无论公私企业或机关的工资，均不得低于此项法律保障的至低限度。 （十四）各种工人，由他们产业组合或职业组合保障，可选举代表参加政府经济机关，及选举代表参加政府企业机关，及政府所管理的私人企业或机关之权。 （十五）国家对于全国公私各企业均须设立劳动检查局。 （十六）国家保障工人有完全参加国家所设劳动检查局之权。 （十七）一切保险事业，须由工人参加规定之，以保障所有在政府的，公共的，私人的企业和机关内的工人之损失或危险。保险费完全由雇主或国家出之，受保险者决不分担。 （十八）各种工人和雇用人，一年工作中有一月之休息，半年中有两星期之休息，并领薪之权。 （十九）国家须以法律保证男女工人有受补习教育的机会。 （附白）工友们！这是本部斟酌各国劳动法拟定的，我们认为是最低的限度，并不过高，我们是非要国会都要通过不可的。但不知各位对于这十九条认为满足不满足？完备不完备？如有认为要增加或更改的请快快来函示知，以便修改。这是关于我们劳动阶级切身的利害，我们不可忽视呀！ 根据一九二二年九月三日出版的《先驱》第十一期刊印 引自：中国劳动组合书记部拟定的劳动法案大纲","categories":[{"name":"笔记","slug":"笔记","permalink":"http://example.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"法律","slug":"法律","permalink":"http://example.com/tags/%E6%B3%95%E5%BE%8B/"}]},{"title":"Note-悬雍垂","slug":"note-uvula","date":"2024-02-04T01:39:41.593Z","updated":"2024-02-04T01:45:36.209Z","comments":true,"path":"2024/02/04/note-uvula/","link":"","permalink":"http://example.com/2024/02/04/note-uvula/","excerpt":"","text":"悬雍垂（uvule[ˈjuːvjʊlə]）又名腭垂，俗称“小舌”、“吊钟”，是人体口腔器官，悬挂于软腭正中间的末端。悬雍垂的功能是在饮食时上升堵住食物通过鼻腔进入气管的通道，从而使食物进入食道。在语言上，部分语言需要利用小舌的振动发出小舌音。 悬雍垂与其连接的软腭，与睡眠呼吸中止症有相当程度的关联，经常好发于中年男性，可以透过手术治疗：例如悬雍垂腭咽整型手术（UPPP手术）、微创软腭支架手术，可以有效解决呼吸中止，以及打鼾音量的问题。","categories":[{"name":"笔记","slug":"笔记","permalink":"http://example.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"医学","slug":"医学","permalink":"http://example.com/tags/%E5%8C%BB%E5%AD%A6/"}]},{"title":"Coding-Hexo运营维护","slug":"coding_hexo","date":"2024-01-23T04:16:53.417Z","updated":"2024-01-23T04:23:22.755Z","comments":true,"path":"2024/01/23/coding_hexo/","link":"","permalink":"http://example.com/2024/01/23/coding_hexo/","excerpt":"","text":"生成推送 为文章添加目录 公式使用 生成推送 1hexo g -d 为文章添加目录 安装插件 1npm install hexo-toc --save 配置博客根目录下的_config.yml文件 12toc: maxdepth: 3 在需要展示目录的地方添加： 1&lt;!-- toc --&gt; 注意：显示的目录只会包含代码段 &lt; !-- toc --&gt;之后的内容 公式使用 安装 12npm un hexo-renderer-marked --savenpm i hexo-renderer-markdown-it-plus --save 在 _config.yml中配置 12345678910111213141516markdown_it_plus: highlight: true html: true xhtmlOut: true breaks: true langPrefix: linkify: true typographer: quotes: “”‘’ plugins: - plugin: name: markdown-it-katex enable: true - plugin: name: markdown-it-mark enable: false 使 mathjax生效 12title: Hello Worldmathjax: true","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://example.com/tags/Hexo/"}]},{"title":"Coding-GAN","slug":"coding_GAN","date":"2024-01-23T03:44:17.535Z","updated":"2024-01-23T03:49:48.721Z","comments":true,"path":"2024/01/23/coding_GAN/","link":"","permalink":"http://example.com/2024/01/23/coding_GAN/","excerpt":"","text":"摘要 cgan.py 摘要 生成器的输入为噪声 zzz ，要求生成器能生成真图片，要求判别器能判别生成的真图片和真正的真图片 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170import argparseimport osimport numpy as npimport mathimport torchvision.transforms as transformsfrom torchvision.utils import save_imagefrom torch.utils.data import DataLoaderfrom torchvision import datasetsfrom torch.autograd import Variableimport torch.nn as nnimport torch.nn.functional as Fimport torchos.makedirs(&quot;images&quot;, exist_ok=True)parser = argparse.ArgumentParser()parser.add_argument(&quot;--n_epochs&quot;, type=int, default=200, help=&quot;number of epochs of training&quot;)parser.add_argument(&quot;--batch_size&quot;, type=int, default=64, help=&quot;size of the batches&quot;)parser.add_argument(&quot;--lr&quot;, type=float, default=0.0002, help=&quot;adam: learning rate&quot;)parser.add_argument(&quot;--b1&quot;, type=float, default=0.5, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--b2&quot;, type=float, default=0.999, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--n_cpu&quot;, type=int, default=8, help=&quot;number of cpu threads to use during batch generation&quot;)parser.add_argument(&quot;--latent_dim&quot;, type=int, default=100, help=&quot;dimensionality of the latent space&quot;)parser.add_argument(&quot;--img_size&quot;, type=int, default=28, help=&quot;size of each image dimension&quot;)parser.add_argument(&quot;--channels&quot;, type=int, default=1, help=&quot;number of image channels&quot;)parser.add_argument(&quot;--sample_interval&quot;, type=int, default=400, help=&quot;interval betwen image samples&quot;)opt = parser.parse_args()print(opt)img_shape = (opt.channels, opt.img_size, opt.img_size)cuda = True if torch.cuda.is_available() else Falseclass Generator(nn.Module): def __init__(self): super(Generator, self).__init__() def block(in_feat, out_feat, normalize=True): layers = [nn.Linear(in_feat, out_feat)] if normalize: layers.append(nn.BatchNorm1d(out_feat, 0.8)) layers.append(nn.LeakyReLU(0.2, inplace=True)) return layers self.model = nn.Sequential( *block(opt.latent_dim, 128, normalize=False), *block(128, 256), *block(256, 512), *block(512, 1024), nn.Linear(1024, int(np.prod(img_shape))), nn.Tanh() ) def forward(self, z): img = self.model(z) img = img.view(img.size(0), *img_shape) return imgclass Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() self.model = nn.Sequential( nn.Linear(int(np.prod(img_shape)), 512), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 256), nn.LeakyReLU(0.2, inplace=True), nn.Linear(256, 1), nn.Sigmoid(), ) def forward(self, img): img_flat = img.view(img.size(0), -1) validity = self.model(img_flat) return validity# Loss functionadversarial_loss = torch.nn.BCELoss()# Initialize generator and discriminatorgenerator = Generator()discriminator = Discriminator()if cuda: generator.cuda() discriminator.cuda() adversarial_loss.cuda()# Configure data loaderos.makedirs(&quot;../../data/mnist&quot;, exist_ok=True)dataloader = torch.utils.data.DataLoader( datasets.MNIST( &quot;../../data/mnist&quot;, train=True, download=True, transform=transforms.Compose( [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])] ), ), batch_size=opt.batch_size, shuffle=True,)# Optimizersoptimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor# ----------# Training# ----------for epoch in range(opt.n_epochs): for i, (imgs, _) in enumerate(dataloader): # Adversarial ground truths valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False) fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False) # Configure input real_imgs = Variable(imgs.type(Tensor)) # ----------------- # Train Generator # ----------------- optimizer_G.zero_grad() # Sample noise as generator input z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim)))) # Generate a batch of images gen_imgs = generator(z) # Loss measures generator&#x27;s ability to fool the discriminator g_loss = adversarial_loss(discriminator(gen_imgs), valid) g_loss.backward() optimizer_G.step() # --------------------- # Train Discriminator # --------------------- optimizer_D.zero_grad() # Measure discriminator&#x27;s ability to classify real from generated samples real_loss = adversarial_loss(discriminator(real_imgs), valid) fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake) d_loss = (real_loss + fake_loss) / 2 d_loss.backward() optimizer_D.step() print( &quot;[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]&quot; % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item()) ) batches_done = epoch * len(dataloader) + i if batches_done % opt.sample_interval == 0: save_image(gen_imgs.data[:25], &quot;images/%d.png&quot; % batches_done, nrow=5, normalize=True)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-模型","slug":"Code-模型","permalink":"http://example.com/tags/Code-%E6%A8%A1%E5%9E%8B/"}]},{"title":"Coding-cGAN","slug":"coding_cGAN","date":"2024-01-23T03:30:58.506Z","updated":"2024-01-23T03:49:34.435Z","comments":true,"path":"2024/01/23/coding_cGAN/","link":"","permalink":"http://example.com/2024/01/23/coding_cGAN/","excerpt":"","text":"摘要 cgan.py 摘要 生成器的输入为噪声 zzz 和标签 lll ，要求生成器能生成真图片，要求判别器能判别生成的真图片和真正的真图片 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204import argparseimport osimport numpy as npimport mathimport torchvision.transforms as transformsfrom torchvision.utils import save_imagefrom torch.utils.data import DataLoaderfrom torchvision import datasetsfrom torch.autograd import Variableimport torch.nn as nnimport torch.nn.functional as Fimport torchos.makedirs(&quot;images&quot;, exist_ok=True)parser = argparse.ArgumentParser()parser.add_argument(&quot;--n_epochs&quot;, type=int, default=200, help=&quot;number of epochs of training&quot;)parser.add_argument(&quot;--batch_size&quot;, type=int, default=64, help=&quot;size of the batches&quot;)parser.add_argument(&quot;--lr&quot;, type=float, default=0.0002, help=&quot;adam: learning rate&quot;)parser.add_argument(&quot;--b1&quot;, type=float, default=0.5, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--b2&quot;, type=float, default=0.999, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--n_cpu&quot;, type=int, default=8, help=&quot;number of cpu threads to use during batch generation&quot;)parser.add_argument(&quot;--latent_dim&quot;, type=int, default=100, help=&quot;dimensionality of the latent space&quot;)parser.add_argument(&quot;--n_classes&quot;, type=int, default=10, help=&quot;number of classes for dataset&quot;)parser.add_argument(&quot;--img_size&quot;, type=int, default=32, help=&quot;size of each image dimension&quot;)parser.add_argument(&quot;--channels&quot;, type=int, default=1, help=&quot;number of image channels&quot;)parser.add_argument(&quot;--sample_interval&quot;, type=int, default=400, help=&quot;interval between image sampling&quot;)opt = parser.parse_args()print(opt)img_shape = (opt.channels, opt.img_size, opt.img_size)cuda = True if torch.cuda.is_available() else Falseclass Generator(nn.Module): def __init__(self): super(Generator, self).__init__() self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes) def block(in_feat, out_feat, normalize=True): layers = [nn.Linear(in_feat, out_feat)] if normalize: layers.append(nn.BatchNorm1d(out_feat, 0.8)) layers.append(nn.LeakyReLU(0.2, inplace=True)) return layers self.model = nn.Sequential( *block(opt.latent_dim + opt.n_classes, 128, normalize=False), *block(128, 256), *block(256, 512), *block(512, 1024), nn.Linear(1024, int(np.prod(img_shape))), nn.Tanh() ) def forward(self, noise, labels): # Concatenate label embedding and image to produce input gen_input = torch.cat((self.label_emb(labels), noise), -1) img = self.model(gen_input) img = img.view(img.size(0), *img_shape) return imgclass Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes) self.model = nn.Sequential( nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 512), nn.Dropout(0.4), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 512), nn.Dropout(0.4), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 1), ) def forward(self, img, labels): # Concatenate label embedding and image to produce input d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1) validity = self.model(d_in) return validity# Loss functionsadversarial_loss = torch.nn.MSELoss()# Initialize generator and discriminatorgenerator = Generator()discriminator = Discriminator()if cuda: generator.cuda() discriminator.cuda() adversarial_loss.cuda()# Configure data loaderos.makedirs(&quot;../../data/mnist&quot;, exist_ok=True)dataloader = torch.utils.data.DataLoader( datasets.MNIST( &quot;../../data/mnist&quot;, train=True, download=True, transform=transforms.Compose( [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])] ), ), batch_size=opt.batch_size, shuffle=True,)# Optimizersoptimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensorLongTensor = torch.cuda.LongTensor if cuda else torch.LongTensordef sample_image(n_row, batches_done): &quot;&quot;&quot;Saves a grid of generated digits ranging from 0 to n_classes&quot;&quot;&quot; # Sample noise z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim)))) # Get labels ranging from 0 to n_classes for n rows labels = np.array([num for _ in range(n_row) for num in range(n_row)]) labels = Variable(LongTensor(labels)) gen_imgs = generator(z, labels) save_image(gen_imgs.data, &quot;images/%d.png&quot; % batches_done, nrow=n_row, normalize=True)# ----------# Training# ----------for epoch in range(opt.n_epochs): for i, (imgs, labels) in enumerate(dataloader): batch_size = imgs.shape[0] # Adversarial ground truths valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False) fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False) # Configure input real_imgs = Variable(imgs.type(FloatTensor)) labels = Variable(labels.type(LongTensor)) # ----------------- # Train Generator # ----------------- optimizer_G.zero_grad() # Sample noise and labels as generator input z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim)))) gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size))) # Generate a batch of images gen_imgs = generator(z, gen_labels) # Loss measures generator&#x27;s ability to fool the discriminator 要求生成器能生成真图片 validity = discriminator(gen_imgs, gen_labels) g_loss = adversarial_loss(validity, valid) g_loss.backward() optimizer_G.step() # --------------------- # Train Discriminator # --------------------- optimizer_D.zero_grad() # Loss for real images validity_real = discriminator(real_imgs, labels) d_real_loss = adversarial_loss(validity_real, valid) # Loss for fake images validity_fake = discriminator(gen_imgs.detach(), gen_labels) d_fake_loss = adversarial_loss(validity_fake, fake) # Total discriminator loss 要求判别器能判别生成的真图片和真正的真图片 d_loss = (d_real_loss + d_fake_loss) / 2 d_loss.backward() optimizer_D.step() print( &quot;[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]&quot; % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item()) ) batches_done = epoch * len(dataloader) + i if batches_done % opt.sample_interval == 0: sample_image(n_row=10, batches_done=batches_done)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-模型","slug":"Code-模型","permalink":"http://example.com/tags/Code-%E6%A8%A1%E5%9E%8B/"}]},{"title":"Note-临床实验","slug":"note-clinical_trial","date":"2024-01-22T02:20:16.691Z","updated":"2024-02-04T01:42:23.635Z","comments":true,"path":"2024/01/22/note-clinical_trial/","link":"","permalink":"http://example.com/2024/01/22/note-clinical_trial/","excerpt":"","text":"0期临床试验 使用微剂量健康受试者或病人进行给药研究。 所谓微剂量，是指低于通过临床前毒理学研究获得的动物安全性数据而推导出的拟用于人体可能产生临床药理学作用剂量的1/100，同时，最大剂量不超过100ug的剂量。 Ⅰ期临床试验 从初始安全剂量开始，逐渐加大，观察人体对该种新药的耐受程度，以确定人体可接受而又不会导致毒副反应发生的剂量大小。 之后将进行多次给药试验，以确定适合于Ⅱ期临床试验所需的剂量和程序。同时，还必须进行人体的单剂量和多剂量的药动学研究，以为Ⅱ期临床试验提供合适的治疗方案。Ⅰ期临床试验通常由健康的志愿者参与。一般而言，Ⅰ期临床试验总共需要试验10~80个病人。 Ⅱ期临床试验 用较小总体的选定适应症的患者，对药物的疗效和安全性进行临床研究，其间将重点观察新药的治疗效果和不良反应。 同时，还要对新药的药动学和生物利用度方面进行研究，以确定患者与健康人的药动学差异。Ⅱ期临床试验的主要目的是为Ⅲ期临床试验做准备，以确定初步的临床适应症和治疗方案。Ⅱ期临床试验总共需要试验100-200个病人。 Ⅲ期临床试验 对已通过Ⅱ期临床试验确定了其疗效的新药，与现有已知活性的药物或无药理活性的安慰剂进行对照试验。 该期试验对于患者的选择非常严格，其还必须具有明确的疗效标准和安全性评价标准。新药在经过对照试验后，将对其疗效和长期安全性进行全面的评价，以判断其是否具有治疗学和安全性特征，这决定着其是否能够批准上市销售。Ⅲ期临床试验总共需要试验300－500个病人，最少要测试100次，否则统计学上会有误差，对照组的数量则无具体规定。 Ⅳ期临床试验 在新药推出后，通过大量调查药物对病人的临床效果及情况，监视新药有无效，如何最好地使用以及副作用的发生机会和程度。 若疗效不理想或出现严重的副作用而且发生率较高，管制部门则会将那新药召回和退市。第4期临床试验会一直进行，只要仍有很多人用这种药物。","categories":[{"name":"笔记","slug":"笔记","permalink":"http://example.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"医学","slug":"医学","permalink":"http://example.com/tags/%E5%8C%BB%E5%AD%A6/"}]},{"title":"Coding-torch datasets总结","slug":"coding_mydatasets","date":"2024-01-08T15:15:12.648Z","updated":"2024-01-23T03:54:25.665Z","comments":true,"path":"2024/01/08/coding_mydatasets/","link":"","permalink":"http://example.com/2024/01/08/coding_mydatasets/","excerpt":"","text":"torch内置datasets的使用方法 适用于图片分类的datasets(数据放在不同的文件夹下表示不同类别) 适用于图片分割，目标检测的datasets(数据和标签都是图像) 子数据集划分 torch内置datasets的使用方法 1234567891011121314151617import torchimport torchvisionimport torchvision.transforms as transforms# 数据预处理transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # 图像归一化])# 加载训练集和测试集trainset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=True, download=True, transform=transform)testset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=False, download=True, transform=transform)# 创建数据加载器trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2) 适用于图片分类的datasets(数据放在不同的文件夹下表示不同类别) 12345678910111213141516171819202122232425262728293031import glob # 导入用于文件路径匹配的模块from torchvision import transforms # 导入图像转换模块from torch.utils import data # 导入PyTorch数据工具模块from PIL import Image # 导入PIL图像处理库# 标准化数据transforms = transforms.Compose([ transforms.ToTensor(), # 将图像转换为张量 transforms.Resize((256, 256)), # 调整图像大小为256x256 transforms.Normalize(mean=0.5, std=0.5) # 标准化图像数据])class my_dataset(data.Dataset): def __init__(self, imgs_path, annos_path): self.imgs_path = imgs_path # 图像文件路径 self.annos_path = annos_path # 标签文件路径 def __getitem__(self, index): img_path = self.imgs_path[index] # 获取图像路径 pil_img = Image.open(img_path) # 使用PIL打开图像 pil_img = transforms(pil_img) # 对图像进行预处理 anno_path = self.annos_path[index] # 获取标签路径 anno_img = Image.open(anno_path) # 使用PIL打开标签图像 pil_anno = transforms(anno_img) # 对标签图像进行预处理 return pil_img, pil_anno# 创建训练和验证数据集train_dataset = CustomDataset(train_data_path, transform=transform)val_dataset = CustomDataset(val_data_path, transform=transform) 适用于图片分割，目标检测的datasets(数据和标签都是图像) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import glob # 导入用于文件路径匹配的模块from torchvision import transforms # 导入图像转换模块from torch.utils import data # 导入PyTorch数据工具模块from PIL import Image # 导入PIL图像处理库# 标准化数据transforms = transforms.Compose([ transforms.ToTensor(), # 将图像转换为张量 transforms.Resize((256, 256)), # 调整图像大小为256x256 transforms.Normalize(mean=0.5, std=0.5) # 标准化图像数据])class my_dataset(data.Dataset): def __init__(self, imgs_path, annos_path): self.imgs_path = imgs_path # 图像文件路径 self.annos_path = annos_path # 标签文件路径 def __getitem__(self, index): img_path = self.imgs_path[index] # 获取图像路径 pil_img = Image.open(img_path) # 使用PIL打开图像 pil_img = transforms(pil_img) # 对图像进行预处理 anno_path = self.annos_path[index] # 获取标签路径 anno_img = Image.open(anno_path) # 使用PIL打开标签图像 pil_anno = transforms(anno_img) # 对标签图像进行预处理 return pil_img, pil_anno def __len__(self): return len(self.imgs_path) # 返回数据集的长度# 训练数据集导入imgs_path = glob.glob(&#x27;facade/train_picture/*.png&#x27;) # 匹配训练图像文件路径label_path = glob.glob(&#x27;facade/train_label/*.jpg&#x27;) # 匹配训练标签文件路径# 测试数据集导入test_imgs_path = glob.glob(&#x27;facade/test_picture/*.png&#x27;) # 匹配测试图像文件路径test_label_path = glob.glob(&#x27;facade/test_label/*.jpg&#x27;) # 匹配测试标签文件路径# 对数据和标签排序，确保一一对应imgs_path = sorted(imgs_path)label_path = sorted(label_path)test_imgs_path = sorted(test_imgs_path)test_label_path = sorted(test_label_path)train_dataset = my_dataset(imgs_path, label_path) test_dataset = my_dataset(test_imgs_path, test_label_path) # 创建测试数据集对象train_loader = data.DataLoader(train_dataset, batch_size=4, shuffle=True) # 创建训练数据加载器test_loader = data.DataLoader(test_dataset, batch_size=4, shuffle=False) # 创建测试数据加载器 子数据集划分 1234567891011121314151617181920from torch.utils.data import Subset# 加载数据集，并分成两个子集train_data = dsets.ImageFolder(root=&#x27;data_self/train&#x27;, transform=transform)test_data = dsets.ImageFolder(root=&#x27;data_self/test&#x27;, transform=transform)# 创建训练数据集的索引列表train_indices1 = list(range(0, len(train_data), 2))train_indices2 = list(range(1, len(train_data), 2))# 创建训练子集1和训练子集2train_data1 = Subset(train_data, train_indices1)train_data2 = Subset(train_data, train_indices2)# 创建测试数据集的索引列表test_indices1 = list(range(0, len(test_data), 2))test_indices2 = list(range(1, len(test_data), 2))# 创建测试子集1和测试子集2test_data1 = Subset(test_data, test_indices1)test_data2 = Subset(test_data, test_indices2)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-数据集","slug":"Code-数据集","permalink":"http://example.com/tags/Code-%E6%95%B0%E6%8D%AE%E9%9B%86/"}]},{"title":"Coding-猫狗数据集分类-基于CNN","slug":"coding_network_CNN","date":"2024-01-08T14:22:20.995Z","updated":"2024-01-23T03:54:29.595Z","comments":true,"path":"2024/01/08/coding_network_CNN/","link":"","permalink":"http://example.com/2024/01/08/coding_network_CNN/","excerpt":"","text":"摘要 main.py 摘要 训练一个图片分类神经网络（CNN） 文件目录 . ├── train │ ├── cats │ └── dogs └── validation ├── cats └── dogs 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140import torchimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import DataLoader, Datasetfrom torchvision import transformsfrom torchvision.datasets import ImageFolder# 定义自定义数据集类class CustomDataset(Dataset): def __init__(self, root_dir, transform=None): self.dataset = ImageFolder(root_dir, transform=transform) def __len__(self): return len(self.dataset) def __getitem__(self, idx): image, label = self.dataset[idx] return image, label# 定义卷积神经网络模型class CNNModel(nn.Module): def __init__(self, num_classes): super(CNNModel, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), ) self.classifier = nn.Sequential( nn.Linear(32 * 3 * 3, 256), nn.ReLU(inplace=True), nn.Linear(256, num_classes) ) def forward(self, x): x = self.features(x) x = x.view(x.size(0), -1) x = self.classifier(x) return x# 设置训练和验证数据集的路径train_data_path = &#x27;data1/train&#x27;val_data_path = &#x27;data1/validation&#x27;# 定义图像转换transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])# 创建训练和验证数据集train_dataset = CustomDataset(train_data_path, transform=transform)val_dataset = CustomDataset(val_data_path, transform=transform)# 创建数据加载器batch_size = 32train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)val_loader = DataLoader(val_dataset, batch_size=batch_size)# 创建模型实例num_classes = len(train_dataset.dataset.classes)model = CNNModel(num_classes)# 计算参数数量num_params = sum(p.numel() for p in model.parameters())print(&quot;模型参数数量：&quot;, num_params)# 定义损失函数和优化器criterion = nn.CrossEntropyLoss()optimizer = optim.Adam(model.parameters(), lr=0.001)# 设置训练参数num_epochs = 2000check_point = 10 #m每10个epoch验证一回device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)model.to(device)# 开始训练for epoch in range(num_epochs): model.train() running_loss = 0.0 correct_predictions = 0 for images, labels in train_loader: images = images.to(device) labels = labels.to(device) optimizer.zero_grad() outputs = model(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() _, predicted = torch.max(outputs.data, 1) correct_predictions += (predicted == labels).sum().item() running_loss += loss.item() epoch_accuracy = correct_predictions / len(train_dataset) epoch_loss = running_loss / len(train_loader) print(f&quot;Epoch [&#123;epoch+1&#125;/&#123;num_epochs&#125;], Loss: &#123;epoch_loss:.4f&#125;, Accuracy: &#123;epoch_accuracy:.4f&#125;&quot;) if epoch%check_point == 0 : # 在验证集上进行评估 model.eval() total_correct = 0 total_samples = 0 with torch.no_grad(): for images, labels in val_loader: images = images.to(device) labels = labels.to(device) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total_samples += labels.size(0) total_correct += (predicted == labels).sum().item() val_accuracy = total_correct / total_samples print(f&quot;Validation Accuracy: &#123;val_accuracy:.4f&#125;&quot;)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-分类","slug":"Code-分类","permalink":"http://example.com/tags/Code-%E5%88%86%E7%B1%BB/"}]},{"title":"Coding-条件Unet(基于注意力)","slug":"coding_c_Unet","date":"2024-01-02T07:34:19.530Z","updated":"2024-01-23T03:17:36.698Z","comments":true,"path":"2024/01/02/coding_c_Unet/","link":"","permalink":"http://example.com/2024/01/02/coding_c_Unet/","excerpt":"","text":"摘要 unet.py test.py 摘要 可将时间条件和类别条件引入模型，共两个文件：unet.py,test.py,模型为 b=Unet(t,c,a)b=Unet(t,c,a) b=Unet(t,c,a) 其中bbb是输出，ttt是时间条件，ccc是类别条件，aaa是输入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386import mathimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.nn.modules.normalization import GroupNormdef get_norm(norm, num_channels, num_groups): if norm == &quot;in&quot;: return nn.InstanceNorm2d(num_channels, affine=True) elif norm == &quot;bn&quot;: return nn.BatchNorm2d(num_channels) elif norm == &quot;gn&quot;: return nn.GroupNorm(num_groups, num_channels) elif norm is None: return nn.Identity() else: raise ValueError(&quot;unknown normalization type&quot;)class PositionalEmbedding(nn.Module): __doc__ = r&quot;&quot;&quot;Computes a positional embedding of timesteps. Input: x: tensor of shape (N) Output: tensor of shape (N, dim) Args: dim (int): embedding dimension scale (float): linear scale to be applied to timesteps. Default: 1.0 &quot;&quot;&quot; def __init__(self, dim, scale=1.0): super().__init__() assert dim % 2 == 0 self.dim = dim self.scale = scale def forward(self, x): device = x.device half_dim = self.dim // 2 emb = math.log(10000) / half_dim emb = torch.exp(torch.arange(half_dim, device=device) * -emb) emb = torch.outer(x * self.scale, emb) emb = torch.cat((emb.sin(), emb.cos()), dim=-1) return embclass Downsample(nn.Module): __doc__ = r&quot;&quot;&quot;Downsamples a given tensor by a factor of 2. Uses strided convolution. Assumes even height and width. Input: x: tensor of shape (N, in_channels, H, W) time_emb: ignored y: ignored Output: tensor of shape (N, in_channels, H // 2, W // 2) Args: in_channels (int): number of input channels &quot;&quot;&quot; def __init__(self, in_channels): super().__init__() self.downsample = nn.Conv2d(in_channels, in_channels, 3, stride=2, padding=1) def forward(self, x, time_emb, y): if x.shape[2] % 2 == 1: raise ValueError(&quot;downsampling tensor height should be even&quot;) if x.shape[3] % 2 == 1: raise ValueError(&quot;downsampling tensor width should be even&quot;) return self.downsample(x)class Upsample(nn.Module): __doc__ = r&quot;&quot;&quot;Upsamples a given tensor by a factor of 2. Uses resize convolution to avoid checkerboard artifacts. Input: x: tensor of shape (N, in_channels, H, W) time_emb: ignored y: ignored Output: tensor of shape (N, in_channels, H * 2, W * 2) Args: in_channels (int): number of input channels &quot;&quot;&quot; def __init__(self, in_channels): super().__init__() self.upsample = nn.Sequential( nn.Upsample(scale_factor=2, mode=&quot;nearest&quot;), nn.Conv2d(in_channels, in_channels, 3, padding=1), ) def forward(self, x, time_emb, y): return self.upsample(x)class AttentionBlock(nn.Module): __doc__ = r&quot;&quot;&quot;Applies QKV self-attention with a residual connection. Input: x: tensor of shape (N, in_channels, H, W) norm (string or None): which normalization to use (instance, group, batch, or none). Default: &quot;gn&quot; num_groups (int): number of groups used in group normalization. Default: 32 Output: tensor of shape (N, in_channels, H, W) Args: in_channels (int): number of input channels &quot;&quot;&quot; def __init__(self, in_channels, norm=&quot;gn&quot;, num_groups=32): super().__init__() self.in_channels = in_channels self.norm = get_norm(norm, in_channels, num_groups) self.to_qkv = nn.Conv2d(in_channels, in_channels * 3, 1) self.to_out = nn.Conv2d(in_channels, in_channels, 1) def forward(self, x): b, c, h, w = x.shape q, k, v = torch.split(self.to_qkv(self.norm(x)), self.in_channels, dim=1) q = q.permute(0, 2, 3, 1).view(b, h * w, c) k = k.view(b, c, h * w) v = v.permute(0, 2, 3, 1).view(b, h * w, c) dot_products = torch.bmm(q, k) * (c ** (-0.5)) assert dot_products.shape == (b, h * w, h * w) attention = torch.softmax(dot_products, dim=-1) out = torch.bmm(attention, v) assert out.shape == (b, h * w, c) out = out.view(b, h, w, c).permute(0, 3, 1, 2) return self.to_out(out) + xclass ResidualBlock(nn.Module): __doc__ = r&quot;&quot;&quot;Applies two conv blocks with resudual connection. Adds time and class conditioning by adding bias after first convolution. Input: x: tensor of shape (N, in_channels, H, W) time_emb: time embedding tensor of shape (N, time_emb_dim) or None if the block doesn&#x27;t use time conditioning y: classes tensor of shape (N) or None if the block doesn&#x27;t use class conditioning Output: tensor of shape (N, out_channels, H, W) Args: in_channels (int): number of input channels out_channels (int): number of output channels time_emb_dim (int or None): time embedding dimension or None if the block doesn&#x27;t use time conditioning. Default: None num_classes (int or None): number of classes or None if the block doesn&#x27;t use class conditioning. Default: None activation (function): activation function. Default: torch.nn.functional.relu norm (string or None): which normalization to use (instance, group, batch, or none). Default: &quot;gn&quot; num_groups (int): number of groups used in group normalization. Default: 32 use_attention (bool): if True applies AttentionBlock to the output. Default: False &quot;&quot;&quot; def __init__( self, in_channels, out_channels, dropout, time_emb_dim=None, num_classes=None, activation=F.relu, norm=&quot;gn&quot;, num_groups=32, use_attention=False, ): super().__init__() self.activation = activation self.norm_1 = get_norm(norm, in_channels, num_groups) self.conv_1 = nn.Conv2d(in_channels, out_channels, 3, padding=1) self.norm_2 = get_norm(norm, out_channels, num_groups) self.conv_2 = nn.Sequential( nn.Dropout(p=dropout), nn.Conv2d(out_channels, out_channels, 3, padding=1), ) self.time_bias = nn.Linear(time_emb_dim, out_channels) if time_emb_dim is not None else None self.class_bias = nn.Embedding(num_classes, out_channels) if num_classes is not None else None self.residual_connection = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity() self.attention = nn.Identity() if not use_attention else AttentionBlock(out_channels, norm, num_groups) def forward(self, x, time_emb=None, y=None): out = self.activation(self.norm_1(x)) out = self.conv_1(out) if self.time_bias is not None: if time_emb is None: raise ValueError(&quot;time conditioning was specified but time_emb is not passed&quot;) out += self.time_bias(self.activation(time_emb))[:, :, None, None] if self.class_bias is not None: if y is None: raise ValueError(&quot;class conditioning was specified but y is not passed&quot;) out += self.class_bias(y)[:, :, None, None] out = self.activation(self.norm_2(out)) out = self.conv_2(out) + self.residual_connection(x) out = self.attention(out) return outclass UNet(nn.Module): __doc__ = &quot;&quot;&quot;UNet model used to estimate noise. Input: x: tensor of shape (N, in_channels, H, W) time_emb: time embedding tensor of shape (N, time_emb_dim) or None if the block doesn&#x27;t use time conditioning y: classes tensor of shape (N) or None if the block doesn&#x27;t use class conditioning Output: tensor of shape (N, out_channels, H, W) Args: img_channels (int): number of image channels base_channels (int): number of base channels (after first convolution) channel_mults (tuple): tuple of channel multiplers. Default: (1, 2, 4, 8) time_emb_dim (int or None): time embedding dimension or None if the block doesn&#x27;t use time conditioning. Default: None time_emb_scale (float): linear scale to be applied to timesteps. Default: 1.0 num_classes (int or None): number of classes or None if the block doesn&#x27;t use class conditioning. Default: None activation (function): activation function. Default: torch.nn.functional.relu dropout (float): dropout rate at the end of each residual block attention_resolutions (tuple): list of relative resolutions at which to apply attention. Default: () norm (string or None): which normalization to use (instance, group, batch, or none). Default: &quot;gn&quot; num_groups (int): number of groups used in group normalization. Default: 32 initial_pad (int): initial padding applied to image. Should be used if height or width is not a power of 2. Default: 0 &quot;&quot;&quot; def __init__( self, img_channels, base_channels, channel_mults=(1, 2, 4, 8), num_res_blocks=2, time_emb_dim=None, time_emb_scale=1.0, num_classes=None, activation=F.relu, dropout=0.1, attention_resolutions=(), norm=&quot;gn&quot;, num_groups=32, initial_pad=0, ): super().__init__() self.activation = activation self.initial_pad = initial_pad self.num_classes = num_classes self.time_mlp = nn.Sequential( PositionalEmbedding(base_channels, time_emb_scale), nn.Linear(base_channels, time_emb_dim), nn.SiLU(), nn.Linear(time_emb_dim, time_emb_dim), ) if time_emb_dim is not None else None self.init_conv = nn.Conv2d(img_channels, base_channels, 3, padding=1) self.downs = nn.ModuleList() self.ups = nn.ModuleList() channels = [base_channels] now_channels = base_channels for i, mult in enumerate(channel_mults): out_channels = base_channels * mult for _ in range(num_res_blocks): self.downs.append(ResidualBlock( now_channels, out_channels, dropout, time_emb_dim=time_emb_dim, num_classes=num_classes, activation=activation, norm=norm, num_groups=num_groups, use_attention=i in attention_resolutions, )) now_channels = out_channels channels.append(now_channels) if i != len(channel_mults) - 1: self.downs.append(Downsample(now_channels)) channels.append(now_channels) self.mid = nn.ModuleList([ ResidualBlock( now_channels, now_channels, dropout, time_emb_dim=time_emb_dim, num_classes=num_classes, activation=activation, norm=norm, num_groups=num_groups, use_attention=True, ), ResidualBlock( now_channels, now_channels, dropout, time_emb_dim=time_emb_dim, num_classes=num_classes, activation=activation, norm=norm, num_groups=num_groups, use_attention=False, ), ]) for i, mult in reversed(list(enumerate(channel_mults))): out_channels = base_channels * mult for _ in range(num_res_blocks + 1): self.ups.append(ResidualBlock( channels.pop() + now_channels, out_channels, dropout, time_emb_dim=time_emb_dim, num_classes=num_classes, activation=activation, norm=norm, num_groups=num_groups, use_attention=i in attention_resolutions, )) now_channels = out_channels if i != 0: self.ups.append(Upsample(now_channels)) assert len(channels) == 0 self.out_norm = get_norm(norm, base_channels, num_groups) self.out_conv = nn.Conv2d(base_channels, img_channels, 3, padding=1) def forward(self, x, time=None, y=None): ip = self.initial_pad if ip != 0: x = F.pad(x, (ip,) * 4) if self.time_mlp is not None: if time is None: raise ValueError(&quot;time conditioning was specified but tim is not passed&quot;) time_emb = self.time_mlp(time) else: time_emb = None if self.num_classes is not None and y is None: raise ValueError(&quot;class conditioning was specified but y is not passed&quot;) x = self.init_conv(x) skips = [x] for layer in self.downs: x = layer(x, time_emb, y) skips.append(x) for layer in self.mid: x = layer(x, time_emb, y) for layer in self.ups: if isinstance(layer, ResidualBlock): x = torch.cat([x, skips.pop()], dim=1) x = layer(x, time_emb, y) x = self.activation(self.out_norm(x)) x = self.out_conv(x) if self.initial_pad != 0: return x[:, :, ip:-ip, ip:-ip] else: return x 123456789101112131415161718192021222324252627282930313233from unet import *import torch.nn.functional as Fif __name__==&quot;__main__&quot;: activations = &#123; &quot;relu&quot;: F.relu, &quot;mish&quot;: F.mish, &quot;silu&quot;: F.silu, &#125; model = UNet( img_channels=3, base_channels=64, channel_mults=(1, 2, 2, 2), time_emb_dim=512, norm=&#x27;gn&#x27;, dropout=0.1, activation=activations[&#x27;silu&#x27;], attention_resolutions=(1,), num_classes=10, initial_pad=0, ) a = torch.FloatTensor(1,3,32,32) t = torch.randn(1).long() y = torch.randn(1).long() b = model(a,t,y) pass","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-模型","slug":"Code-模型","permalink":"http://example.com/tags/Code-%E6%A8%A1%E5%9E%8B/"}]},{"title":"Coding-3D医学图像分类-基于3D-Resnet","slug":"coding_medical_network_example","date":"2023-12-26T08:18:48.892Z","updated":"2024-01-23T03:54:18.379Z","comments":true,"path":"2023/12/26/coding_medical_network_example/","link":"","permalink":"http://example.com/2023/12/26/coding_medical_network_example/","excerpt":"","text":"摘要 main.py 摘要 训练一个3D医学图像分类神经网络（3D-Resnet），包括： 1.自定义dataload制作 2.网络定义(3D-Resnet) 3.训练过程 4.测试过程 5.模型评估（准确率） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225import torchimport torch.nn as nnimport numpy as npfrom torch.utils.data import Datasetimport osclass mydatasets(Dataset): def __init__(self, data,label): self.data = data # 加上通道数 self.label = label def __getitem__(self, index): data = self.data[index] # 获取高阶FCN label = self.label[index] return data,label def __len__(self): return self.data.shape[0] # 返回数据集的长度class BasicBlock(nn.Module): expansion = 1 def __init__(self, in_channels, out_channels, stride=1): super(BasicBlock, self).__init__() # 第一个卷积层 self.conv1 = nn.Conv3d( in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False ) self.bn1 = nn.BatchNorm3d(out_channels) self.relu = nn.ReLU(inplace=True) # 第二个卷积层 self.conv2 = nn.Conv3d( out_channels, out_channels * self.expansion, kernel_size=3, stride=1, padding=1, bias=False, ) self.bn2 = nn.BatchNorm3d(out_channels * self.expansion) # 残差连接（shortcut connection） self.shortcut = nn.Sequential() if stride != 1 or in_channels != out_channels * self.expansion: self.shortcut = nn.Sequential( nn.Conv3d( in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False, ), nn.BatchNorm3d(out_channels * self.expansion), ) def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out += self.shortcut(residual) out = self.relu(out) return outclass ResNet(nn.Module): def __init__(self, block, num_blocks, num_classes=10): super(ResNet, self).__init__() self.in_channels = 64 # 第一个卷积层 self.conv1 = nn.Conv3d( 1, 64, kernel_size=3, stride=1, padding=1, bias=False ) self.bn1 = nn.BatchNorm3d(64) self.relu = nn.ReLU(inplace=True) # ResNet的四个阶段 self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1) self.layer2 = self.make_layer(block, 128, num_blocks[1], stride=2) self.layer3 = self.make_layer(block, 256, num_blocks[2], stride=2) self.layer4 = self.make_layer(block, 512, num_blocks[3], stride=2) # 全局平均池化层和全连接层 self.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1)) self.fc = nn.Linear(512 * block.expansion, num_classes) def make_layer(self, block, out_channels, num_blocks, stride): layers = [] layers.append(block(self.in_channels, out_channels, stride)) self.in_channels = out_channels * block.expansion for _ in range(1, num_blocks): layers.append(block(self.in_channels, out_channels)) return nn.Sequential(*layers) def forward(self, x): out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.layer1(out) out = self.layer2(out) out = self.layer3(out) out = self.layer4(out) out = self.avg_pool(out) out = torch.flatten(out, 1) out = self.fc(out) return outdef ResNet18(num_classes=10): return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)def data_load(data_path,batch_size): data_label = np.load(data_path,batch_size) torch.manual_seed(9) random_index = np.random.permutation(data_label[&#x27;data&#x27;].shape[0]) data = torch.from_numpy(data_label[&#x27;data&#x27;][random_index]).float() label= torch.from_numpy(data_label[&#x27;label&#x27;][random_index]).float() train_data = data[:160] train_label = label[:160] test_data = data[160:] test_label = label[160:] train_dataset = mydatasets(train_data,train_label) test_dataset = mydatasets(test_data,test_label) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True) # 创建训练数据加载器 test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False) # 创建测试数据加载器 return train_loader,test_loaderdef train(model, train_loader, criterion, optimizer, device): model.train() # 设置模型为训练模式 train_loss = 0 for data, label in train_loader: data = data.to(device) data = torch.unsqueeze(data,1) optimizer.zero_grad() # 清除梯度 output = model(data) # 前向传播 loss = criterion(output, label.to(device).long()) # 计算损失 loss.backward() # 反向传播，计算梯度 optimizer.step() # 更新模型参数 train_loss += loss.item() * data.size(0) train_loss /= len(train_loader.dataset) # 计算平均训练损失 return train_lossdef validate(model, val_loader, criterion, device): model.eval() # 设置模型为评估模式 val_loss = 0 correct = 0 #正确个数 total = 0 #总数 with torch.no_grad(): for data, label in val_loader: data = data.to(device) data = torch.unsqueeze(data,1) output = model(data) # 前向传播 _, predicted = torch.max(output.data, 1) total += label.size(0) correct += (predicted == label.to(device)).sum().item() loss = criterion(output, label.to(device).long()) # 计算损失 val_loss += loss.item() * data.size(0) accuracy = 100 * correct / total # print(&#x27;Accuracy on the test set: %d %%&#x27; % accuracy) val_loss /= len(val_loader.dataset) # 计算平均验证损失 return accuracy,val_loss if __name__ == &quot;__main__&quot;: epoch_times = 100 batch_size = 4 device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;) train_loader,test_loader = data_load(&#x27;/home/yeshixin/work/newwork/DDPM-main/data/mri_ad90_cn113_data_label_normal.npz&#x27;,batch_size) model = ResNet18(2) model.to(device) # criterion = nn.MSELoss() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(),lr=0.001) train_losses = [] val_losses = [] best_val_loss = np.inf best_val_acc = 0 if not os.path.exists(&#x27;ckpt&#x27;): os.mkdir(&#x27;./ckpt&#x27;) # 训练模型 for epoch in range(epoch_times): train_loss = train(model, train_loader, criterion, optimizer, device) # 训练模型 val_acc,val_loss = validate(model, test_loader, criterion, device) # 验证模型 train_losses.append(train_loss) # 保存训练损失 val_losses.append(val_loss) # 保存验证损失 # 存储最小损失模型 if val_loss &lt; best_val_loss: best_val_loss = val_loss best_model = model.state_dict() torch.save(best_model, &#x27;ckpt/BestLoss_&#x27;+str(best_val_loss)+&#x27;_model.ckpt&#x27;) # 保存最佳模型参数 print(&quot;best_val_loss: &quot; + str(best_val_loss)) with open(&quot;ckpt/model_loss.txt&quot;, &quot;w&quot;) as f: f.write(str(val_loss)) # 存储最大准确率模型 if val_acc &gt; best_val_acc: best_val_acc = val_acc best_model = model.state_dict() torch.save(best_model, &#x27;ckpt/BestAcc_&#x27;+str(best_val_acc)+&#x27;_model.ckpt&#x27;) # 保存最佳模型参数 print(&quot;best_val_acc: &quot; + str(best_val_acc)) with open(&quot;ckpt/model_acc.txt&quot;, &quot;w&quot;) as f: f.write(str(best_val_acc)) print(&#x27;Epoch [&#123;&#125;/&#123;&#125;], Train Loss: &#123;:.4f&#125;, Val Loss: &#123;:.4f&#125;, Val Acc: &#123;:.4f&#125; %&#x27;.format(epoch+1, epoch_times, train_loss, val_loss,val_acc))","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-分类","slug":"Code-分类","permalink":"http://example.com/tags/Code-%E5%88%86%E7%B1%BB/"}]},{"title":"Coding-条件生成对抗网络(cGAN)-基于高阶FCN","slug":"coding_GAN_High_Fcn","date":"2023-12-21T16:08:30.580Z","updated":"2024-01-23T03:28:00.332Z","comments":true,"path":"2023/12/22/coding_GAN_High_Fcn/","link":"","permalink":"http://example.com/2023/12/22/coding_GAN_High_Fcn/","excerpt":"","text":"摘要 main.py generate.py 摘要 main.py包括高阶FCN的处理，生成对抗网络的训练 generate.py使用生成器生成样本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299import argparseimport osimport numpy as npimport mathimport torchvision.transforms as transformsfrom torchvision.utils import save_imagefrom torch.utils.data import DataLoaderfrom torchvision import datasetsfrom torch.autograd import Variableimport torch.nn as nnimport torch.nn.functional as Fimport torchfrom scipy.io import loadmatfrom torch.utils import data # 导入PyTorch数据工具模块import randomfrom scipy.stats import pearsonros.makedirs(&quot;train_images&quot;, exist_ok=True)os.makedirs(&quot;test_images&quot;, exist_ok=True)os.makedirs(&quot;save_model&quot;, exist_ok=True)parser = argparse.ArgumentParser()parser.add_argument(&quot;--n_epochs&quot;, type=int, default=200, help=&quot;number of epochs of training&quot;)parser.add_argument(&quot;--batch_size&quot;, type=int, default=4, help=&quot;size of the batches&quot;)parser.add_argument(&quot;--lr&quot;, type=float, default=0.0002, help=&quot;adam: learning rate&quot;)parser.add_argument(&quot;--b1&quot;, type=float, default=0.5, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--b2&quot;, type=float, default=0.999, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--n_cpu&quot;, type=int, default=8, help=&quot;number of cpu threads to use during batch generation&quot;)parser.add_argument(&quot;--latent_dim&quot;, type=int, default=100, help=&quot;噪声的维度&quot;)parser.add_argument(&quot;--n_classes&quot;, type=int, default=2, help=&quot;类别数量&quot;)parser.add_argument(&quot;--img_size&quot;, type=int, default=116, help=&quot;功能连接矩阵的维度&quot;)parser.add_argument(&quot;--channels&quot;, type=int, default=1, help=&quot;矩阵通道&quot;)parser.add_argument(&quot;--sample_interval&quot;, type=int, default=400, help=&quot;interval between image sampling&quot;)opt = parser.parse_args()print(opt)img_shape = (opt.channels, opt.img_size, opt.img_size)cuda = True if torch.cuda.is_available() else Falseclass my_dataset(data.Dataset): def __init__(self, Hig_X,label_): self.Hig_X = np.expand_dims(Hig_X,1) # 加上通道数 self.label = label_ def __getitem__(self, index): X = self.Hig_X[index] # 获取高阶FCN label = self.label[index] return X,label def __len__(self): return self.Hig_X.shape[0] # 返回数据集的长度def calculate_similarity(matrix1, matrix2): correlation, _ = pearsonr(matrix1.flatten(), matrix2.flatten()) return correlation# 精度判断，即计算两个矩阵的相关系数def calculate_accuracy(generator, dataloader, device, val_subjects=None): generator.eval() correct_predictions = 0 for batch_idx, (batch_matrix) in enumerate(dataloader): matrix = batch_matrix.to(device) size = matrix.size(0) random_noise = torch.randn(size, 100).to(device=device) with torch.no_grad(): gen_matrix = generator(random_noise) for i in range(size): similarity = calculate_similarity(gen_matrix[i].cpu().numpy(), matrix[i].cpu().numpy()) # 阈值是一个经验值，根据实际情况调整 if similarity &gt; 0.6 and (val_subjects is None or batch_idx in val_subjects): correct_predictions += 1 accuracy = correct_predictions / len(dataloader.dataset) return accuracyclass Generator(nn.Module): def __init__(self): super(Generator, self).__init__() self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes) def block(in_feat, out_feat, normalize=True): layers = [nn.Linear(in_feat, out_feat)] if normalize: layers.append(nn.BatchNorm1d(out_feat, 0.8)) layers.append(nn.LeakyReLU(0.2, inplace=True)) return layers self.model = nn.Sequential( *block(opt.latent_dim + opt.n_classes, 128, normalize=False), *block(128, 256), *block(256, 512), *block(512, 1024), nn.Linear(1024, int(np.prod(img_shape))), nn.Tanh() ) def forward(self, noise, labels): # Concatenate label embedding and image to produce input gen_input = torch.cat((self.label_emb(labels), noise), -1) img = self.model(gen_input) img = img.view(img.size(0), *img_shape) return imgclass Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes) self.model = nn.Sequential( nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 512), nn.Dropout(0.4), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 512), nn.Dropout(0.4), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 1), ) def forward(self, img, labels): # Concatenate label embedding and image to produce input d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1) validity = self.model(d_in) return validity# Loss functionsadversarial_loss = torch.nn.MSELoss()# Initialize generator and discriminatorgenerator = Generator()discriminator = Discriminator()if cuda: generator.cuda() discriminator.cuda() adversarial_loss.cuda()# Configure data loader # 原始文件路径fMRI_file_path = &#x27;.//ROISignals_insomnia_aal116.mat&#x27;# 加载数据fMRI_data = loadmat(fMRI_file_path)[&#x27;ROISignals&#x27;]# 正常人为1,病人为0fMRI_label = torch.cat((torch.ones(32,1),torch.zeros(30,1)),dim=0).squeeze()# 低阶矩阵计算Low_X_ = []for i in range(fMRI_data.shape[2]): temp = np.corrcoef(fMRI_data[:,:,i],rowvar=False) Low_X_.append(temp)Low_X = np.array(Low_X_) #(62,116,116)# 高阶矩阵计算Hig_X_ = []for i in range(Low_X.shape[0]): temp = np.corrcoef(Low_X[:,:,i],rowvar=False) Hig_X_.append(temp)Hig_X = np.array(Hig_X_) #(62,116,116)random_index = np.random.permutation(len(fMRI_label))Hig_X = Hig_X[random_index]fMRI_label = fMRI_label[random_index]train_data = Hig_X[:50]train_label = fMRI_label[:50]test_data = Hig_X[50:]test_label = fMRI_label[:50]train_dataset = my_dataset(train_data,train_label)test_dataset = my_dataset(test_data,test_label)train_loader = data.DataLoader(train_dataset,batch_size=opt.batch_size,shuffle=True)test_loader = data.DataLoader(test_dataset,batch_size=opt.batch_size,shuffle=False)# Optimizersoptimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensorLongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor# ----------# Training# ----------for epoch in range(opt.n_epochs): for i, (imgs, labels) in enumerate(train_loader): batch_size = imgs.shape[0] # Adversarial ground truths valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False) fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False) # Configure input real_imgs = Variable(imgs.type(FloatTensor)) labels = Variable(labels.type(LongTensor)) # ----------------- # Train Generator # ----------------- optimizer_G.zero_grad() # Sample noise and labels as generator input z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim)))) gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size))) # Generate a batch of images gen_imgs = generator(z, gen_labels) # Loss measures generator&#x27;s ability to fool the discriminator validity = discriminator(gen_imgs, gen_labels) g_loss = adversarial_loss(validity, valid) g_loss.backward() optimizer_G.step() # --------------------- # Train Discriminator # --------------------- optimizer_D.zero_grad() # Loss for real images validity_real = discriminator(real_imgs, labels) d_real_loss = adversarial_loss(validity_real, valid) # Loss for fake images validity_fake = discriminator(gen_imgs.detach(), gen_labels) d_fake_loss = adversarial_loss(validity_fake, fake) # Total discriminator loss d_loss = (d_real_loss + d_fake_loss) / 2 d_loss.backward() optimizer_D.step() print( &quot;[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]&quot; % (epoch, opt.n_epochs, i, len(train_loader), d_loss.item(), g_loss.item()) ) batches_done = epoch * len(train_loader) + i # 存储训练过程的结果 if batches_done % opt.sample_interval == 0: #一个epoch中每隔多少间隔保存一次 gen_imgs_normalized = (gen_imgs - gen_imgs.min()) / (gen_imgs.max() - gen_imgs.min()) save_image(gen_imgs_normalized.data[:25], &quot;train_images/%d.png&quot; % batches_done, nrow=5, normalize=False) # 测试过程 with torch.no_grad(): best_acc = 0 generator.eval() correct_predictions = 0 for imgs, label in test_loader: z = Variable(FloatTensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim)))) gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, imgs.shape[0]))) gen_imgs = generator(z, gen_labels) for i in range(imgs.shape[0]): similarity = calculate_similarity(gen_imgs[i].cpu().numpy(), imgs[i].cpu().numpy()) # 阈值是一个经验值，根据实际情况调整 if similarity &gt; 0.6: correct_predictions += 1 accuracy = correct_predictions / len(test_loader.dataset) # 保存epoch中精度最好的模型 if best_acc &lt; accuracy: best_acc = accuracy #保存模型 torch.save(generator.state_dict(), &quot;save_model/best_model.pth&quot;) print(&#x27;epoch:&#x27;+str(epoch)+&#x27; 测试集acc:&#x27;+str(accuracy)+&quot; best_acc:&quot;+str(best_acc)) gen_imgs_normalized = (gen_imgs - gen_imgs.min()) / (gen_imgs.max() - gen_imgs.min()) save_image(gen_imgs_normalized.data[:25], &quot;test_images/acc_&#123;&#125;epoch_&#123;&#125;.png&quot;.format(accuracy,epoch), nrow=5, normalize=False) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import argparseimport torch.nn as nnimport torch.nn.functional as Fimport torchimport numpy as npfrom torch.autograd import Variablefrom torchvision.utils import save_imageimport osparser = argparse.ArgumentParser()parser.add_argument(&quot;--n_epochs&quot;, type=int, default=200, help=&quot;number of epochs of training&quot;)parser.add_argument(&quot;--batch_size&quot;, type=int, default=4, help=&quot;size of the batches&quot;)parser.add_argument(&quot;--lr&quot;, type=float, default=0.0002, help=&quot;adam: learning rate&quot;)parser.add_argument(&quot;--b1&quot;, type=float, default=0.5, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--b2&quot;, type=float, default=0.999, help=&quot;adam: decay of first order momentum of gradient&quot;)parser.add_argument(&quot;--n_cpu&quot;, type=int, default=8, help=&quot;number of cpu threads to use during batch generation&quot;)parser.add_argument(&quot;--latent_dim&quot;, type=int, default=100, help=&quot;噪声的维度&quot;)parser.add_argument(&quot;--n_classes&quot;, type=int, default=2, help=&quot;类别数量&quot;)parser.add_argument(&quot;--img_size&quot;, type=int, default=116, help=&quot;功能连接矩阵的维度&quot;)parser.add_argument(&quot;--channels&quot;, type=int, default=1, help=&quot;矩阵通道&quot;)parser.add_argument(&quot;--sample_interval&quot;, type=int, default=400, help=&quot;interval between image sampling&quot;)opt = parser.parse_args()print(opt)img_shape = (opt.channels, opt.img_size, opt.img_size)cuda = True if torch.cuda.is_available() else Falseclass Generator(nn.Module): def __init__(self): super(Generator, self).__init__() self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes) def block(in_feat, out_feat, normalize=True): layers = [nn.Linear(in_feat, out_feat)] if normalize: layers.append(nn.BatchNorm1d(out_feat, 0.8)) layers.append(nn.LeakyReLU(0.2, inplace=True)) return layers self.model = nn.Sequential( *block(opt.latent_dim + opt.n_classes, 128, normalize=False), *block(128, 256), *block(256, 512), *block(512, 1024), nn.Linear(1024, int(np.prod(img_shape))), nn.Tanh() ) def forward(self, noise, labels): # Concatenate label embedding and image to produce input gen_input = torch.cat((self.label_emb(labels), noise), -1) img = self.model(gen_input) img = img.view(img.size(0), *img_shape) return img FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensorLongTensor = torch.cuda.LongTensor if cuda else torch.LongTensoros.makedirs(&quot;Generated_samples&quot;, exist_ok=True)generator = Generator()generator.cuda()# 指定保存的模型文件路径model_path = &#x27;save_model\\\\best_model.pth&#x27;# 加载保存的模型状态字典generator.load_state_dict(torch.load(model_path))# 将模型设置为评估模式generator.eval()# 设置生成样本数batch_size = 25z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size)))gen_imgs = generator(z , gen_labels)# 标准化gen_imgs_normalized = (gen_imgs - gen_imgs.min()) / (gen_imgs.max() - gen_imgs.min()) # 存储样本和标签np.savez(&#x27;Generated_samples/Generated_samples.npz&#x27;, array1=gen_imgs.detach().cpu(), array2=gen_labels.detach().cpu())#存储图片save_image(gen_imgs_normalized.data[:25], &quot;Generated_samples/acc_epoch_.png&quot;, nrow=25, normalize=False)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-生成","slug":"Code-生成","permalink":"http://example.com/tags/Code-%E7%94%9F%E6%88%90/"}]},{"title":"PaperReading-基于对抗训练和不确定性校正的一次性创伤脑分割","slug":"paper_基于对抗训练和不确定性校正的一次性创伤脑分割","date":"2023-10-21T08:33:48.787Z","updated":"2024-01-20T03:01:41.982Z","comments":true,"path":"2023/10/21/paper_基于对抗训练和不确定性校正的一次性创伤脑分割/","link":"","permalink":"http://example.com/2023/10/21/paper_%E5%9F%BA%E4%BA%8E%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83%E5%92%8C%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E6%A0%A1%E6%AD%A3%E7%9A%84%E4%B8%80%E6%AC%A1%E6%80%A7%E5%88%9B%E4%BC%A4%E8%84%91%E5%88%86%E5%89%B2/","excerpt":"","text":"One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification 1.问题总结 基于学习变换的单次分割方法存在以下问题:(1)增强样本的多样性受限。(2)学习变换引入了潜在的错误标签 2.解决方法 (1)通过增强样本的对抗性分布来提高增强数据的多样性和分割的鲁棒性。(2)根据分割过程中的不确定性，对学习变换带来的潜在标签误差进行校正。 3.总结 在这项工作中，我们提出了一种新的一次性分割方法，用于严重创伤性脑分割，这是一种困难的临床场景，可用的注释数据有限。我们的方法解决了sTBI脑分割中的关键问题，即需要多样化的训练数据和减少由外观变换引入的潜在标签错误。对抗训练的引入增强了数据的多样性和分割的鲁棒性，同时设计了不确定性校正来补偿潜在的标签错误。 在sTBI脑上的实验结果证明了我们提出的方法的有效性及其相对于最先进的替代方法的优势，突出了我们的方法在实现更准确的严重创伤脑分割方面的潜力，这可能有助于临床管道。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"MICCAI","slug":"MICCAI","permalink":"http://example.com/tags/MICCAI/"}]},{"title":"PaperReading-使用学习变换的数据增强，用于一次性医学图像分割","slug":"paper_使用学习变换的数据增强_用于一次性医学图像分割","date":"2023-10-21T02:43:59.360Z","updated":"2024-01-20T03:01:26.546Z","comments":true,"path":"2023/10/21/paper_使用学习变换的数据增强_用于一次性医学图像分割/","link":"","permalink":"http://example.com/2023/10/21/paper_%E4%BD%BF%E7%94%A8%E5%AD%A6%E4%B9%A0%E5%8F%98%E6%8D%A2%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA_%E7%94%A8%E4%BA%8E%E4%B8%80%E6%AC%A1%E6%80%A7%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/","excerpt":"","text":"Data augmentation using learned transformations for one-shot medical image segmentation 1.问题总结 图像分割是医学应用中的重要任务。基于卷积网络的方法已经实现了非常好的效果。但是它们通常依赖于大量标记的数据集。标记医学图像往往需要大量的专业知识和时间，并且用于数据增强的典型手动调整方法无法捕获此类图像中复杂变化。 2.解决方法 通过学习合成多样的现实的标签实例来解决有限的标签数据问题。我们的数据增强新方法利用了没有标签的图像。使用基于学习的配准方法，对数据集中的图像之间的空间和外观变换集进行建模。这些模型捕获了未标记图像之间的解剖和图像多样性。我们合成新的实例通过采样变换并把他们应用到单个有标签的实例上。 3.总结 提出了一个基于学习的数据增强方法，并且在一次医学图像分割中进行了演示。 我们从一个带标签的图像和一些未带标签的图像开始。使用基于学习的配准方法，我们在带标签和未带标签的图像之间建模空间和外观转换集。这些转换可以捕获诸如非线性变形和图像强度的变换。我们合成新的标签通过采样转换并且并将其应用于标签，产生各种逼真的新图像。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"CVPR","slug":"CVPR","permalink":"http://example.com/tags/CVPR/"}]},{"title":"Cpp-C++基础知识","slug":"cpp_tips","date":"2023-10-04T03:08:12.608Z","updated":"2023-10-28T06:29:06.750Z","comments":true,"path":"2023/10/04/cpp_tips/","link":"","permalink":"http://example.com/2023/10/04/cpp_tips/","excerpt":"","text":"虚析构函数 在 C++ 中，虚析构函数（virtual destructor）是在基类中声明并使用 virtual 关键字修饰的析构函数。 虚析构函数在处理基类指针指向派生类对象时非常有用。当使用基类指针来删除一个指向派生类对象的实例时，如果基类的析构函数不是虚拟的，只会调用基类的析构函数，而不会调用派生类的析构函数。这可能导致派生类对象中的资源无法正确释放，造成内存泄漏。 通过将基类的析构函数声明为虚拟的，可以确保在删除基类指针时正确调用派生类的析构函数，从而正确释放派生类对象的资源。下面是一个使用虚析构函数的示例： 1234567891011121314151617181920212223#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;class base&#123;public: virtual ~base()&#123; cout&lt;&lt;&quot;base&#x27;s ~&quot;; &#125;;&#125;;class child:public base&#123; public: ~child()&#123; cout&lt;&lt;&quot;child&#x27;s~&quot;; &#125;&#125;;int main()&#123; base* p = new child(); delete p; return 0;&#125; for循环遍历迭代器 123456789101112#include&lt;iostream&gt;#include&lt;list&gt;int main()&#123; std::list&lt;int&gt; a; a.push_back(1); a.push_back(2); a.push_back(3); for(auto &amp;i:a )&#123; std::cout&lt;&lt;i&lt;&lt;std::endl; &#125; return 0;&#125; cmake初级 文件夹设置 ├── CMakeLists.txt ├── sylar │ ├── CMakeLists.txt │ ├── main.cpp 第一个CMakeLists.txt 1234567cmake_minimum_required(VERSION 3.12)project(sylar)set(CMAKE_CXX_STANDARD 14)add_subdirectory(sylar)# add_executable(sylar sylar/main.cpp) 第二个CMakeLists.txt 1add_executable(sylar main.cpp) 可变参数列表 1234567891011121314151617181920#include &lt;iostream&gt;#include &lt;cstdarg&gt;// 可变参数列表的函数定义void print(int count, ...) &#123; va_list args; va_start(args, count); for (int i = 0; i &lt; count; ++i) &#123; int value = va_arg(args, int); std::cout &lt;&lt; value &lt;&lt; std::endl; &#125; va_end(args);&#125;int main() &#123; print(6,1,7,9,2,3,4); return 0;&#125; 操作符重载 12345678910111213141516171819#include &lt;iostream&gt;// 自定义类 MyClassclass MyClass &#123;public: int data; MyClass(int value) : data(value) &#123;&#125; friend std::ostream&amp; operator&lt;&lt;(std::ostream&amp; os, const MyClass&amp; obj) &#123; os &lt;&lt; &quot;MyClass: &quot; &lt;&lt; obj.data; return os; &#125;&#125;;int main() &#123; MyClass obj(42); std::cout &lt;&lt; obj &lt;&lt; std::endl; return 0;&#125; 单例模式 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;iostream&gt;class Singleton &#123;private: // 私有的静态成员变量，用于存储单例实例 static Singleton* instance; // 私有的构造函数，防止外部直接实例化对象 Singleton() &#123;&#125;public: // 静态成员函数，用于获取单例对象的唯一实例 static Singleton* getInstance() &#123; if (instance == nullptr) &#123; instance = new Singleton(); &#125; return instance; &#125; // 示例成员函数 void showMessage() &#123; std::cout &lt;&lt; &quot;Hello from Singleton!&quot; &lt;&lt; std::endl; &#125;&#125;;// 初始化静态成员变量Singleton* Singleton::instance = nullptr;int main() &#123; Singleton* instance1 = Singleton::getInstance(); Singleton* instance2 = Singleton::getInstance(); // instance1和instance2指向相同的对象 instance1-&gt;showMessage(); // 输出: Hello from Singleton! instance2-&gt;showMessage(); // 输出: Hello from Singleton! // 清理单例对象 delete instance1; instance1 = nullptr; return 0;&#125; 模板类 1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;boost/lexical_cast.hpp&gt;template &lt;class F, class T&gt;class LexicalCast &#123;public: T operator()(const F&amp; v) &#123; return boost::lexical_cast&lt;T&gt;(v); //将T转换为int &#125;&#125;;int main() &#123; LexicalCast&lt;std::string, int&gt; stringToInt; int num = stringToInt(&quot;12345&quot;); std::cout &lt;&lt; &quot;Converted integer: &quot; &lt;&lt; num &lt;&lt; std::endl; LexicalCast&lt;double, std::string&gt; doubleToString; std::string str = doubleToString(3.14159); std::cout &lt;&lt; &quot;Converted string: &quot; &lt;&lt; str &lt;&lt; std::endl; return 0;&#125; static 关键词 在 C++ 头文件（.h）中定义的静态函数，在对应的实现文件（.cpp）中不需要再使用 static 关键词修饰。 当在头文件中声明静态函数时，使用 static 关键词是为了将函数的作用域限制在当前文件中，避免与其他文件中同名的函数产生冲突。这样做是因为头文件通常会被多个源文件包含，如果在头文件中使用 static 修饰符，每个包含该头文件的源文件都会有一个独立的静态函数副本，可能导致链接错误。 而在实现文件中，已经处于单个文件的作用域中，不需要再次使用 static 关键词来限制函数的作用域。实现文件中的静态函数的定义只需要与头文件中的函数声明相匹配即可。 因此，头文件中的静态函数声明不需要再使用 static 关键词修饰，而在对应的实现文件中，只需要提供静态函数的定义即可。这样可以保持函数在整个程序中的唯一性，并正确地与其他文件中的代码进行链接。 在 C++ 中，# 是一种预处理操作符，称为字符串化操作符（stringizing operator）。它用于将宏参数转换为字符串字面值。","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://example.com/tags/Cpp/"}]},{"title":"everything","slug":"knowledge_daily_study","date":"2023-09-29T08:10:40.338Z","updated":"2024-01-23T04:24:11.993Z","comments":true,"path":"2023/09/29/knowledge_daily_study/","link":"","permalink":"http://example.com/2023/09/29/knowledge_daily_study/","excerpt":"","text":"screen命令 ssh: connect to host github.com port 22: Connection refused nignx正向代理配置 code-server错误解决 一些windows指令 图卷积神经网络基础 真理 酸葡萄心理 好坏制度 screen命令 1234567891011121314# 新建窗口screen -S name# 挂起 [detached] ctrl + a + d # 列出窗口列表screen -ls# 杀死多余窗口kill -9 threadnum# 清除死去的窗口screen -wipe ssh: connect to host port 22: Connection refused 解决方法： 1$ vim ~/.ssh/config 1234# Add section below to itHost github.com Hostname ssh.github.com Port 443 1234$ ssh -T git@github.comHi xxxxx! You&#x27;ve successfully authenticated, but GitHub does notprovide shell access. nignx正向代理配置 123456789101112131415161718192021#正向代理转发http请求 server &#123; #指定DNS服务器IP地址 # resolver 114.114.114.114; #监听80端口，http默认端口80 listen 80; #服务器IP或域名 server_name xxx.xxxxx.top; #正向代理转发http请求 location / &#123; proxy_pass http://xxxxx.top:20003; proxy_set_header HOST $host; proxy_buffers 256 4k; proxy_max_temp_file_size 0k; proxy_connect_timeout 30; proxy_send_timeout 60; proxy_read_timeout 60; proxy_next_upstream error timeout invalid_header http_502; &#125;&#125; code-server错误解决 1234567891011121314151617181920212223#正向代理转发http请求 code2server &#123; #指定DNS服务器IP地址 # resolver 114.114.114.114; #监听80端口，http默认端口80 listen 80; #服务器IP或域名 server_name xxx.xxxxx.top; #正向代理转发http请求 location / &#123; proxy_pass http://xxxxx.top:20013; proxy_set_header HOST $host; proxy_buffers 256 4k; proxy_max_temp_file_size 0k; proxy_connect_timeout 30; proxy_send_timeout 60; proxy_read_timeout 60; proxy_next_upstream error timeout invalid_header http_502; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection upgrade; &#125;&#125; 一些windows指令 Windows转发wsl 123netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=7777 connectaddress=localhost connectport=8888netsh interface portproxy show allnetsh interface portproxy delete v4tov4 listenport=7777 listenaddress=0.0.0.0 protocol=tcp 代理设置 12set http_proxy=sock5://192.168.31.100:1080set https_proxy=sock5://192.168.31.100:1080 麻黄 麻黄碱属于肾上腺素受体激动剂，是从植物麻黄草中提取的一种生物碱。有以下药理作用：1、舒张支气管。2、增加心肌收缩力量，加大心脏血液输出量。3、能够兴奋中枢神经系统。在临床上用于慢性低血压、缓解支气管哮喘、对抗荨麻疹、血管神经性水肿。 厚此薄彼，汉语成语，拼音是hòu cǐ bó bǐ，比喻对人、对事不同看待。 真理 不患寡而患不均 酸葡萄心理 否定自己之前想吃葡萄的意愿，把自己的“不得已”包装成一种自由，因为我是主动不吃的，因为葡萄不好吃。酸葡萄心理的本质是客观环境无力改变，那我就改变自己，调整自己来适应环境酸葡萄心理和甜柠檬心理往往是同时出现。两者一组合，往小了说，得过且过凑合用吧，往大了说，那就可以培养出奴性。比如说在古代皇权社会，臣民得不到自由，于是就会嫌弃自由，同时我没办法摆脱专制，那我就转而拥护专制，嗷嗷赞美统治者，人一旦经历这两个心理过程，奴性就养成了。奴才在得不到葡萄的情况下，给自己编造了甜柠檬神话 好坏制度 一个制度的好坏，并不在于它鼓吹或奉行什么主义，而在于现实层面权力结构是否合理。 权力结构合理，有制约有监督，即使有国王皇帝也并不意味着就是君主专制。 但权力结构是垄断的，即使国名是民主主义共和国、宪法中写满民主，即使鼓吹解放全人类，自诩手里拿着一本宇宙真理，那也注定是暴政。 马克思主义认为国家是阶级矛盾不可调和的产物，认为国家是统治阶级用来压迫另一阶级，维护自身统治的工具 他们是恐惧的奴仆，谁掌控他们生死他们就爱谁 掠之于民，民变在即便掠之于商 摆不完的阔气，弄不完的权，吃不完的珍馐，花不完的钱，听不完的颂歌，收不完的了礼，享不尽的荣华富贵，过不完的年 “长君之恶其罪小，逢君之恶其罪大”，只知道感恩戴德，不知道批评监督；只知道义务，不知道权利；只关心私人事务，不关心公共事务，对苦难和社会问题视而不见，对指出问题的人恶语相向，对持不同意见者进行思想霸凌，这些人要么是别有用心，要么是故意装傻，无论是哪一种，都是赢在当下，祸在千秋 权力导致腐败，绝对的权力导致绝对的腐败 他们始终把政治作为达成经济目的的手段 传统文化占支配地位的部分是最腐朽的东西，必须进行严厉的自我批判，剩下的内容才有资格和共产主义拉关系。 了解自己落后的原因，看看别人强大的根源，一味地仇恨，不会让人进步，只会让人疯狂 “长君之恶其罪小，逢君之恶其罪大”，只知道感恩戴德，不知道批评监督；只知道义务，不知道权利；只关心私人事务，不关心公共事务，对苦难和社会问题视而不见，对指出问题的人恶语相向，对持不同意见者进行思想霸凌，这些人要么是别有用心，要么是故意装傻，无论是哪一种，都是赢在当下，祸在千秋 一句没有经过任何证明的、不包含形容词副词修饰的、仅仅带了性别标签的话语，就成了某性别伪装成弱势群体无下限予取予求的遮羞布。 亡国亡天下的区别，改变了皇帝的姓名和国家的称号，叫亡国。而灭绝人性和仁义，把人搞得和禽兽一样互相残害，叫亡天下。 乌合之中只有把渺小的个体投身于一个虚无缥缈的伟大的事业中的时候，他才会觉得自己的人生有价值，才会觉得自己也曾干过大事业，绝对看不清自己从始至终炮灰人生的这种命运本质 极权政治的一大特点，就是所有的政治博弈，都是暗箱操作，权力完全在一堆秘不示人的潜规则下运作，不可预知。拿到台面上来的结果都是不明不白的结果。所有人的安危都寄于一人之念，包括独裁者自己都没有绝对的安全可言，在独夫之下，都是奴才。 在特权政治下的政治权力，不是被用来表达人民的意志，图谋人民的利益，在特权政治下的政治权力不是被运用来表达人民的意志，图谋人民的利益，反而是在“国家的”或“国民的”名义下，被运用来管制人民，奴役人民，以达成权势者自私自利的目的。这种政治形态存在的前提，是人民还被束缚被限制在愚昧无知的状态中 文化存异，文明趋同 但是特权阶级宣称他们的特权是天授，乃天命。后来又学会了更厉害的招数，那就是把自己隐藏在民族，国家这些宏大叙事后面，人民反对他们的特权就是逆天而行，就是不爱国。没有知识的民众也不晓得研究这些话有没有道理，只是盲目附和支持特权阶级，反而去反对那些为他们争取平等和自由权利的人 奇技淫巧终究是小道，家国发展，需注重礼法，人情世故，钻营经济仕途，弘扬酒文化，方是正途。取中庸之道，无为而成大才。茶杯的摆放，酒桌上的礼仪，点菜的取舍，酒品的选择，敬酒的顺序，宣传稿的措辞，领导照相时的站位，领导的用词，领导的潜沟通，领导外部发言、领导内部谈话，领导之间的关系，这些才是艺术。我们还年轻，现在刚进入社会几年还不懂，年纪轻轻不理解这些，未来要学的还很多，路还很长","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"knowledge","slug":"knowledge","permalink":"http://example.com/tags/knowledge/"}]},{"title":"Coding-表格操作","slug":"coding_table_opeartion","date":"2023-09-29T08:01:50.305Z","updated":"2024-01-23T04:02:39.675Z","comments":true,"path":"2023/09/29/coding_table_opeartion/","link":"","permalink":"http://example.com/2023/09/29/coding_table_opeartion/","excerpt":"","text":"读取日期表格数据并显示 读取日期表格数据并显示 12345678910111213141516171819202122import matplotlib.pyplot as pltimport pandas as pddf = pd.read_csv(&#x27;plot.csv&#x27;)# 格式转为日期df[&#x27;date&#x27;] = pd.to_datetime(df[&#x27;date&#x27;])df.set_index(&#x27;date&#x27;, inplace=True)#输入折线图数据plt.plot(df.index,df[&quot;a2c&quot;],label=&#x27;a2c&#x27;,linewidth=1,color=&#x27;c&#x27;,marker=&#x27;&#x27;,markerfacecolor=&#x27;blue&#x27;,markersize=5)plt.xlabel(&quot;date&quot;)#横坐标为物品编号plt.ylabel(&#x27;loss&#x27;)#纵坐标为各类指标plt.title(&quot;&quot;)#折线图的名称#图例说明plt.legend()#显示网格plt.grid()#显示图像plt.show()","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-表格处理","slug":"Code-表格处理","permalink":"http://example.com/tags/Code-%E8%A1%A8%E6%A0%BC%E5%A4%84%E7%90%86/"}]},{"title":"Coding-机器学习分类器","slug":"coding_python__classification_method","date":"2023-09-27T04:30:22.190Z","updated":"2024-01-23T03:55:13.457Z","comments":true,"path":"2023/09/27/coding_python__classification_method/","link":"","permalink":"http://example.com/2023/09/27/coding_python__classification_method/","excerpt":"","text":"T检验 SVM 朴素贝叶斯 K近邻 T检验 对标签为1和-1的样本进行T检验 12345678910111213141516from scipy.stats import ttest_indfrom statsmodels.stats.multitest import fdrcorrectiondef t_test(data, label): # 将正负样本分开 pos_data = data[label == 1] neg_data = data[label == -1] # 对正负样本做独立样本t检验 t_values, p_values = ttest_ind(pos_data, neg_data, axis=0) # 使用FDR控制方法去除不显著的特征 reject, p_values_corrected = fdrcorrection(p_values, alpha=0.1) significant_features = np.where(reject)[0] # 返回显著特征的索引 return data[:,significant_features] SVM SVM对莺尾花数据集分类，可以利用此代码进行数据集的初步分类可行性验证 1234567891011121314151617181920212223242526272829303132333435363738import numpy as npfrom sklearn.datasets import load_irisfrom sklearn.model_selection import GridSearchCV, KFold,train_test_splitfrom sklearn.svm import SVCiris = load_iris()data = iris.datalabels = iris.target# 划分训练集和测试集train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=1)print(train_data.shape)print(test_data.shape)# 定义参数范围x = np.logspace(-4, 4, num=9, base=2)param_grid = &#123; &quot;C&quot;: x,&#125;# 定义模型和交叉验证model = SVC(kernel=&quot;linear&quot;)cv = KFold(n_splits=10, shuffle=True, random_state=1)# 定义网格参数搜索法grid_search = GridSearchCV(model, param_grid, cv=cv, scoring=&quot;accuracy&quot;)# 进行交叉验证grid_search.fit(train_data, train_labels)# 输出最佳参数和交叉验证得分print(&quot;Best parameters: &quot;, grid_search.best_params_)print(&quot;Cross-validation score: &quot;, grid_search.best_score_)# 在测试集上评估模型test_score = grid_search.score(test_data, test_labels)print(&quot;Test set score: &quot;, test_score) 朴素贝叶斯 朴素贝叶斯对莺尾花数据集分类 12345678910111213141516171819202122from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.naive_bayes import GaussianNBfrom sklearn.metrics import accuracy_score# 加载莺尾花数据集iris = load_iris()data = iris.datalabels = iris.target# 划分训练集和测试集train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=1)# 创建贝叶斯分类器对象classifier = GaussianNB()# 在训练集上训练模型classifier.fit(train_data, train_labels)# 在测试集上进行预测predictions = classifier.predict(test_data) K近邻 K近邻对莺尾花数据集分类 12345678910111213141516171819202122232425from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.metrics import accuracy_score# 加载莺尾花数据集iris = load_iris()data = iris.datalabels = iris.target# 划分训练集和测试集train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=1)# 创建KNN分类器对象classifier = KNeighborsClassifier(n_neighbors=5)# 在训练集上训练模型classifier.fit(train_data, train_labels)# 在测试集上进行预测predictions = classifier.predict(test_data)# 计算分类准确率accuracy = accuracy_score(test_labels, predictions)print(&quot;Accuracy: &quot;, accuracy)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-分类","slug":"Code-分类","permalink":"http://example.com/tags/Code-%E5%88%86%E7%B1%BB/"}]},{"title":"Coding-文件操作","slug":"coding_python_file_operation","date":"2023-09-27T03:48:03.191Z","updated":"2024-01-23T03:55:17.327Z","comments":true,"path":"2023/09/27/coding_python_file_operation/","link":"","permalink":"http://example.com/2023/09/27/coding_python_file_operation/","excerpt":"","text":"中文字符文件名换成拼音 其他格式图片转JPG 中文字符文件名换成拼音 将输入文件夹里的图片压缩有存储到输出文件夹中 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import osimport refrom pypinyin import pinyin, Styledef sanitize_filename(filename): # 将汉字转换为拼音 pinyin_list = pinyin(filename, style=Style.NORMAL) pinyin_string = &#x27;&#x27;.join([item[0] for item in pinyin_list]) # 移除非法字符，只保留字母、数字、&quot;-&quot;、&quot;_&quot;和&quot;.&quot; sanitized_filename = re.sub(r&#x27;[^\\w\\-_.]&#x27;, &#x27;&#x27;, pinyin_string) return sanitized_filenamedef sanitize_directory(directory): # 获取目录下的所有文件和文件夹 items = os.listdir(directory) # 遍历所有项 for item in items: item_path = os.path.join(directory, item) if os.path.isdir(item_path): # 如果是目录，递归处理子目录 sanitize_directory(item_path) # 修改目录名称 sanitized_dirname = sanitize_filename(item) new_dir_path = os.path.join(directory, sanitized_dirname) if item != sanitized_dirname: os.rename(item_path, new_dir_path) else: # 如果是文件，修改文件名称 sanitized_filename = sanitize_filename(item) new_file_path = os.path.join(directory, sanitized_filename) if item != sanitized_filename: os.rename(item_path, new_file_path)# 设置文件夹路径folder_path = &#x27;input_floder&#x27;# 递归修改目录和文件名称sanitize_directory(folder_path) 其他格式图片转JPG 12345678910111213141516171819202122232425262728293031323334import osfrom PIL import Imagedef convert_to_jpg(input_path, output_path): with Image.open(input_path) as image: # 转换为JPEG格式并保存 rgb_image = image.convert(&quot;RGB&quot;) rgb_image.save(output_path, format=&quot;JPEG&quot;)def convert_images_to_jpg(input_folder, output_folder): # 如果输出文件夹不存在，则创建该文件夹 if not os.path.exists(output_folder): os.makedirs(output_folder) # 遍历文件夹下的所有子文件夹和文件 for root, dirs, files in os.walk(input_folder): for file in files: # 获取文件的完整路径 input_path = os.path.join(root, file) # 检查文件是否为图片格式 if file.lower().endswith((&#x27;.png&#x27;, &#x27;.bmp&#x27;, &#x27;.gif&#x27;, &#x27;.tiff&#x27;)): # 构建输出路径 output_path = os.path.join(output_folder, os.path.splitext(file)[0] + &#x27;.jpg&#x27;) # 将文件转换为JPEG格式 convert_to_jpg(input_path, output_path)# 设置输入和输出文件夹路径input_folder = &#x27;input_folder&#x27;output_folder = &#x27;output_folder&#x27;# 转换图片格式为JPEGconvert_images_to_jpg(input_folder, output_folder)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-文件操作","slug":"Code-文件操作","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/"}]},{"title":"Coding-图像处理","slug":"coding_python_image_process","date":"2023-09-26T09:05:15.426Z","updated":"2024-01-23T03:55:27.746Z","comments":true,"path":"2023/09/26/coding_python_image_process/","link":"","permalink":"http://example.com/2023/09/26/coding_python_image_process/","excerpt":"","text":"恢复将归一化后的图片 多线程压缩图片 其他格式图片转JPG 将文件夹下的三通道图片转为单通道 恢复将归一化后的图片 变换 123456transform = transforms.Compose([transforms.Resize((224, 224)),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]) 存储验证集中预测错误的图片 12345678910111213141516171819202122232425with torch.no_grad(): for images, labels in val_loader: images = images.to(device) labels = labels.to(device) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total_samples += labels.size(0) total_correct += (predicted == labels).sum().item() # 找出识别错误的图片 misclassified_idx = (predicted != labels) misclassified_images.extend(images[misclassified_idx].cpu().numpy()) val_accuracy = total_correct / total_samples print(f&quot;Validation Accuracy: &#123;val_accuracy:.4f&#125;&quot;) # 保存识别错误的图片 for i, image in enumerate(misclassified_images): image = np.transpose(image, (1, 2, 0)) # 转换为通道在最后的形式 image = (image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]) * 255 #恢复过程 image = image.astype(np.uint8) # 将像素值还原为 0-255 范围 image = Image.fromarray(image) image.save(f&quot;error_images/misclassified_&#123;epoch&#125;_&#123;i&#125;.jpg&quot;) 多线程压缩图片 将输入文件夹里的图片压缩有存储到输出文件夹中 123456789101112131415161718192021222324252627282930313233343536373839404142import osimport shutilfrom concurrent.futures import ThreadPoolExecutorfrom PIL import Imageinput_folder = &#x27;input_folder&#x27;output_folder = &#x27;output_folder&#x27;# 创建输出文件夹if not os.path.exists(output_folder): os.makedirs(output_folder)# 获取输入文件夹中所有图片文件的文件名file_names = [f for f in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, f)) and any(f.endswith(ext) for ext in [&#x27;.jpg&#x27;, &#x27;.jpeg&#x27;, &#x27;.png&#x27;, &#x27;.bmp&#x27;, &#x27;.gif&#x27;])]def process_image(file_name): # 获取输入文件的路径 input_file_path = os.path.join(input_folder, file_name) output_file_path = os.path.join(output_folder, file_name) # 打开图片文件 with Image.open(input_file_path) as image: # 检查图片格式 if image.format in [&#x27;JPEG&#x27;, &#x27;JPG&#x27;, &#x27;PNG&#x27;]: # 压缩图片并保存到输出文件夹 compressed_image = image.copy() compressed_image.save(output_file_path, optimize=True, quality=25) # 检查图片大小是否超过500KB while os.path.getsize(output_file_path) &gt; 500 * 800: # 缩小图片尺寸 width, height = compressed_image.size new_width = int(width * 0.9) new_height = int(height * 0.9) compressed_image = compressed_image.resize((new_width, new_height)) # 保存压缩后的图片 compressed_image.save(output_file_path, optimize=True, quality=25)# 使用ThreadPoolExecutor来并行处理每一张图片with ThreadPoolExecutor() as executor: executor.map(process_image, file_names) 其他格式图片转JPG 12345678910111213141516171819202122232425262728293031323334import osfrom PIL import Imagedef convert_to_jpg(input_path, output_path): with Image.open(input_path) as image: # 转换为JPEG格式并保存 rgb_image = image.convert(&quot;RGB&quot;) rgb_image.save(output_path, format=&quot;JPEG&quot;)def convert_images_to_jpg(input_folder, output_folder): # 如果输出文件夹不存在，则创建该文件夹 if not os.path.exists(output_folder): os.makedirs(output_folder) # 遍历文件夹下的所有子文件夹和文件 for root, dirs, files in os.walk(input_folder): for file in files: # 获取文件的完整路径 input_path = os.path.join(root, file) # 检查文件是否为图片格式 if file.lower().endswith((&#x27;.png&#x27;, &#x27;.bmp&#x27;, &#x27;.gif&#x27;, &#x27;.tiff&#x27;)): # 构建输出路径 output_path = os.path.join(output_folder, os.path.splitext(file)[0] + &#x27;.jpg&#x27;) # 将文件转换为JPEG格式 convert_to_jpg(input_path, output_path)# 设置输入和输出文件夹路径input_folder = &#x27;input_folder&#x27;output_folder = &#x27;output_folder&#x27;# 转换图片格式为JPEGconvert_images_to_jpg(input_folder, output_folder) 将文件夹下的三通道图片转为单通道 123456789101112from PIL import Imageimport ospath = &#x27;input_floder&#x27;list_ = os.listdir(path)list_ = [ os.path.join(path,i) for i in list_]for i in list_: image = Image.open(i) gray_image = image.convert(&#x27;L&#x27;) gray_image.save(i)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-图像处理","slug":"Code-图像处理","permalink":"http://example.com/tags/Code-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"Coding-医学图像处理","slug":"coding_python_medical","date":"2023-09-04T02:09:53.960Z","updated":"2024-01-23T03:55:36.622Z","comments":true,"path":"2023/09/04/coding_python_medical/","link":"","permalink":"http://example.com/2023/09/04/coding_python_medical/","excerpt":"","text":"读取nii文件，存储nii文件 读取nii文件，存储nii文件 12345678910111213141516import nibabel as nib# 读取NIfTI文件nii_file = &#x27;1_output.nii&#x27;nii_img = nib.load(nii_file)# 获取图像数据和元数据data = nii_img.get_fdata()header = nii_img.header# 进行必要的操作，例如处理数据或分析# 保存NIfTI文件output_file = &#x27;2_output.nii&#x27;output_img = nib.Nifti1Image(data, affine=None, header=header)nib.save(output_img, output_file)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-图像处理","slug":"Code-图像处理","permalink":"http://example.com/tags/Code-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"PaperReading-DMC-Fusion:基于分类器特征合成的深度多级联融合针对医学多模态图像","slug":"DMC-Fusion Deep Multi-cascade Fusion with Classifier-Based Feature Synthesis for Medical Multi-modal Images","date":"2023-09-03T11:38:59.208Z","updated":"2024-01-20T03:02:04.245Z","comments":true,"path":"2023/09/03/DMC-Fusion Deep Multi-cascade Fusion with Classifier-Based Feature Synthesis for Medical Multi-modal Images/","link":"","permalink":"http://example.com/2023/09/03/DMC-Fusion%20Deep%20Multi-cascade%20Fusion%20with%20Classifier-Based%20Feature%20Synthesis%20for%20Medical%20Multi-modal%20Images/","excerpt":"","text":"DMC-Fusion: Deep Multi-cascade Fusion with Classifier-Based Feature Synthesis for Medical Multi-modal Images 1.摘要 多模态医学图像融合是临床精确诊断和手术计划的重要课题。尽管像Densefuse这样的单特征融合策略取得了令人鼓舞的效果，但它往往不能完全保留源图像的特征。本文提出了一种基于分类器特征合成的深度多融合框架，用于多模态医学图像的自动融合。该算法由基于密集连接（dense connections）的预训练自编码器、特征分类器和一种分别融合高频和低频的多级联融合解码器。编码器和解码器从MS-COCO数据集传输，并在多模态医学图像公共数据集上同时进行预训练以提取特征。通过高斯高通滤波和峰值信噪比阈值法对特征进行分类，然后将预训练的Dense-Block和解码器的每一层特征映射划分为高频和低频序列。具体而言，在所提出的特征融合块中，采用参数自适应脉冲耦合神经网络和L1加权分别进行高频和低频融合。最后，我们在全解码特征阶段设计了一种新型的多级融合解码器，以选择性地融合不同模态的有用信息。我们还使用融合图像验证了我们的方法对脑部疾病的分类，并进行了统计显著性检验，以说明分类性能的提高是由于融合。实验结果表明，该方法在定性和定量评价方面都达到了最先进的水平。 2.引言 3.论文结构 4.总结 5.文章笔记 MRI和PET的基本原理的叙述： 磁共振成像(MRI)是一种利用磁场和无线电波对人体内部器官进行成像的结构方式。这种非侵入性诊断工具测量所需身体部位的解剖结构。而正电子发射断层扫描(PET)是一种功能模式，使用放射性示踪剂来量化人体组织和器官中的代谢活动。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[]},{"title":"Coding-常用神经网络","slug":"coding_network_usual_use","date":"2023-09-03T11:20:46.630Z","updated":"2024-01-23T03:11:35.385Z","comments":true,"path":"2023/09/03/coding_network_usual_use/","link":"","permalink":"http://example.com/2023/09/03/coding_network_usual_use/","excerpt":"","text":"MLP 2D-Unet 2D-Resnet 3D-Resnet MLP 1234567891011121314151617181920212223242526import torch.nn as nnclass FCModel(nn.Module): def __init__(self, input_size, hidden_size, num_classes): super(FCModel, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.fc2 = nn.Linear(hidden_size, num_classes) def forward(self, x): batch_size, seq_length, input_size = x.size() x = x.view(batch_size * seq_length, input_size) # Reshape input to (batch_size * seq_length, input_size) h = self.fc1(x) out = self.fc2(h) out = out.view(batch_size, seq_length, -1) # Reshape output back to (batch_size, seq_length, num_classes) out = out[:, -1, :] # Take the last time step&#x27;s output return out# 设置超参数input_size = 13456hidden_size = 64num_classes = 2# 创建模型实例model = FCModel(input_size, hidden_size, num_classes) 2D-Unet 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import torchimport torch.nn as nnimport torch.nn.functional as F# UNet的一大层，包含了两层小的卷积class DoubleConv(nn.Module): def __init__(self, in_ch, out_ch): super(DoubleConv, self).__init__() self.conv = nn.Sequential( nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True), nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True) ) def forward(self, x): x = self.conv(x) return x# 定义输入进来的第一层class InConv(nn.Module): def __init__(self, in_ch, out_ch): super(InConv, self).__init__() self.conv = DoubleConv(in_ch, out_ch) def forward(self, x): x = self.conv(x) return x# 定义encoder中的向下传播，包括一个maxpool和一大层 class Down(nn.Module): def __init__(self, in_ch, out_ch): super(Down, self).__init__() self.mpconv = nn.Sequential( nn.MaxPool2d(2), DoubleConv(in_ch, out_ch) ) def forward(self, x): x = self.mpconv(x) return x# 定义decoder中的向上传播class Up(nn.Module): def __init__(self, in_ch, out_ch, bilinear=True): super(Up, self).__init__() # 定义了self.up的方法 if bilinear: self.up = nn.Upsample(scale_factor=2, mode=&#x27;bilinear&#x27;, align_corners=True) else: self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2) # // 除以的结果向下取整 self.conv = DoubleConv(in_ch, out_ch) def forward(self, x1, x2): # x2是左侧的输出，x1是上一大层来的输出 x1 = self.up(x1) diffY = x2.size()[2] - x1.size()[2] diffX = x2.size()[3] - x1.size()[3] x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2)) x = torch.cat([x2, x1], dim=1) # 将两个tensor拼接在一起 dim=1：在通道数（C）上进行拼接 x = self.conv(x) return x# 定义最终的输出class OutConv(nn.Module): def __init__(self, in_ch, out_ch): super(OutConv, self).__init__() self.conv = nn.Conv2d(in_ch, out_ch, 1) def forward(self, x): x = self.conv(x) return xclass Unet(nn.Module): def __init__(self, in_channels, classes): # in_channels 图片的通道数，1为灰度图，3为彩色图 super(Unet, self).__init__() self.n_channels = in_channels self.n_classes = classes self.inc = InConv(in_channels, 64) self.down1 = Down(64, 128) self.down2 = Down(128, 256) self.down3 = Down(256, 512) self.down4 = Down(512, 512) self.up1 = Up(1024, 256) self.up2 = Up(512, 128) self.up3 = Up(256, 64) self.up4 = Up(128, 64) self.outc = OutConv(64, classes) def forward(self, x): x1 = self.inc(x) x2 = self.down1(x1) x3 = self.down2(x2) x4 = self.down3(x3) x5 = self.down4(x4) x = self.up1(x5, x4) x = self.up2(x, x3) x = self.up3(x, x2) x = self.up4(x, x1) x = self.outc(x) return x 2D-Resnet 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105import torchimport torch.nn as nnclass BasicBlock(nn.Module): expansion = 1 def __init__(self, in_channels, out_channels, stride=1): super(BasicBlock, self).__init__() # 第一个卷积层 self.conv1 = nn.Conv2d( in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False ) self.bn1 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) # 第二个卷积层 self.conv2 = nn.Conv2d( out_channels, out_channels * self.expansion, kernel_size=3, stride=1, padding=1, bias=False, ) self.bn2 = nn.BatchNorm2d(out_channels * self.expansion) # 残差连接（shortcut connection） self.shortcut = nn.Sequential() if stride != 1 or in_channels != out_channels * self.expansion: self.shortcut = nn.Sequential( nn.Conv2d( in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False, ), nn.BatchNorm2d(out_channels * self.expansion), ) def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out += self.shortcut(residual) out = self.relu(out) return outclass ResNet(nn.Module): def __init__(self, block, num_blocks, num_classes=10): super(ResNet, self).__init__() self.in_channels = 64 # 第一个卷积层 self.conv1 = nn.Conv2d( 3, 64, kernel_size=3, stride=1, padding=1, bias=False ) self.bn1 = nn.BatchNorm2d(64) self.relu = nn.ReLU(inplace=True) # ResNet的四个阶段 self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1) self.layer2 = self.make_layer(block, 128, num_blocks[1], stride=2) self.layer3 = self.make_layer(block, 256, num_blocks[2], stride=2) self.layer4 = self.make_layer(block, 512, num_blocks[3], stride=2) # 全局平均池化层和全连接层 self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Linear(512 * block.expansion, num_classes) def make_layer(self, block, out_channels, num_blocks, stride): layers = [] layers.append(block(self.in_channels, out_channels, stride)) self.in_channels = out_channels * block.expansion for _ in range(1, num_blocks): layers.append(block(self.in_channels, out_channels)) return nn.Sequential(*layers) def forward(self, x): out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.layer1(out) out = self.layer2(out) out = self.layer3(out) out = self.layer4(out) out = self.avg_pool(out) out = torch.flatten(out, 1) out = self.fc(out) return outdef ResNet18(num_classes=10): return ResNet(BasicBlock, [2, 2, 2, 2], num_classes) 3D-Resnet 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103class BasicBlock(nn.Module): expansion = 1 def __init__(self, in_channels, out_channels, stride=1): super(BasicBlock, self).__init__() # 第一个卷积层 self.conv1 = nn.Conv3d( in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False ) self.bn1 = nn.BatchNorm3d(out_channels) self.relu = nn.ReLU(inplace=True) # 第二个卷积层 self.conv2 = nn.Conv3d( out_channels, out_channels * self.expansion, kernel_size=3, stride=1, padding=1, bias=False, ) self.bn2 = nn.BatchNorm3d(out_channels * self.expansion) # 残差连接（shortcut connection） self.shortcut = nn.Sequential() if stride != 1 or in_channels != out_channels * self.expansion: self.shortcut = nn.Sequential( nn.Conv3d( in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False, ), nn.BatchNorm3d(out_channels * self.expansion), ) def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out += self.shortcut(residual) out = self.relu(out) return outclass ResNet(nn.Module): def __init__(self, block, num_blocks, num_classes=10): super(ResNet, self).__init__() self.in_channels = 64 # 第一个卷积层 self.conv1 = nn.Conv3d( 1, 64, kernel_size=3, stride=1, padding=1, bias=False ) self.bn1 = nn.BatchNorm3d(64) self.relu = nn.ReLU(inplace=True) # ResNet的四个阶段 self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1) self.layer2 = self.make_layer(block, 128, num_blocks[1], stride=2) self.layer3 = self.make_layer(block, 256, num_blocks[2], stride=2) self.layer4 = self.make_layer(block, 512, num_blocks[3], stride=2) # 全局平均池化层和全连接层 self.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1)) self.fc = nn.Linear(512 * block.expansion, num_classes) def make_layer(self, block, out_channels, num_blocks, stride): layers = [] layers.append(block(self.in_channels, out_channels, stride)) self.in_channels = out_channels * block.expansion for _ in range(1, num_blocks): layers.append(block(self.in_channels, out_channels)) return nn.Sequential(*layers) def forward(self, x): out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.layer1(out) out = self.layer2(out) out = self.layer3(out) out = self.layer4(out) out = self.avg_pool(out) out = torch.flatten(out, 1) out = self.fc(out) return outdef ResNet18_3D(num_classes=10): return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-模型","slug":"Code-模型","permalink":"http://example.com/tags/Code-%E6%A8%A1%E5%9E%8B/"}]},{"title":"Coding-彩色图片分类-基于2D-Unet","slug":"coding_network_example","date":"2023-09-02T13:09:09.106Z","updated":"2024-01-23T03:54:55.135Z","comments":true,"path":"2023/09/02/coding_network_example/","link":"","permalink":"http://example.com/2023/09/02/coding_network_example/","excerpt":"","text":"摘要 main.py 摘要 训练一个图片分类神经网络（2D-Unet），包括 1.自定义dataload制作 2,网络定义 3.训练过程 4.测试过程 5.模型评估（准确率） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120import glob # 导入用于文件路径匹配的模块from torchvision import transforms # 导入图像转换模块from torch.utils import data # 导入PyTorch数据工具模块from PIL import Image # 导入PIL图像处理库import matplotlib.pyplot as plt # 导入绘图库import torch # 导入PyTorch库import torch.nn as nn # 导入PyTorch神经网络模块import torch.nn.functional as F # 导入PyTorch函数库from unet import Unet # 导入自定义的U-Net模型import numpy as np # 导入NumPy库# 标准化数据transforms = transforms.Compose([ transforms.ToTensor(), # 将图像转换为张量 transforms.Resize((256, 256)), # 调整图像大小为256x256 transforms.Normalize(mean=0.5, std=0.5) # 标准化图像数据])class my_dataset(data.Dataset): def __init__(self, imgs_path, annos_path): self.imgs_path = imgs_path # 图像文件路径 self.annos_path = annos_path # 标签文件路径 def __getitem__(self, index): img_path = self.imgs_path[index] # 获取图像路径 pil_img = Image.open(img_path) # 使用PIL打开图像 pil_img = transforms(pil_img) # 对图像进行预处理 anno_path = self.annos_path[index] # 获取标签路径 anno_img = Image.open(anno_path) # 使用PIL打开标签图像 pil_anno = transforms(anno_img) # 对标签图像进行预处理 return pil_img, pil_anno def __len__(self): return len(self.imgs_path) # 返回数据集的长度def train(model, train_loader, criterion, optimizer, device): model.train() # 设置模型为训练模式 train_loss = 0 for data, label in train_loader: data = data.to(device) optimizer.zero_grad() # 清除梯度 output = model(data) # 前向传播 loss = criterion(output, label.to(device).float()) # 计算损失 loss.backward() # 反向传播，计算梯度 optimizer.step() # 更新模型参数 train_loss += loss.item() * data.size(0) train_loss /= len(train_loader.dataset) # 计算平均训练损失 return train_lossdef validate(model, val_loader, criterion, device): model.eval() # 设置模型为评估模式 val_loss = 0 with torch.no_grad(): for data, label in val_loader: data = data.to(device) output = model(data) # 前向传播 loss = criterion(output, label.to(device).float()) # 计算损失 val_loss += loss.item() * data.size(0) val_loss /= len(val_loader.dataset) # 计算平均验证损失 return val_loss if __name__ ==&#x27;__main__&#x27;: # 训练数据集导入 imgs_path = glob.glob(&#x27;facade/train_picture/*.png&#x27;) # 匹配训练图像文件路径 label_path = glob.glob(&#x27;facade/train_label/*.jpg&#x27;) # 匹配训练标签文件路径 # 测试数据集导入 test_imgs_path = glob.glob(&#x27;facade/test_picture/*.png&#x27;) # 匹配测试图像文件路径 test_label_path = glob.glob(&#x27;facade/test_label/*.jpg&#x27;) # 匹配测试标签文件路径 # 对数据和标签排序，确保一一对应 imgs_path = sorted(imgs_path) label_path = sorted(label_path) test_imgs_path = sorted(test_imgs_path) test_label_path = sorted(test_label_path) train_dataset = my_dataset(imgs_path, label_path) test_dataset = my_dataset(test_imgs_path, test_label_path) # 创建测试数据集对象 train_loader = data.DataLoader(train_dataset, batch_size=4, shuffle=True) # 创建训练数据加载器 test_loader = data.DataLoader(test_dataset, batch_size=4, shuffle=False) # 创建测试数据加载器 # 创建U-Net模型 in_channels = 3 # 输入通道数 out_channels = 3 # 输出通道数 model = Unet(in_channels, out_channels) # 创建U-Net模型对象 criterion = nn.MSELoss() # 创建均方误差损失函数对象 optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # 创建优化器对象 # 将模型和数据移动到GPU上 device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;) # 检查是否有可用的GPU model.to(device) # 将模型移动到GPU上 train_losses = [] # 保存训练损失的列表 val_losses = [] # 保存验证损失的列表 best_val_loss = np.inf # 初始化最佳验证损失为正无穷 best_model = None # 初始化最佳模型为空 epoch_times = 300 # 设定迭代次数 # 训练模型 for epoch in range(epoch_times): train_loss = train(model, train_loader, criterion, optimizer, device) # 训练模型 val_loss = validate(model, test_loader, criterion, device) # 验证模型 train_losses.append(train_loss) # 保存训练损失 val_losses.append(val_loss) # 保存验证损失 if val_loss &lt; best_val_loss: best_val_loss = val_loss best_model = model.state_dict() torch.save(best_model, &#x27;ckpt/model.ckpt&#x27;) # 保存最佳模型参数 print(&quot;best_val_loss: &quot; + str(val_loss)) with open(&quot;ckpt/model_loss.txt&quot;, &quot;w&quot;) as f: f.write(str(val_loss)) print(&#x27;Epoch [&#123;&#125;/&#123;&#125;], Train Loss: &#123;:.4f&#125;, Val Loss: &#123;:.4f&#125;&#x27;.format(epoch+1, epoch_times, train_loss, val_loss)) val.py (导入模型进行生成测试) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from unet import Unet # 导入自定义的U-Net模型import torch # 导入PyTorch库from torch.utils import data # 导入PyTorch数据工具模块from PIL import Image # 导入PIL图像处理库from torchvision import transforms # 导入图像转换模块import glob # 导入用于文件路径匹配的模块# 标准化数据transforms = transforms.Compose([ transforms.ToTensor(), # 将图像转换为张量 transforms.Resize((256, 256)), # 调整图像大小为256x256 transforms.Normalize(mean=0.5, std=0.5) # 标准化图像数据])class my_dataset(data.Dataset): def __init__(self, imgs_path, annos_path): self.imgs_path = imgs_path # 图像文件路径 self.annos_path = annos_path # 标签文件路径 def __getitem__(self, index): img_path = self.imgs_path[index] # 获取图像路径 pil_img = Image.open(img_path) # 使用PIL打开图像 pil_img = transforms(pil_img) # 对图像进行预处理 anno_path = self.annos_path[index] # 获取标签路径 anno_img = Image.open(anno_path) # 使用PIL打开标签图像 pil_anno = transforms(anno_img) # 对标签图像进行预处理 return pil_img, pil_anno def __len__(self): return len(self.imgs_path) # 返回数据集的长度# 测试数据集导入test_imgs_path = glob.glob(&#x27;facade/test_picture/*.png&#x27;) # 匹配测试图像文件路径test_label_path = glob.glob(&#x27;facade/test_label/*.jpg&#x27;) # 匹配测试标签文件路径test_dataset = my_dataset(test_imgs_path, test_label_path) # 创建测试数据集对象test_loader = data.DataLoader(test_dataset, batch_size=1, shuffle=False) # 创建测试数据加载器model = Unet(3, 3) # 创建U-Net模型对象checkpoint = torch.load(&#x27;ckpt/model.ckpt&#x27;) # 加载模型参数model.load_state_dict(checkpoint) # 加载模型参数for data, label in test_loader: data = data * 0.5 + 0.5 # 反标准化图像数据 output = model(data) # 前向传播 output = torch.squeeze(output, 0) # 去除输出张量的维度为1的维度 array = output.cpu().detach().numpy().transpose(1, 2, 0) # 将输出张量转换为NumPy数组，并调整通道顺序为HWC image = Image.fromarray((array * 255).astype(&#x27;uint8&#x27;)) # 创建PIL图像对象 image.save(&#x27;image1.jpg&#x27;) # 保存图像为JPEG文件 unet.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import torchimport torch.nn as nnimport torch.nn.functional as F# UNet的一大层，包含了两层小的卷积class DoubleConv(nn.Module): def __init__(self, in_ch, out_ch): super(DoubleConv, self).__init__() self.conv = nn.Sequential( nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True), nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True) ) def forward(self, x): x = self.conv(x) return x# 定义输入进来的第一层class InConv(nn.Module): def __init__(self, in_ch, out_ch): super(InConv, self).__init__() self.conv = DoubleConv(in_ch, out_ch) def forward(self, x): x = self.conv(x) return x# 定义encoder中的向下传播，包括一个maxpool和一大层 class Down(nn.Module): def __init__(self, in_ch, out_ch): super(Down, self).__init__() self.mpconv = nn.Sequential( nn.MaxPool2d(2), DoubleConv(in_ch, out_ch) ) def forward(self, x): x = self.mpconv(x) return x# 定义decoder中的向上传播class Up(nn.Module): def __init__(self, in_ch, out_ch, bilinear=True): super(Up, self).__init__() # 定义了self.up的方法 if bilinear: self.up = nn.Upsample(scale_factor=2, mode=&#x27;bilinear&#x27;, align_corners=True) else: self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2) # // 除以的结果向下取整 self.conv = DoubleConv(in_ch, out_ch) def forward(self, x1, x2): # x2是左侧的输出，x1是上一大层来的输出 x1 = self.up(x1) diffY = x2.size()[2] - x1.size()[2] diffX = x2.size()[3] - x1.size()[3] x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2)) x = torch.cat([x2, x1], dim=1) # 将两个tensor拼接在一起 dim=1：在通道数（C）上进行拼接 x = self.conv(x) return x# 定义最终的输出class OutConv(nn.Module): def __init__(self, in_ch, out_ch): super(OutConv, self).__init__() self.conv = nn.Conv2d(in_ch, out_ch, 1) def forward(self, x): x = self.conv(x) return xclass Unet(nn.Module): def __init__(self, in_channels, classes): # in_channels 图片的通道数，1为灰度图，3为彩色图 super(Unet, self).__init__() self.n_channels = in_channels self.n_classes = classes self.inc = InConv(in_channels, 64) self.down1 = Down(64, 128) self.down2 = Down(128, 256) self.down3 = Down(256, 512) self.down4 = Down(512, 512) self.up1 = Up(1024, 256) self.up2 = Up(512, 128) self.up3 = Up(256, 64) self.up4 = Up(128, 64) self.outc = OutConv(64, classes) def forward(self, x): x1 = self.inc(x) x2 = self.down1(x1) x3 = self.down2(x2) x4 = self.down3(x3) x5 = self.down4(x4) x = self.up1(x5, x4) x = self.up2(x, x3) x = self.up3(x, x2) x = self.up4(x, x1) x = self.outc(x) return x","categories":[{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Code-分类","slug":"Code-分类","permalink":"http://example.com/tags/Code-%E5%88%86%E7%B1%BB/"}]},{"title":"PaperReading-生成对抗网络在医学成像中的应用综述","slug":"paper_生成对抗网络在医学成像中的应用综述","date":"2023-09-01T08:00:37.614Z","updated":"2024-01-20T03:01:36.386Z","comments":true,"path":"2023/09/01/paper_生成对抗网络在医学成像中的应用综述/","link":"","permalink":"http://example.com/2023/09/01/paper_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E5%9C%A8%E5%8C%BB%E5%AD%A6%E6%88%90%E5%83%8F%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E7%BB%BC%E8%BF%B0/","excerpt":"","text":"Generative adversarial network in medical imaging: A review 1.摘要 生成对抗网络由于其无需明确建模概率密度函数即可生成数据的能力而在计算机视觉界受到了广泛的关注。鉴别器带来的对抗损失提供了一种巧妙的方法，将未标记的样本整合到训练中，并实现更高的阶一致性。事实证明，这在许多情况下都很有用，例如域适应、数据增强和图像到图像的转换。这些特性吸引了医学影像界的研究人员，并在许多传统和新型应用中得到了迅速的应用，如图像重建、分割、检测、分类和交叉模态合成。根据我们的观察，这一趋势将继续下去，因此我们对使用对抗训练方案的医学成像的最新进展进行了回顾，希望能使对这项技术感兴趣的研究人员受益。 2.引言 从2012年开始，随着深度学习在计算机视觉领域的复兴(Krizhevsky et al, 2012)，深度学习方法在医学成像领域的应用急剧增加。 gan是一种特殊类型的神经网络模型，它同时训练两个网络，一个专注于图像生成，另一个专注于识别。 深度学习的根源可以追溯到20世纪80年代(Fukushima and Miyake, 1982)，而对抗训练的概念相对较新，最近取得了重大进展(Goodfellow et al .， 2014)。本文介绍了gan的总体概况，描述了它们在医学成像中的应用前景，并确定了一些需要解决的挑战，以使它们在其他医学成像相关任务中成功应用 为了全面概述gan在医学成像方面的所有相关工作，我们检索了PubMed、arXiv… 本文的其余部分结构如下… 3.论文结构 2.背景 2.1 Vanilla GAN 2.2 优化gan的挑战 2.3 gan的变体 2.3.1 D的变化目标 2.3.2 G的变化目标 3.在医学成像中的应用 3.1. 重建 3.2 医学图像合成 3.2.1 无条件的合成 3.2.2 交叉模态合成 3.2.3 其他条件合成 3.3 分割 3.4 分类 3.5 检测 3.6 配准 3.7 其他工作 4.讨论 4.1 未来的挑战 4.2 有趣的未来应用 4.文章笔记 GAN存在的问题 并不能保证G和D的训练在JS发散的情况下达到平衡。因此，一个网络可能不可避免地比另一个网络更强大，在大多数情况下是D。当D变得过于强大而不是G时，生成的样本变得太容易与真实样本分离，从而达到D的梯度接近于零的阶段，无法为G的进一步训练提供指导。由于难以生成有意义的高频细节，这种情况在生成高分辨率图像时更常见。 另一个在训练gan时经常遇到的问题是模态崩溃，顾名思义，模态崩溃是指G学习到的分布集中在数据分布的几个有限模态上。因此，它产生的不是不同的图像，而是一组有限的样本。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"GAN","slug":"GAN","permalink":"http://example.com/tags/GAN/"}]},{"title":"PaperReading-BPGAN-使用光谱归一化和定位的多生成多对抗网络进行CT-MRI双向预测","slug":"paper_BPGAN-使用光谱归一化和定位的多生成多对抗网络进行ct-mri双向预测","date":"2023-08-31T12:54:24.310Z","updated":"2024-01-20T03:01:15.435Z","comments":true,"path":"2023/08/31/paper_BPGAN-使用光谱归一化和定位的多生成多对抗网络进行ct-mri双向预测/","link":"","permalink":"http://example.com/2023/08/31/paper_BPGAN-%E4%BD%BF%E7%94%A8%E5%85%89%E8%B0%B1%E5%BD%92%E4%B8%80%E5%8C%96%E5%92%8C%E5%AE%9A%E4%BD%8D%E7%9A%84%E5%A4%9A%E7%94%9F%E6%88%90%E5%A4%9A%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E8%BF%9B%E8%A1%8Cct-mri%E5%8F%8C%E5%90%91%E9%A2%84%E6%B5%8B/","excerpt":"","text":"BPGAN: Bidirectional CT-to-MRI prediction using multi-generative multi-adversarial nets with spectral normalization and localization 1.摘要 磁共振成像(MRI)和计算机断层扫描(CT)是广泛应用于临床和研究的筛查、诊断和图像引导治疗的检测技术。然而，CT在采集过程中对患者施加电离辐射。与CT相比，MRI更安全，没有任何辐射，但更昂贵，采集时间更长。因此，在放疗规划的情况下，有必要从同一受试者的另一个给定的模态图像中估计一个模态图像。考虑到目前MRI和CT图像之间没有双向预测模型，我们提出了一种双向预测方法，即使用多生成多对抗网络(BPGAN)以成对和非成对的方式从另一模态图像中预测任意模态。在BPGAN中，采用循环一致性策略，通过将相同的病理特征从一个域投射到另一个域来学习两个非线性映射。在技术上，引入病理先验信息来约束特征生成，以攻击病理变异的潜在风险，并采用边缘保留度量来保留几何畸变和解剖结构。在算法上，设计了谱归一化来控制鉴别器的性能，使预测器更好更快地学习;提出了局部化来对预测器施加正则化，以减少泛化误差。实验结果表明，BPGAN比目前最先进的方法产生更好的预测。其中，BPGAN在两个基线数据集上的MAE和SSIM的平均增量分别为33.2%和37.4%，SSIM分别为24.5%和44.6% 2.引言 MRI和CT在各种医疗病例中都是重要的和广泛应用的技术。与MRI相比，CT的成像时间更短，空间分辨率更高，适用于骨骼和胸部的检测。但CT软组织信息对比度较低。考虑到电离辐射和CT的不同耐受性，MRI更适合正电子发射断层扫描(PET)中的衰减校正(AC)和现代放疗治疗计划中的剂量计算，MRI优越的软组织对比度有助于精确描绘肿瘤和危险器官。但MRI价格昂贵，且出于患者舒适度和依从性的考虑，采集时间较长，而标准的MRI引导临床治疗包括CT和MRI图像的采集。因此，由相应的和真实的MRI/CT图像准确合成的无偏伪CT/MRI图像(pCT/pMRI)，在无法获得真实CT/MRI信息的情况下，在临床应用中是非常有用的。 为了减少不必要的电离剂量和患者的额外费用，临床上需要从另一个模态图像中估计一个模态图像。在这一需求的启发下，人们提出了许多创新性的单向预测方法，但实现这一目标仍然存在两个主要挑战 1)几乎所有的预测算法都只能进行单向预测，即从给定的MRI预测pCT或从给定的CT预测pMRI 2)在双向预测中，预测器可能产生目标图像中未显示的特征，这是一个潜在的风险。 为了解决这两个具有挑战性的问题，需要解决两个子问题:首先，学习MRI和CT域之间的双向映射;其次，预测的伪MRI和CT所描述的病理信息应与原始图像所描述的病理信息相同。CycleGAN和conditional GAN在这两个子问题上取得了巨大进展。然而，CycleGAN在几何变换方面存在固有的模糊性。 基于以上分析，我们提出了一种基于多生成多对抗网络(BPGAN)的双向预测方法: (1) 提出了一种新的双向预测方法，以配对和不配对的方式从另一个给定模态中预测任意模态图像，这是跨模态预测的第一个端到端双向预测模型。 (2) 引入病理辅助信息约束特征生成，打击病理变异的潜在风险，采用局部预测器消除预测器反求的约束，搜索给定模态图像对应的全局坐标，降低泛化误差; (3) 设计了谱归一化来控制鉴别器的性能，保证了在控制Lipschitz界方面的理论论证，在稳定性和收敛性方面做出了重大贡献; (4) 更全面的评价，包括对pCT/pMRI影像的客观评价和对诊断质量的主观评价。大量实验表明，该方法在主观上和客观上都取得了令人满意的预测结果。 本文的其余部分组织如下:… 3.总结 提出了一种基于高斯的交叉模态医学图像双向预测方案，该方案采用多生成多对抗网络进行光谱归一化和定位。为了消除病理变异的潜在风险，在同一类中加入辅助信息来生成特征，并采用局部定位来直接访问局部几何，而不是在全局GAN中反转预测器。然后利用光谱归一化控制鉴别器的性能，间接提高了预测图像的质量。此外，边缘保留度量用于保留解剖结构，总变异损失用于抑制训练过程中的噪声。总的来说，所提出的BPGAN产生了有希望的交叉模态预测结果。特别的是，它在基准上优于30% MAE, 20% SSIM, 20% FSIM, 50% MSIM, 15% GAN-train和10% GAN-test的平均增量。然后，主任医师的专业评估进一步证明BPGAN产生了令人信服的诊断质量，这与广泛的定量评估是一致的。 4.文章笔记 病理不变性： 在临床上，同一患者的同一器官的病理信息在预测时应该是相同的，本文称之为病理不变性。 CycleGAN的缺点 CycleGAN (Zhu et al .， 2017)在几何变换方面存在固有的模糊性。具体来说，CycleGAN中循环一致性的核心是保证GA(GB(x))→x和GB(GA(y))→y，但GB(x)→y和GA(y)→x不能保证几何畸变，它们是完全预期的。 谱归一化 归一化（Spectral Normalization）是一种在神经网络中常用的正则化技术，旨在稳定和改进生成对抗网络（GANs）和其他深度学习模型的训练过程。在谱归一化中，权重矩阵的每一行都被约束在单位球（L2 范数为1的球）上。这样做的目的是通过限制权重的范围来控制模型的复杂度，并提高模型的泛化性能。谱归一化的主要步骤是通过计算权重矩阵的特征值分解来估计权重矩阵的最大奇异值（spectral norm）。然后将权重矩阵除以最大奇异值以进行归一化。这可以通过迭代幂法（power iteration）来实现，迭代幂法通过多次迭代权重矩阵和其转置矩阵的乘积来逐渐逼近最大奇异值。谱归一化的优点包括： (1)改善模型的稳定性：通过限制权重矩阵的范围，谱归一化可以降低模型训练过程中的梯度爆炸和梯度消失问题，从而提高模型的稳定性。 (2)提高生成对抗网络（GANs）的训练效果：谱归一化在生成器和判别器网络中应用广泛，可以使GANs的训练更加稳定，生成的样本质量更高。 (3)不增加额外的模型参数：与其他正则化方法（如权重衰减）相比，谱归一化不需要引入额外的超参数或调整权重衰减系数，因此更易于使用。 总之，谱归一化是一种用于正则化神经网络的技术，通过限制权重矩阵的谱范数来提高模型的稳定性和泛化性能。它在生成对抗网络和其他深度学习模型中具有重要的应用价值。 几种损失: 双向条件对抗性损失(Bidirectional conditional adversarial loss): 边缘保持损失(Edge retention loss):Edge retention loss是一种用于保留医学图像中边缘信息的损失函数，它被用于这篇论文中的医学图像配准任务。在医学图像配准任务中，保留边缘信息对于保持图像的解剖结构非常重要，因为它可以帮助医生更准确地诊断和治疗疾病。具体来说，Edge retention loss的计算方式是通过计算生成的图像与真实图像之间的边缘信息的差异来实现的。边缘信息可以通过计算图像的梯度来获得，因为梯度可以反映图像中像素值的变化。在这篇论文中，Matting Laplacian矩阵被用于计算图像的梯度，因为它可以帮助保留图像中的边缘信息。通过计算生成的图像与真实图像之间的边缘信息的差异。 内容损失（Content Loss） 在CNN网络中，一般认为较低层的特征描述了图像的具体视觉特征（即纹理、颜色等），较高层的特征则是较为抽象的图像内容描述。所以要比较两幅图像的内容相似性，可以比较两幅图像在CNN网络中高层特征的相似性（欧式距离）。 风格损失（Style Loss） 而要比较两幅图像的风格相似性，则可以比较它们在CNN网络中较低层特征的相似性。不过值得注意的是，不能像内容相似性计算一样，简单的采用欧式距离度量，因为低层特征包含较多的图像局部特征（即空间信息过于显著），比如两幅风格相似但内容完全不同的图像，若直接计算它们的欧式距离，则可能会产生较大的误差，认为它们风格不相似。论文中使用了Gram矩阵，用于计算不同响应层之间的联系，即在保留低层特征的同时去除图像内容的影响，只比较风格的相似性。 感知损失perceptual loss（VGG损失） 对于图像风格化，图像超分辨率重建等任务来说，早期都使用了图像像素空间的L2 loss，但是L2 loss与人眼感知的图像质量并不匹配，恢复出来的图像往往细节表现不好。 现在的研究中，L2 loss逐步被人眼感知loss所取代。人眼感知loss也被称为perceptual loss（感知损失），它与MSE（L2损失）采用图像像素进行求差的不同之处在于所计算的空间不再是图像空间。 研究者们常使用VGG等网络的特征，令φ来表示损失网络，Cj表示网络的第j层，CjHjWj表示第j层的特征图的大小，感知损失的定义与L2 loss同样的形式，只是计算的空间被转换到了特征空间。 TV Loss(Total Variation Loss) 全名为总变分损失函数，TV Loss作为一种正则项配合损失函数去调节网络学习。总变分的公式如下： LTV=∑(xi,j−1−xi,j)2+(xi+1,j−xi,j)2L_{T V}=\\sum \\sqrt{\\left(x_{i, j-1}-x_{i, j}\\right)^2+\\left(x_{i+1,j}-x_{i, j}\\right)^2} LTV​=∑(xi,j−1​−xi,j​)2+(xi+1,j​−xi,j​)2​ 即求每一个像素与其下方像素和右方像素的差的平方相加再开根号的和。 TV值和噪声是线性相关的，噪声越大TV值也会越大，所以TV值可以作为在图像复原或超分辨等任务中的一种指导正侧项，TVloss越小则图像噪声越小，图像更加平滑。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"GAN","slug":"GAN","permalink":"http://example.com/tags/GAN/"}]},{"title":"PaperReading-医学影像中的扩散模型:综合综述","slug":"paper_医学影像中的扩散模型 综合综述","date":"2023-08-31T03:45:29.886Z","updated":"2024-01-20T03:01:23.118Z","comments":true,"path":"2023/08/31/paper_医学影像中的扩散模型 综合综述/","link":"","permalink":"http://example.com/2023/08/31/paper_%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E4%B8%AD%E7%9A%84%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%20%E7%BB%BC%E5%90%88%E7%BB%BC%E8%BF%B0/","excerpt":"","text":"DIFFUSION MODELS IN MEDICAL IMAGING: A COMPREHENSIVE SURVEY 1.摘要 去噪扩散模型是一类生成模型，最近在各种深度学习问题中引起了极大的兴趣。扩散概率模型定义了一个正向扩散阶段，在这个阶段中，输入数据通过加入高斯噪声在几个步骤中逐渐受到扰动，然后学习反向扩散过程以从有噪声的数据样本中恢复所需的无噪声数据。扩散模型因其强大的风格覆盖和生成样本的质量而广受欢迎，尽管它们已知的计算负担。利用计算机视觉的进步，医学成像领域也被观察到对扩散模型的兴趣日益增长。为了帮助研究人员浏览这一丰富的内容，本调查旨在提供医学成像学科中扩散模型的全面概述。具体来说，我们首先介绍扩散模型和三种通用扩散建模框架(即扩散概率模型、噪声条件评分网络和随机微分方程)背后的坚实理论基础和基本概念。然后，我们对医学领域的扩散模型进行了系统的分类，并提出了基于其应用，成像方式，感兴趣的器官和算法的多视角分类。为此，我们涵盖了扩散模型在医学领域的广泛应用，包括图像到图像的转换、重建、配准、分类、分割、去噪、2/3D生成、异常检测和其他与医学相关的挑战。此外，我们强调了一些选定方法的实际用例，然后讨论了扩散模型在医学领域的局限性，并提出了满足该领域需求的几个方向。最后，我们在GitHub上收集了概述的研究及其可用的开源实现。我们的目标是定期更新其中的相关最新论文。 2.引言 （1）在过去十年中，使用神经网络的生成建模一直是深度学习的主导力量。自其出现以来，生成模型在图像、音频、文本和点云等各个领域产生了巨大的影响。（2）在过去的几年里，由于一般深度学习架构的发展，人们对生成模型的兴趣重新燃起，揭示了视觉保真度和采样速度的提高。具体来说，已经出现了生成对抗网络(GANs)、变分自编码器(VAEs)和归一化流。除此之外，基于扩散过程的生成模型为现有的VAEs、EBMs、gan和规范化流提供了一种替代方案，这些模型不需要对后验分布进行对齐、估计难以处理的配分函数、引入额外的判别器网络或分别放置网络约束。（3）迄今为止，已经发现扩散模型在许多领域都很有用，从生成建模任务(如图像生成、图像超分辨率、图像绘制)到判别任务(如图像分割、分类和异常检测)。（4）最近，医学影像领域基于扩散的技术数量呈指数级增长。我们的主要贡献包括: 1）是第一篇全面涵盖扩散模型在医学成像领域应用的调查论文。具体来说，我们将全面概述所有可用的相关论文(直到2022年10月)，并展示2023年4月之前的一些最新技术。 2）我们设计了一个医学界扩散模型的多视角分类，为扩散模型及其应用的研究提供了一个系统的分类。我们将现有的扩散模型分为两类:基于变分的模型和基于分数的模型。此外，我们将扩散模型的应用分为九类:图像到图像的翻译、重建、配准、分类、分割、去噪、图像生成、异常检测和其他应用。 3）我们没有将注意力限制在应用上，并提供了一个新的分类法，其中每篇论文分别根据所提出的算法以及相关器官和成像方式进行了广泛的分类。 3）最后，我们讨论了挑战和开放的问题，并确定了新的趋势，提出了关于扩散模型在医疗领域的算法和应用的未来发展的开放问题 本次调查的动机和独特性。生成方法在医学成像领域取得了重大进展，其中一些论文只关注特定的应用，而另一些则专注于特定的图像形态。尽管在这一领域得到充分发展之前就已经发表了综述文章，但自那时以来，医学领域已经取得了许多进展。另一方面，这些调查都没有关注扩散模型在医学成像中的应用，这是推动这一研究方向向前发展的核心方面。因此，这些调查留下了明显的空白。此外，我们相信医学界可以通过回顾我们的调查提供的扩散模型的过去和未来的研究方向，从视觉扩散模型的成功产品中获得启示。 搜索策略。我们搜索了DBLP、Google Scholar和Arxiv Sanity Preserver，使用定制的搜索查询，因为它们允许定制搜索查询，并提供所有学术出版物的列表:同行评议的期刊论文或在会议或研讨会论文集中发表的论文，非同行评议的论文和预印本。值得注意的是，我们根据对其新颖性、贡献、意义的仔细评估，以及是否为医学成像领域的第一篇介绍论文，选择了论文进行详细检查。 论文的组织。 3.论文结构 理论 2.1 扩散模型在哪里适合生成式学习? 2.2 变分视角 2.2.1 去噪扩散概率模型(ddpm) 2.3 分数视角 2.3.1 噪声条件评分网络(ncsn) 2.3.2 随机微分方程(SDEs) 临床重要性 应用中的扩散模型 4.1 图像到图像的转换 4.2 重建 4.3 配准 4.4 分类 4.5 分割 4.6 去噪 4.7 图像生成 4.8 异常检测 4.9 其他应用和多任务 4.10 对比概述 未来方向和开放挑战 4.总结 本文综述了扩散模型的相关文献，重点介绍了扩散模型在医学成像领域的应用。具体来说，我们研究了扩散模型在异常检测、医学图像分割、去噪、分类、重建、配准、生成等任务中的应用。特别是，对于这些应用程序中的每一个，我们都从不同的角度提供了核心技术的分类和高级抽象。此外，我们基于技术对现有模型进行了表征，其中我们确定了基于ddpm, ncsn和SDEs的扩散建模的三种主要公式。最后，我们概述了未来研究的可能途径。 虽然我们的调查强调了医学成像中基于扩散的技术的快速增长，但我们也承认，该领域仍处于早期阶段，可能会发生变化。随着扩散模型越来越受欢迎，在这一领域的研究也越来越多，我们的调查为希望在工作中使用这些模型的研究人员和从业者提供了一个重要的起点和参考。我们希望这项调查将激发进一步的兴趣和探索扩散模型在医学领域的潜力。值得注意的是，本调查中引用的一些论文是预印本。然而，我们尽一切努力只包括来自信誉良好的来源的高质量研究，我们相信，包括预印本提供了对这个快速发展的领域当前最先进技术的全面概述。总的来说，我们相信我们的调查为扩散模型在医学成像中的应用提供了有价值的见解，并突出了未来研究的有前途的领域。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"扩散模型","slug":"扩散模型","permalink":"http://example.com/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/"}]},{"title":"PaperReading-综述-MRI与PET交叉模态合成研究进展","slug":"paper_MRI与PET交叉模态合成研究进展","date":"2023-08-30T04:42:17.929Z","updated":"2024-01-17T12:21:59.121Z","comments":true,"path":"2023/08/30/paper_MRI与PET交叉模态合成研究进展/","link":"","permalink":"http://example.com/2023/08/30/paper_MRI%E4%B8%8EPET%E4%BA%A4%E5%8F%89%E6%A8%A1%E6%80%81%E5%90%88%E6%88%90%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95/","excerpt":"","text":"A Review on Cross-modality Synthesis from MRI to PET 1.摘要 随着时间的推移，医学影像合成越来越受欢迎。在各具特色的成像技术中，MRI和PET在医疗领域有着重要的意义。但由于PET的某些局限性，如费用、辐射暴露和缺乏可用性，人们倾向于采用跨模态合成的方法。深度学习为该领域模型的发展铺平了道路，完成了跨模态合成任务。使用这些模型合成生物医学图像可以节省患者的时间、金钱和精力，并改善疾病诊断。本文旨在总结以MRI-PET交叉模态合成为最终目标的深度学习模型 2.引言 医学成像技术在医疗保健领域发挥了显著的作用,每种类型的成像技术都提供了一些人体解剖或功能信息，这意味着像MR和PET这样的多模态生物医学图像将提供补充信息，从而实现细致和快速的诊断。 磁共振成像(MRI)和正电子发射断层扫描(PET)这两种扫描在一起使用时可以提供补充信息，帮助医生做出强有力的临床判断，但是每个患者同时拥有这两种扫描的可能性很低，这有多种原因 1)PET模式的可用性有限。发展中国家的大多数医疗中心不提供PET扫描。 2)与MRI相比，PET是一种昂贵的方式。 3) PET中放射性示踪剂的使用，增加了终生癌症风险。 尽管存在这些障碍，但PET由于其独特的分子成像特性是必不可少的。而核磁共振成像作为一种更安全的成像方式，在这些限制方面具有优势。因此，为了有效解决这一问题，采用了跨模态合成策略。 跨模态合成是在源模态下评估同一受试者的目标模态图像的过程，即在我们的案例中是MR-to-PET。该方法不仅克服了PET的划界因素，而且简化了患者的治疗，减少了患者的整体扫描时间，减少了诊所和医院的工作量，提高了疾病预测的准确性，并且可以为医学成像数据集做出贡献。简而言之，它节省了人们的时间、金钱和精力。从而改善社会的健康和生活。 属于同一受试者的MR和PET扫描具有不同的外观，这使得学习跨域映射成为一项具有挑战性的任务。深度学习已被认为是实现计算机视觉任务的重要技术。随着GAN的体系结构及其变体得到了广泛的认可,在完成跨模态合成时，同样创建了基于GAN的模型 在本文中，我们总结了现有的用于MRI-PET交叉模态合成的基于深度学习的模型。 3.论文结构 1)文献调查 A.基于深度学习的影像数据补全改善脑部疾病诊断 本文使用3D CNN来估计缺失数据。该模型以MRI为输入，预测相应的PET。 B. MRI到FDG-PET:使用3D U-Net进行多模态阿尔茨海默病分类的跨模态合成 本文采用三维U-Net模型来捕捉这些模态之间的非局部相关和非线性关系。 C. 合成PET从PET使用周期一致的生成对抗网络用于阿尔茨海默病诊断 本文开发了一个两阶段的框架，该框架在初级阶段使用周期一致的GAN (3DcGAN)进行mri -PET合成，在后续阶段构建用于AD诊断的分类方法(LM3 IL)。 D.用pix2pix从MRI推断PET 为了有一个共同的框架，引入了pix2pix GAN，本文使用相同的架构来演示使用成对数据集的MRI到PET翻译任务。 E.使用不同归一化的对抗U-Net从MRI到PET的交叉模态合成 为了减少内部协变偏移问题和对数据集中范围更广的特征的偏倚，深度网络采用了归一化技术。本文指出了BN的某些缺点，用各种归一化方法对对抗U-Net模型进行了实验，以找出其中最适合该跨模态综合任务的方法。为了解决批量归一化中生物医学图像的稀少性与小批量大小要求之间的矛盾，采用批范数(Batch Norm, BN)、层范数(Layer Norm, LN)、实例范数(Instance Norm, in)和组范数(Group Norm, GN)四种归一化方法，对不同小批量大小的对抗U-Net的性能进行了评价。定量结果表明，IN对所有样本的每个通道相对于该通道的均值和标准差进行标准化，比其他归一化技术具有更好的性能。 F.基于条件流的模态迁移生成模型 G.双向映射生成对抗网络用于脑MR到PET合成 这项工作提出了一种类似于CGAN的方法，在这种方法中，它被端到端地指导，最终目标是AD分类。当使用分类目标进行训练时，可能会影响生成语用图像的性能。这一限制是克服自适应微调GAN损耗和分类损耗。同时，通过损失微调使一般GAN训练变得稳定。 I.FREA-Unet:模态传输的频率感知U-net 本文注意到PET扫描中存在不同的频率尺度以及深度学习模型中注意学习的本质，提出了端到端的频率感知U-net模型。为了反映合成PET与真实PET的不同频率尺度，该模型在解码部分从两个不同的层获得低频和高频PET图像，并在每一层上附加一个可训练的注意模块。根据本文的定义，低频/高频层的注意力是指从频率层提取的激活图与U-net解码路径中前一层到最后一层的产出之间的兼容性分数。然后在频率尺度层中使用这些分数来突出相关特征。将获得的低频/高频层的输出融合并馈送到最后一个解码器层，以生成最终的真实PET图像。利用从注意力模块获得的不同权重分别优化低/高频尺度，使模型能够生成更精确的合成PET，具有更高的分辨率和保留的器官结构和分辨率。 2)比较 4.总结 PET数据的缺乏一直是疾病准确诊断的障碍。跨模态综合方法是解决这一问题的有效方法。在本文中，我们总结了为实现MRI-PET交叉模态合成任务而开发的深度学习模型，并对这些模型进行了比较。可以看出，由于GAN在图像合成任务中的出色性能，大多数都提出了基于GAN的架构。此外，我们还简要概述了数据集和用于评估合成PET图像的常用定量矩阵。 5.文章笔记 MRI和PET的基本原理的叙述： 磁共振成像(MRI)是一种利用磁场和无线电波对人体内部器官进行成像的结构方式。这种非侵入性诊断工具测量所需身体部位的解剖结构。而正电子发射断层扫描(PET)是一种功能模式，使用放射性示踪剂来量化人体组织和器官中的代谢活动。 UNet优点： 码器和解码器网络之间的跳跃连接有助于保留输出图像中的空间/定位信息，并使用二值交叉熵作为损失函数，有助于生成平滑输出。 U-Net架构以其通过跳过连接将低级特征从编码器传输到解码器的能力而闻名，在这一过程中对于保留MR和PET切片之间常见的低级特征非常重要 评估合成医学图像的标准: • Mean Absolute Error (MAE) • Mean Squared Error (MSE) • Peak Signal-to-Noise Ratio (PSNR) • Structure Similarity Index (SSIM) • Multi-Scale Structural Similarity (MS-SSIM) • Frechet Inception Distance (FID) 注意力的优点: 注意力学习背后的主要思想是更多地关注与任务相关的特征，而不是简单地平等地关注图像的所有部分。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"GAN","slug":"GAN","permalink":"http://example.com/tags/GAN/"}]},{"title":"PaperReading-多模态医学图像融合技术综述","slug":"paper_多模态医学图像融合技术综述","date":"2023-08-30T03:55:56.125Z","updated":"2024-01-20T03:01:57.659Z","comments":true,"path":"2023/08/30/paper_多模态医学图像融合技术综述/","link":"","permalink":"http://example.com/2023/08/30/paper_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/","excerpt":"","text":"A Review of Multimodal Medical Image Fusion Techniques 1.摘要 医学图像融合是将来自多种成像方式的多幅图像进行融合，得到信息量大的融合图像，以提高医学图像的临床适用性的过程。本文对多模态医学图像融合方法进行了综述，重点介绍了该领域的最新进展，包括:(1)当前的融合方法，包括基于深度学习的融合方法;(2)医学图像融合的成像方式;(3)主要数据集上医学图像融合的性能分析。最后，本文的结论是，目前多模态医学图像融合的研究成果较为显著，发展趋势呈上升趋势，但研究领域存在诸多挑战。 2.引言 多模态医学图像融合被广泛研究。 医学图像融合通过融合同一部位的不同成像信息以获得更好的对比度，融合质量和感知体验。融合结果应满足以下条件： (a)融合后的图像应完全保留源图像的信息; (b)融合后的图像不应产生任何合成信息，如伪影; ©应避免不良状态，如误登记和噪音 传统的医学图像融合方法分为空间域和变换域。随着深度学习热潮的到来，出现了基于深度学习的医学图像融合方法，但只有CNN和U-Net网络得到了应用 本文结合近年来医学图像融合的相关论文，对该领域的研究进展及未来发展进行综述，分为以下几个部分： (1)对当前融合方法的介绍 (2)多模态融合的模式 (3)对同一数据库中具有相同评价指标的不同医学图像融合方法的数据进行比较 (4)讨论医学图像融合方法面临的挑战和未来的研究趋势 3.总结 医学图像融合方法存在的问题： 医学图像融合的评价指标多，评价指标的非唯一性限制了评价指标的应用前景 医学图像融合的创新性低，融合结果中存在的颜色失真、特征信息提取等问题只是得到了改善，而没有完全解决。其中深度学习提高了融合的效果，但研究也存在如下问题： （1）如何获取海量的数据集 （2）如何简化训练模型或提出新的训练模型 （3）部分融合方法依赖于精确的图像配准，独立性小 不同传感器获取的医学图像信息存在差异。目前的研究热点是两模融合，而三模融合的研究很少。 4.文章笔记 U-Net是基于全卷积神经网络改进的，使用数据增强可以训练少量的样本。这一优点正好弥补了医学图像数据样本量小的缺点 CNN是医学领域的新挑战;主要原因是(a)需要大量带注释的训练集数据，(b)训练时间长，©收敛问题复杂，过拟合需要反复调整。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"医学影像","slug":"医学影像","permalink":"http://example.com/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F/"},{"name":"图像融合","slug":"图像融合","permalink":"http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88/"},{"name":"综述文章","slug":"综述文章","permalink":"http://example.com/tags/%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/"}]},{"title":"PaperReading-基于不完整多模态数据以诊断导向神经图像合成的疾病图像特异性学习","slug":"paper_基于不完整多模态数据以诊断导向神经图像合成的疾病图像特异性学习","date":"2023-08-30T03:55:56.097Z","updated":"2024-01-20T03:01:48.695Z","comments":true,"path":"2023/08/30/paper_基于不完整多模态数据以诊断导向神经图像合成的疾病图像特异性学习/","link":"","permalink":"http://example.com/2023/08/30/paper_%E5%9F%BA%E4%BA%8E%E4%B8%8D%E5%AE%8C%E6%95%B4%E5%A4%9A%E6%A8%A1%E6%80%81%E6%95%B0%E6%8D%AE%E4%BB%A5%E8%AF%8A%E6%96%AD%E5%AF%BC%E5%90%91%E7%A5%9E%E7%BB%8F%E5%9B%BE%E5%83%8F%E5%90%88%E6%88%90%E7%9A%84%E7%96%BE%E7%97%85%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BC%82%E6%80%A7%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"Disease-Image-Specific Learning for Diagnosis-Oriented Neuroimage Synthesis With Incomplete Multi-Modality Data 1.摘要 在多源数据的分类任务中，数据不完整是一个普遍存在的问题，特别是在多模态神经图像的疾病诊断中，为了跟踪这一问题，人们提出了一些方法，通过输入缺失的神经图像来利用所有可用的被试。然而，这些方法通常将图像合成和疾病诊断视为两个独立的任务，从而忽略了不同模态所传达的特异性，即不同模态可能突出大脑中不同的疾病相关区域。为此，我们提出了一种疾病图像特异性深度学习(DSDL)框架，用于使用不完整多模态神经图像用于联合神经图像合成和疾病诊断。具体而言，以每次全脑扫描为输入，我们首先设计了一个带有空间余弦模块的疾病图像特异性网络(DSNet)，以隐式建模疾病图像特异性。然后，我们开发了一个特征一致性生成对抗网络(FGAN)来补全缺失的神经图像，其中合成图像的特征映射(由DSNet生成)与其各自的真实图像被鼓励保持一致，同时保留疾病图像特定信息。由于我们的FGAN与DSNet相关，缺失的神经图像可以以诊断为导向的方式合成。在三个数据集上的实验结果表明，我们的方法不仅可以生成合理的神经图像，而且在阿尔茨海默病识别和轻度认知障碍转换预测两项任务上都取得了较好的效果 2.引言 多模态神经影像学数据已被证明可以提供互补信息，以提高疾病的的计算机辅助诊断性能。数据缺失问题一直是使用多模态神经成像数据进行脑疾病自动诊断的常见挑战。 传统方法只使用模态完整的被试而丢弃模态不完整的被试，这种策略减少了训练样本数量，忽略了数据缺失受试者提供的有用信息，从而降低了诊断性能。已经提出了几种数据输入方法，利用数据完整受试者的特征来估计缺失数据受试者的手工特征。然而，这些方法依赖于手工制作的图像特征，可能无法区分脑部疾病的诊断，从而导致次优的学习性能。 更有希望的替代方法是通过深度学习直接估计缺失数据。然而之前的方法平等地处理每个脑容量中的所有体素，从而忽略了多模态神经成像数据中传达的疾病图像特异性。疾病图像的特异性是双重的： （1）并不是MRI/PET扫描的所有区域都与特定的脑部疾病有关 （2）与疾病相关的大脑区域可能在不同模态中是有差别的 对于第一个方面，现有的深度学习方法通常在图像合成过程中平等对待所有大脑区域，某些区域与疾病是高度相关的相比于其他区域。对于第二个方面，现有方法直接基于另一种模态图像合成一种模态图像，而不考虑疾病相关区域的模态差距。值得注意的是，已有研究表明，疾病诊断模型可以通过感兴趣区域和解剖标志来隐式或显式地捕捉疾病图像特异性。因此，为了捕获和利用疾病图像的特异性，直观地需要将疾病诊断和图像合成整合到一个统一的框架中，以诊断为导向的方式输入缺失的神经图像 在本文中，我们提出了一个疾病图像特异性深度学习(DSDL)框架，用于使用不完整的多模态神经图像用于联合疾病诊断和图像合成(见图1)。如图1a和1b所示，我们的方法主要包含两个单模态疾病图像特异性网络(DSNet)，用于基于MRI和pet的疾病诊断，以及一个用于图像合成的特征一致性生成对抗网络(FGAN)。本文中，DSNet对基于MRI和pet的特征图中的疾病图像特异性进行编码，以辅助FGAN的训练，而FGAN则对缺失图像进行补全以提高诊断性能。由于DSNet和FGAN可以联合训练，因此可以以诊断为导向的方式合成缺失的神经图像。使用完整的MRI和PET扫描(植入后)，我们可以通过提出的多模态DSNet进行疾病诊断(如图1c所示)。在三个公开数据集上的实验结果表明，我们的方法不仅可以合成合理的MRI和PET图像，而且在AD识别和MCI转换预测方面都取得了最先进的结果。 与我们之前的工作相比，本工作的贡献如下： (1)提出了一种统一的DSDL框架，用于不完整多模态神经图像的联合图像合成和AD诊断。缺失的图像以诊断为导向的方式输入，因此从诊断的角度来看，合成的神经图像与真实的神经图像更一致。 (2)设计了空间余弦模型，对全脑MRI/PET扫描的疾病图像特异性进行隐式自动建模。 (3)提出了一种特征一致性约束，可以帮助图像合成模型在模态转换过程中保留疾病相关信息 3.总结 我们提出了一种基于不完全多模态数据的面向任务的神经图像合成的疾病图像特异性深度学习框架，其中使用诊断网络为图像合成网络提供疾病图像特异性。具体来说，我们设计了一个单模态疾病图像特异性网络(DSNet)，对全脑图像进行训练，以隐式捕获MRI和PET传达的疾病相关信息。然后，我们开发了一个特征一致性生成对抗网络(FGAN)来合成缺失的神经图像，通过鼓励每个合成图像的特征映射与其各自的真实图像保持一致。我们进一步提出了一个多模态DSNet (mDSNet)，用于使用完整的(植入后)MRI和PET扫描进行疾病诊断。在三个公共数据集上的实验表明，我们的方法可以生成合理的神经图像，并在AD识别和MCI转换预测方面达到了最先进的性能。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"PAMI","slug":"PAMI","permalink":"http://example.com/tags/PAMI/"}]},{"title":"PaperReading-用于医学图像分类的双制导扩散网络","slug":"paper_用于医学图像分类的双制导扩散网络","date":"2023-08-30T03:55:56.062Z","updated":"2024-01-20T03:01:19.842Z","comments":true,"path":"2023/08/30/paper_用于医学图像分类的双制导扩散网络/","link":"","permalink":"http://example.com/2023/08/30/paper_%E7%94%A8%E4%BA%8E%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%9A%84%E5%8F%8C%E5%88%B6%E5%AF%BC%E6%89%A9%E6%95%A3%E7%BD%91%E7%BB%9C/","excerpt":"","text":"DiffMIC Dual-Guidance Diffusion Network for Medical Image Classification 1.摘要 近年来，扩散概率模型在生成图像建模中表现出了显著的性能，引起了计算机视觉界的广泛关注。然而，尽管大量基于扩散的研究集中在生成任务上，但很少有研究将扩散模型应用于一般医学图像分类。在本文中，我们提出了第一个基于扩散的模型(称为DiffMIC)，通过消除医学图像中的意外噪声和扰动并鲁棒地捕获语义表示来解决一般医学图像分类问题。为了实现这一目标，我们设计了一种双条件引导策略，该策略将每个扩散步骤设定为多个粒度，以提高逐步的区域注意力。此外，我们提出在扩散前向过程中通过强制最大均值差异正则化来学习每个粒度的互信息。我们评估了DiffMIC在三种不同图像模式下的医学分类任务中的有效性，包括超声图像上的胎盘成熟度分级、皮肤镜图像上的皮肤病变分级和眼底图像上的糖尿病视网膜病变分级。我们的实验结果表明，DiffMIC在很大程度上优于最先进的方法，表明了所提出模型的通用性和有效性。 2.引言 医学图像分析不可或缺，医学图像分类是医学图像分析的一个基本步骤。深度学习方法可以帮助医生解读医学图像，这些方法有可能减少人工分类所需的时间和精力，并提高结果的准确性和一致性。然而，由于存在各种模糊病变和细粒度组织，如超声(US)、皮肤镜和眼底图像，各种形式的医学图像仍然对现有方法提出挑战。此外，在硬件限制下生成医学图像可能会导致噪声和模糊效果，从而降低图像质量，因此需要更有效的特征表示建模以实现鲁棒分类。 最近，去噪扩散概率模型在图像生成和合成任务中取得了优异的效果。虽然有一些先驱作品试图将扩散模型用于图像分割和目标检测任务，但其在高级视觉方面的潜力尚未得到充分挖掘。 我们提出了一种新的基于扩散去噪的模型DiffMIC，用于准确分类不同的医学图像模态。 （1）据我们所知，我们是第一个提出基于扩散的一般医学图像分类模型。由于医学图像的扩散过程是随机的，因此我们的方法可以适当地消除医学图像中的不良噪声。 （2）特别地，我们引入了双粒度条件指导(DCG)策略来指导去噪过程，在扩散过程中使用全局和局部先验来调节每一步。通过在较小的斑块上进行扩散过程，我们的方法可以区分具有细粒度能力的关键组织。 （3）此外，我们引入了特定条件的最大平均差异(MMD)正则化来学习每个粒度潜在空间中的互信息，使网络能够建模整个图像和补丁共享的鲁棒特征表示。 （4）我们评估了DiffMIC在胎盘成熟度分级、皮肤病变分级和糖尿病视网膜病变分级三个二维医学图像分类任务中的有效性。实验结果表明，我们的基于扩散的分类方法在所有三个任务上都一致且显著地超过了最先进的方法。 3.总结 本文提出了一种基于扩散的医学图像分类网络(DiffMIC)。我们的DiffMIC的主要思想是在普通DDPM上引入双粒度条件指导，并强制执行特定于条件的MMD正则化以提高分类性能。在三个不同图像模式的医学图像分类数据集上的实验结果表明，我们的网络比最先进的方法具有更好的性能。作为第一个基于扩散的一般医学图像分类模型，我们的DiffMIC有可能成为该领域未来研究的基本基线 4.文章笔记 通过有限的跨模态信息生成的医学图像可能会导致噪声和模糊效果，从而降低诊断准确性，因此需要更有效的特征表示建模以实现鲁棒分类","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"扩散模型","slug":"扩散模型","permalink":"http://example.com/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/"}]},{"title":"Hello World","slug":"hello-world","date":"2023-08-30T03:04:39.444Z","updated":"2023-08-30T03:04:39.444Z","comments":true,"path":"2023/08/30/hello-world/","link":"","permalink":"http://example.com/2023/08/30/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"笔记","slug":"笔记","permalink":"http://example.com/categories/%E7%AC%94%E8%AE%B0/"},{"name":"编程","slug":"编程","permalink":"http://example.com/categories/%E7%BC%96%E7%A8%8B/"},{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"法律","slug":"法律","permalink":"http://example.com/tags/%E6%B3%95%E5%BE%8B/"},{"name":"医学","slug":"医学","permalink":"http://example.com/tags/%E5%8C%BB%E5%AD%A6/"},{"name":"Hexo","slug":"Hexo","permalink":"http://example.com/tags/Hexo/"},{"name":"Code-模型","slug":"Code-模型","permalink":"http://example.com/tags/Code-%E6%A8%A1%E5%9E%8B/"},{"name":"Code-数据集","slug":"Code-数据集","permalink":"http://example.com/tags/Code-%E6%95%B0%E6%8D%AE%E9%9B%86/"},{"name":"Code-分类","slug":"Code-分类","permalink":"http://example.com/tags/Code-%E5%88%86%E7%B1%BB/"},{"name":"Code-生成","slug":"Code-生成","permalink":"http://example.com/tags/Code-%E7%94%9F%E6%88%90/"},{"name":"MICCAI","slug":"MICCAI","permalink":"http://example.com/tags/MICCAI/"},{"name":"CVPR","slug":"CVPR","permalink":"http://example.com/tags/CVPR/"},{"name":"Cpp","slug":"Cpp","permalink":"http://example.com/tags/Cpp/"},{"name":"knowledge","slug":"knowledge","permalink":"http://example.com/tags/knowledge/"},{"name":"Code-表格处理","slug":"Code-表格处理","permalink":"http://example.com/tags/Code-%E8%A1%A8%E6%A0%BC%E5%A4%84%E7%90%86/"},{"name":"Code-文件操作","slug":"Code-文件操作","permalink":"http://example.com/tags/Code-%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/"},{"name":"Code-图像处理","slug":"Code-图像处理","permalink":"http://example.com/tags/Code-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"},{"name":"GAN","slug":"GAN","permalink":"http://example.com/tags/GAN/"},{"name":"扩散模型","slug":"扩散模型","permalink":"http://example.com/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/"},{"name":"医学影像","slug":"医学影像","permalink":"http://example.com/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F/"},{"name":"图像融合","slug":"图像融合","permalink":"http://example.com/tags/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88/"},{"name":"综述文章","slug":"综述文章","permalink":"http://example.com/tags/%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/"},{"name":"PAMI","slug":"PAMI","permalink":"http://example.com/tags/PAMI/"}]}